\section{Test set}

We possess a test set $\mathcal{D}_{test} = {(\mathbf{x}_j , t_j )}_{j=1}^J$ comprising independent and identically distributed random variable samples from $\Pr$, completely separate from the training dataset $\mathcal{D}_{train}$.
For any arbitrary hypothesis $h\in\mathcal{H}$, we can assess the test loss:
\[\tilde{\mathcal{L}}(h)=\dfrac{1}{J}\sum_{j=1}^{J}\ell(h(\mathbf{x}_j),t_j)\]
The empirical risk minimizer $\hat{h}$ remains independent of $\mathcal{D}_{test}$ (while dependent on $\mathcal{D}_{train}$): consequently, the test loss $\tilde{L}(\hat{h})$ stands as an unbiased estimator for the true loss $L(\hat{h})$.

\paragraph*{Hoeffding inequality bound}
Suppose $X_1,\ldots, X_t$ are independent and identically distributed random variables with support in $[0, L]$, all having the identical mean $\mathbb{E}[X_i] =: X$, and let denote the sample mean as: 
\[\bar{X}_t=\dfrac{\sum_{i=1}^tX_i}{t}\] 

Then:
\[\Pr(X\leq\bar{X}_t+u)\geq1-e^{-\frac{2tu^2}{L^2}}\]
This implies an upper bound can be constructed with at least $1-\delta$ confidence by setting $\delta=e^{-\frac{2tu^2}{L^2}}$. 
Consequently, the bound becomes:
\[X\leq\bar{X}_t+u=\bar{X}_t+L\sqrt{\dfrac{\log\left(\frac{1}{\delta}\right)}{2n}}\]
A crucial observation: all losses $\{\ell(\hat{h}(\mathbf{x}_j),t_j)\}^J_{j=1}$ are independent and identically distributed random variables conditioned to $\hat{h}$. 
$\tilde{\mathcal{L}}(\hat{h})$  can be interpreted as a sample mean of independent and identically distributed random variable samples, estimating the true mean $\mathcal{L}(\hat{h})$.

Under the assumption of bounded loss $\ell(y, y^\prime) \in [0, L]$, Hoeffding's inequality can be applied:
\[\mathcal{L}(\hat{h})\leq\tilde{\mathcal{L}}(\hat{h})+L\sqrt{\dfrac{\log\left(\frac{1}{\delta}\right)}{2J}}\]
With probability $1-\delta$. 
The larger the test set ($J$), the more precise the estimate $\tilde{\mathcal{L}}(\hat{h})$ becomes. 
Notably, there's no dependence on the complexity of the hypothesis space $\mathcal{H}$.