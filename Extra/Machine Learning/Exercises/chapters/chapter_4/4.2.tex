\section{Exercise two}

State whether the following claims about Bagging and Boosting are true or false, motivating your answers:
\begin{enumerate}
    \item Since Boosting and Bagging are ensemble methods, they can be both parallelized.
    \item Bagging should be applied with weak learners.
    \item The central idea of Boosting consists in using bootstrapping.
    \item It is not a good idea to use Boosting with a deep neural network as a base learner.
\end{enumerate}

\subsection*{Solution}
\begin{enumerate}
    \item False, only Bagging can be parallelized, since training is done on different datasets, while Boosting is sequential by nature.
    \item  False, weak learners are good candidate for Boosting, since they have low variance. 
        Typically, one uses instead bagging when more complex and unstable learners are needed, to reduce their variance.
    \item  False, bootstrapping is used in bagging, whose name derived indeed from boosting aggregation.
    \item  True, it is not a good idea to do that, since deep neural networks are very complex predictor, which can have large variance. 
        Therefore, you may not succeed in lowering bias without increasing variance. 
        Moreover, since you need to train the network multiple times, the procedure may require a lot of time.
\end{enumerate}