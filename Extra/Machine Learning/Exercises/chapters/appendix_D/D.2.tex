\section{Model selection}

\subsection{Filter method}
In the absence of a predefined hypothesis space for models, the filter method for feature selection involves the following steps: 
\begin{enumerate}
    \item For each feature $j \in \{1,\ldots,M\}$, compute the Pearson correlation coefficient between $x_k$ and the target variable $y$:
        \[\hat{\rho}(x_j,y)=\dfrac{\sum_{n=1}^N(x_{j,n}-\bar{x}_j)(y_n-\bar{y})}{\sqrt{\sum_{n=1}^N(x_{j,n}-\bar{x}_j)^2}\sqrt{\sum_{n=1}^N(y_n-\bar{y})^2}}\]
        Where: 
        \[\bar{x}_j=\dfrac{1}{N}\sum_{n=1}^Nx_{j,n} \qquad \bar{y}=\dfrac{1}{N}\sum_{n=1}^Ny_{j}\]
    \item Select the features with higher Pearson correlation coefficient, which captures only linear relationships between features and the target variable.
\end{enumerate}
For addressing non-linear relationships, alternative approaches such as mutual information can be employed.

\subsection{Wrapper method}
In the wrapper method for feature selection, where a hypothesis space of models  $\mathcal{H}$ is provided as input, the following steps are undertaken: 
\begin{enumerate}
    \item For each $k$ number of features $k \in \{1,\ldots,M\}$ learn all possible $\binom{M}{k}$ models within $\mathcal{H}$ with $k$ inputs. 
    \item Select the model with the smallest loss.
\end{enumerate}
Choose the number of features $M$ that provides the selected model with the smallest loss.

\paragraph*{Application to Iris dataset}
Consider a classification problem where the goal is to discriminate between Virginica and Non-Virginica iris species.
We select a performance index: validation accuracy on 20\% of the data.
Initially, we train a logistic regression model on the full dataset $(x_1, x_2, x_3, x_4)^T$ and observe the accuracies after removing individual features:
\begin{itemize}
    \item Model with $(x_1, x_2, x_3)^T$: accuracy 1.
    \item Model with $(x_1, x_3, x_4)^T$: accuracy 1.
    \item Model with $(x_1, x_2, x_4)^T$: accuracy 1.
    \item Model with $(x_2, x_3, x_4)^T$: accuracy 1.
\end{itemize}
Removing a single feature doesn't affect the model's performance.
Let's randomly remove $x_4$ and then another feature to check the error:
\begin{itemize}
    \item Model with $(x_1, x_3)^T$: accuracy 0.96.
    \item Model with $(x_1, x_2)^T$: accuracy 0.96.
    \item Model with $(x_2, x_3)^T$: accuracy 1.
\end{itemize}
The model with features $(x_2, x_3)^T$ exhibits superior performance.

\subsection{Principal Component Analysis}
Principal Component Analysis (PCA) is an unsupervised dimensionality reduction technique used to extract low-dimensional features from a dataset.

It performs a linear transformation of the original data matrix $\mathbf{X}$, such that the largest variance is captured by the first transformed feature, the second largest variance by the second transformed feature, and so forth.

Finally, to reduce dimensionality, only a subset of the extracted features is retained.

\paragraph*{Procedure}
The steps are: 
\begin{enumerate}
    \item Translate the original data matrix $\mathbf{X}$ to $\tilde{\mathbf{X}}$ such that they have zero mean.
    \item Compute the covariance matrix of $\tilde{\mathbf{X}}$: $\mathbf{C}=\tilde{\mathbf{X}}^T\tilde{\mathbf{X}}$. 
    \item The eigenvectors of $\mathbf{C}$ are the principal components. 
        The computation of the eigenvectors can be done with Singular Value Decomposition (SVD). 
    \item Given a sample vector $\tilde{\mathbf{x}}$, its transformed version $\mathbf{t}$ can be computed using:
        \[\mathbf{T}=\tilde{\mathbf{X}}\mathbf{W}\]
        Here: 
        \begin{itemize}
            \item Loadings: $\mathbf{W} = (\mathbf{e}_1|\mathbf{e}_2|\ldots |\mathbf{e}_M)$ matrix of the principal components. 
            \item Scores: $\mathbf{W}$ transformation of the input dataset $\tilde{\mathbf{X}}$. 
            \item Variance: $(\lambda_1,\ldots, \lambda_M)^T$ vector of the variance of principal components.
        \end{itemize}
    \item There are several methods to determine how many features to choose:
        \begin{itemize}
            \item Keep all the principal components until we have a cumulative variance of 90\%-95\%:
                \[\text{cumulative variance with }k\text{ components}=\dfrac{\sum_{j=1}^k\lambda_i}{\sum_{j=1}^M\lambda_i}\]
            \item Keep all the principal components which have more than 5\% of variance (discard only those with low variance).
            \item Find the elbow in the cumulative variance.
        \end{itemize}
\end{enumerate}

\paragraph*{Purposes}
\begin{itemize}
    \item \textit{Feature extraction}: reduce the dimensionality of the dataset by selecting only the number of principal components that retain information about the problem.
    \item \textit{Compression}: retain the first $k$ principal components and obtain $\mathbf{T}_k = \tilde{\mathbf{X}}\mathbf{W}_k$. 
        The linear transformation $\mathbf{W}_k$ minimizes the reconstruction error:
        \[\min_{\mathbf{W}_k\in\mathbb{R}^{M\times k}}\left\lVert \mathbf{T}\mathbf{W}_k^T-\tilde{\mathbf{X}}\right\rVert _2^2 \]
    \item \textit{Data visualization}: reduce the dimensionality of the input dataset to 2 or 3 dimensions to facilitate visualization of the data.
\end{itemize}

\subsection{Regularization}
Regularization techniques such as ridge, lasso, and elastic net are well-established procedures used to mitigate overfitting in linear regression models. 
While originally developed for linear regression, these methods can also be extended to other machine learning algorithms.

For classification tasks, specific regularization methods tailored to the nature of the problem are employed. 