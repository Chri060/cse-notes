\section{Monte Carlo methods}

Dynamic Programming enables us to determine the optimal value function and corresponding optimal policy. 
However, its major limitation lies in the assumption that we have full knowledge of the problem dynamics. 
To overcome this limitation, we seek methods that can learn the optimal policy directly from data.

Monte Carlo methods rely solely on experience (data) to learn value functions and policies.
They can be utilized in two ways:
\begin{itemize}
    \item \textit{Model-free}: no model is necessary, yet it can still achieve optimality.
    \item \textit{Simulated}: requires only a simulation, not a complete model.
\end{itemize}
Monte Carlo methods learn from complete sample returns and are exclusively defined for episodic tasks.

\subsection{Policy evaluation}
The goal of Monte Carlo policy evaluation is to learn $V_{\pi}(s)$ given some number of episodes under $\pi$ which contain $s$. 
The idea is to average the returns observed after visits to $s$: 
\[V_{\pi}(s)\doteq\mathbb{E}_{\pi}[G_t|S_t=s]\rightarrow V_{\pi}(s)\approx\text{average}[G_t|S_t=s]\]
We can perform Monte Carlo policy evaluation in two ways: 
\begin{itemize}
    \item Every-Visit MC: average returns for every time s is visited in an episode
    \item First-visit MC: average returns only for first time s is visited in an episode
\end{itemize}
Note that Both converge asymptotically
\begin{algorithm}[H]
    \caption{Monte Carlo policy evaluation algorithm}
        \begin{algorithmic}[1]
            \State{Initialize $V(s)\in\mathbb{R}$ arbitrarily, for all $s\in\mathcal{S}$}\Comment{Initialization}
            \State{Initialize Returns$(s)$ as an empty list, for all $s\in\mathcal{S}$}
            \Repeat 
                \For{each episode}
                    \State{Generate an episode following $\pi:S_0,A_0,R_1S_1,A_1,R_2,\cdots,S_{T-1},A_{T-1},R_{T}$}
                    \State{$G=0$}
                    \For{each step of episode $t=T-1,T-2,\cdots,0$}
                        \State{$G=\gamma G+R_{t+1}$}
                        \If{$S_t\notin\{S_0,S_1.\cdots,S_{t-1}\}$}: 
                            \State{Append $G$ to Returns($S_t$)}
                            \State{$V(S_t)=\text{average}(\text{Returns}(S_t))$}
                        \EndIf
                    \EndFor
                \EndFor
            \Until{true}
        \end{algorithmic}
\end{algorithm}
The input of this algorithm is a policy $\pi$ to be evaluated.
The incremental updates of lines ten and eleven can be done in the following way: 
\[N(S_t)=N(S_t)+1\]
\[V(S_t)=V(S_t)+\dfrac{1}{N(S_t)}(G-V(S_t)) \text{  or  } V(S_t)=V(S_t)+\alpha(G_t-V(S_t))\]

\subsection{Policy iteration}
To improve the policy we need to find a policy that maximized the q value function: 
\[\pi^\prime(s)=\argmax_a Q_{\pi}(s,a)\]
To do so, we average return starting from state $s$ and action a following $\pi$:
\[Q_{\pi}(s,a)\doteq\mathbb{E}_{\pi}[G_t|S_t=s,A_t=a]\rightarrow Q_{\pi}(s,a)\approx\text{average}[G_t|S_t=s,A_t=a]\]
This method Converges asymptotically if every state-action pair is visited.  

To have this full exploration in a simple way we use exploring starts. 
We choose randomly the first state and the first action, and we perform the following algorithm. 
\begin{algorithm}[H]
    \caption{Monte Carlo exploring starts}
        \begin{algorithmic}[1]
            \State{$\pi(s) \in\mathcal{A}(s)$ arbitrarily, for all $s \in\mathcal{S}$}
            \State{$Q(s, a) \in \mathbb{R}$ arbitrarily, for all $s \in \mathcal{S}$, $a \in \mathcal{A}(s)$}
            \State{Returns$(s, a)=$ empty list, for all $s 2 \mathcal{S}$, $a \in \mathcal{A}(s)$}
            \Loop 
                \State{Choose $S_0 \in\mathcal{S}, A_0 \in \mathcal{A}(S_0)$ randomly such that all pairs have probability greater than zero}
                \State{Generate an episode from $S_0$, $A_0$, following $\pi: S_0, A_0, R_1,\cdots,S_{T-1}, A_{T-1}, R_T$}
                \State{$G=0$}
                \For{each step of episode, $t = T_1, T_2,\cdots, 0$} 
                    \State{$G=\gamma G+R_{t+1}$}
                    \If{$S_t,A_t\notin S_0, A_0, S_1, A_1,\cdots,S_{t-1}, A_{t-1}$} 
                        \State{Append $G$ to Returns($S_t, A_t$)}
                        \State{$Q(S_t, A_t)=$ average(Returns($S_t, A_t$))}
                        \State{$\pi(S_t)=\argmax_aQ(S_t,a)$}
                    \EndIf
                \EndFor
            \EndLoop
        \end{algorithmic}
\end{algorithm}

\subsection{Epsilon-soft Monte Carlo policy iteration}
Exploring starts is a simple idea but it is not always possible. 
But, we need to keep exploring during the learning process
This leads to a key problem in RL: the Exploration-Exploitation Dilemma


$\varepsilon$-Greedy Exploration is the simplest solution to the exploration-exploitation dilemma
Instead of searching the optimal deterministic policy we search the optimal $\varepsilon$-soft
policy, i.e., a policy that selects each action with a probability that is at least $\frac{\varepsilon}{\left\lvert \mathcal{A}\right\rvert}$. 

In particular we use $\varepsilon$-greedy policy:
\[\pi(a|s)=\begin{cases}
    \frac{\varepsilon}{\left\lvert \mathcal{A}(s)\right\rvert}+1-\varepsilon \qquad \text{if }a^\ast=\argmax_{a\in\mathcal{A}}Q_(s,a) \\
    \frac{\varepsilon}{\left\lvert \mathcal{A}(s)\right\rvert}\qquad\qquad\quad\:\:\text{otherwise}
\end{cases}\]
This algorithm takes as input a small $\varepsilon>0$
\begin{algorithm}[H]
    \caption{$\varepsilon$-soft Monte Carlo policy iteration}
    \begin{algorithmic}[1]
        \State{$\pi$=an arbitrary $\varepsilon$-soft policy}
        \State{$Q(s, a) \in\mathbb{R}$ arbitrarily, for all $s \in\mathcal{S}$, $a \in\mathcal{A}(s)$}
        \State{Returns($s, a$) empty list, for all $s \in\mathcal{S}$, $a \in\mathcal{A}(s)$}
        \Loop{ for each episode}
            \State{Generate an episode following $\pi: S_0, A_0, R_1,\ldots,S_{T-1}, A_{T-1}, R_T$}
            \State{$G=0$}
            \For{each step of episode, $t = T-1, T-2,\ldots, 0$}
                \State{$G=\gamma G+R_{t+1}$}
                \If{$S_t,A_t\notin S_0, A_0, S_1, A_1,\ldots,S_{t-1}, A_{t-1}$}
                    \State{Append $G$ to Returns($S_t,A_t$)}
                    \State{$Q(S_t,A_t)=$average(Returns($S_t,A_t$))}
                    \State{$A^\ast=\argmax_aQ(S_t,a)$}\Comment{Ties broken arbitrarily}
                    \For{$a\in\mathcal{A}(S_t)$}
                        \State{$\pi(a|S_t)=\begin{cases}
                            1 -\varepsilon +\frac{\varepsilon}{\left\lvert \mathcal{A}(S_t)\right\rvert } \:\:\qquad\text{if } a = A^\ast \\
                            \frac{\varepsilon}{\left\lvert \mathcal{A}(S_t)\right\rvert }\qquad\qquad\qquad \text{if } a \neq A^\ast
                        \end{cases}$}
                    \EndFor
                \EndIf
            \EndFor
        \EndLoop
    \end{algorithmic}
\end{algorithm}
\begin{theorem}
    Any $\varepsilon$-greedy policy $\pi^\prime$ with respect to $Q_{\pi}$ is an improvement over any $\varepsilon$-soft policy $\pi$. 
\end{theorem}
\begin{proof}
    We have that: 
    \begin{align*}
        V_{\pi}(s)  &= Q_{\pi}(s, \pi^\prime(s)) \\
                    &= \sum_{a \in \mathcal{A}} \pi^\prime(a|s)Q_{\pi}(s, a) \\
                    &= \varepsilon \sum_{a \in \mathcal{A}} \frac{1}{|A|} \sum_{a \in \mathcal{A}} Q_{\pi}(s, a) + (1 - \varepsilon) \max_{a \in \mathcal{A}} Q^{\pi}(s, a) \\
                    &\geq \varepsilon \sum_{a \in \mathcal{A}} Q_{\pi}(s, a) + (1 - \varepsilon) \sum_{a \in \mathcal{A}} \frac{\pi(a|s) - \frac{\varepsilon}{|A|}}{1 - \varepsilon} \bar{Q}_{\pi}(s, a) \\
                    &= \sum_{a \in \mathcal{A}} \pi(a|s)Q_{\pi}(s, a) = V_{\pi}(s)
    \end{align*}
\end{proof}

\subsection{Off-policy learning}
\paragraph*{On-policy learning}
On-policy learning involves the agent learning the value functions based on the same policy it uses to select actions. 
This method faces challenges in balancing exploration and exploitation, making it difficult to converge to an optimal deterministic policy.

\paragraph*{Off-policy learning}
Off-policy learning, on the other hand, allows the agent to select actions using a behavior policy $b(a|s)$, while learning the value functions of a different target policy $\pi(a|s)$. 
This flexibility enables the agent to use an explorative behavior policy, while still learning towards an optimal deterministic policy $\pi^\ast(a|s)$.

Regardless of our behavior policy, it's impossible to learn any policy $\pi(a|s)$ if there are actions in that state with zero probability according to the behavior policy $b(a|s)$. 
This situation occurs when the behavior policy never transitions to a particular state from the current one.

\paragraph*{Importance sampling}
Importance sampling enables the estimation of expectations of a distribution that differs from the one used to draw the samples:
\[\mathbb{E}_p[x]=\sum_{x\in X}xp(x)=\sum_{x\in X}x\dfrac{p(x)}{q(x)}q(x)=\sum_{x\in X}z\rho(x)q(x)=\mathbb{E}_q[x\rho(x)]\]
Consequently, for sample-based estimation:
\[\mathbb{E}_p[x]\approx\dfrac{1}{N}\sum_{i=1}^{N}x_i\text{ if }x_i\sim p(x)\rightarrow \mathbb{E}_p[x]\approx\dfrac{1}{N}\sum_{i=1}^{N}x_i\rho(x_i)\text{ if }x_i\sim q(x)\]

\paragraph*{Importance sampling in policy evaluation}
When adhering to policy $\pi$, the computation of the state value function is expressed as:
\[V_{\pi}(s)\approx\text{average}(\text{Returns}[0],\text{Returns}[1],\text{Returns}[2],\cdots)\]
However, under policy $b$, the value function transforms into:
\[V_{\pi}(s)\approx\text{average}(\rho_0\text{Returns}[0],\rho_1\text{Returns}[1],\rho_2\text{Returns}[2],\cdots)\]
Here, $\rho_i$ denotes the probability of executing the trajectory observed in episode $i$ while adhering to policy $\pi$, relative to the probability of observing the same trajectory while following policy $b$:
\[\rho=\dfrac{\Pr(\text{trajectory under }\pi)}{\Pr(\text{trajectory under }b)}\]
In practical terms, $\rho_{t:T-1}$ can be calculated as:
\[\rho_{t:T-1}=\prod_{k=t}^{T-1}\dfrac{\pi(A_k|S_k)}{b(A_k|S_k)}\]
Sampling methods include:
\begin{itemize}
    \item \textit{Ordinary}: unbiased with higher variance:
        \[V_{\pi}(s)\approx\dfrac{\sum_i\rho[i]\text{Return}[i]}{N(s)}\]
    \item \textit{Weighted}: biased (bias converges to zero) with lower variance:
    \[V_{\pi}(s)\approx\dfrac{\sum_i\rho[i]\text{Return}[i]}{\sum_i\rho[i]}\]
\end{itemize}

\begin{algorithm}[H]
    \caption{Off-Policy every visit Monte Carlo prediction}
        \begin{algorithmic}[1]
            \State{$V(s) \in\mathbb{R}$ arbitrarily, for all $s\in S$}
            \State{Returns$(s)=$ an empty list, for all $s\in S$}
            \For{each episode}
                \State{Generate an episode following $b:S_0, A_0, R_1, S_1, \ldots, S_{T-1}, A_{T-1}, R_T$}
                \State{$G=0$}
                \State{$W=1$}
                \For{each step of episode, $t = T - 1, T - 2, \ldots, 0$}
                    \State $G=\gamma WG + R_{t+1}$
                    \State Append $G$ to Returns($S_t$)
                    \State $V(S_t)=$ average(Returns($S_t$))
                    \State $W=W \dfrac{\pi(A_t | S_t)}{b(A_t | S_t)}$
                \EndFor
            \EndFor
        \end{algorithmic}
\end{algorithm}
The input of this algorithm is a policy $\pi$ to be evaluated.