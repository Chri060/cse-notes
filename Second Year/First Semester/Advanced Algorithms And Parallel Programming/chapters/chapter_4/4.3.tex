\section{Performance}

The main values used to evaluate the performance are: 
\renewcommand*{\arraystretch}{2}
\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|}
    \hline
    \textbf{Parameter} & \textbf{Description} \\ \hline
    $T^\ast(n)$ & \makecell{Time to solve a problem of input size $n$ on one processor \\ using best sequential algorithm}               \\ \hline
    $T_1(n)$ & Time to solve a problem on one processor                  \\ \hline
    $T_p(n)$ & Time to solve a problem on $p$ processors                    \\ \hline
    $T_{\infty}(n)$ & Time to solve a problem on $\infty$ processors        \\ \hline
    $\text{SU}_p = \dfrac{T^\ast(n)}{T_p(n)}$ & Speedup on $p$ processors    \\ \hline
    $E_p = \dfrac{T_1}{p T_p(n)}$ & Efficiency                               \\ \hline
    $C(n) = p T_p(n)$ & Cost                                               \\ \hline
    $W(n)$ & Work (total number of operations)                               \\ \hline
    \end{tabular}
\end{table}
\renewcommand*{\arraystretch}{1}

\subsection{Matrix-vector multiplication}
Matrix-vector multiplication involves multiplying a matrix by a vector.

To perform the multiplication, each element of the resulting vector is computed by taking the dot product of the rows of the matrix with the vector. 
Specifically, if you have a matrix $\mathbf{A}$ of size $n\times n$ and a vector $\mathbf{v}$ of size $n$, the resulting vector $\mathbf{u}$ will have size $n\times n$:
\[\mathbf{u}=\mathbf{Av}\]
The entry $u_i$ of the resulting vector is calculated as:
\[u_i=\sum_{j=1}^na_{ij}v_j\]
Here, $a_{ij}$ are the elements of the matrix $\mathbf{A}$. 
The algorithm that computes the vector $\mathbf{u}$ is:
\begin{algorithm}[H]
    \caption{Matrix-vector multiplication}
    \begin{algorithmic}[1]
        \State Global read $x \leftarrow \mathbf{v}$ \Comment{Broadcast vector $\mathbf{v}$ to all processors}
        \State Global read $y \leftarrow \mathbf{a}_{i}$ \Comment{Read corresponding rows of matrix $\mathbf{A}$}
        \State Compute $w = x y$ \Comment{Multiply matrix row with vector $\mathbf{v}$}
        \State Global write $w \rightarrow u_i$ \Comment{Write result to the output vector $\mathbf{u}$}
    \end{algorithmic}
\end{algorithm}  
The performance measures of this algorithm in the best-case scenario are shown in the following table:
\renewcommand*{\arraystretch}{2}
\begin{table}[H]
    \centering
    \begin{tabular}{|c|cc|}
    \hline
    \textbf{Measure} & $T_1$ & $T_p$ \\ \hline
    \textbf{Complexity} & $\mathcal{O}(n^2)$ & $\mathcal{O}\left(\dfrac{n^2}{p}\right)$ \\ \hline
    \end{tabular}
\end{table}
\renewcommand*{\arraystretch}{1}

\subsection{Single program multiple data sum}
In single program multiple data (SPMD), each processor operates independently on its subset of the data, typically using the same code but possibly with different input data. 
This model is commonly used in high-performance computing, scientific simulations, and data analysis tasks, enabling significant performance improvements by leveraging parallelism. 

In the context of SPMD, a sum refers to the process of aggregating data from multiple processors or cores that are executing the same program on different segments of data. 
Here's how it typically works:
\begin{enumerate}
    \item  \textit{Data distribution}: the data is divided into chunks, with each CPU assigned a specific subset to work on.
    \item \textit{Local computation}: each processor executes the same summation program on its assigned data.
    \item \textit{Local results}: after computing their local sums, each processor has a partial sum.
    \item \textit{Reduction}: the partial sums are then combined (reduced) to get the final sum. 
    \item \textit{Final output}: the final result is the total sum of all the partial sums computed by the individual processors.
\end{enumerate}

\begin{algorithm}[H]
    \caption{SPMD sum}
    \begin{algorithmic}[1]
        \State Global read $x \leftarrow \mathbf{b}$ \Comment{Broadcast array $\mathbf{b}$ to all processors}
        \State Global write $y \rightarrow \mathbf{c}$ \Comment{Broadcast array $\mathbf{c}$ to all processors}
        \State Compute $z=x+y$ \Comment{Sum all vectors elements}
        \State Global write $z\rightarrow \mathbf{a}$ \Comment{Write result to the output array $\mathbf{a}$}
    \end{algorithmic}
\end{algorithm}
The performance measures of this algorithm are shown in the following table:
\renewcommand*{\arraystretch}{2}
\begin{table}[H]
    \centering
    \begin{tabular}{|c|cc|}
    \hline
    \textbf{Measure} & $T_1$ & $T_p$  \\ \hline
    \textbf{Complexity} & $\mathcal{O}(n)$ & $\mathcal{O}\left(\frac{n}{p}+\log p\right)$ \\ \hline
    \end{tabular}
\end{table}
\renewcommand*{\arraystretch}{1}

\subsection{Matrix-matrix multiplication}
Matrix-matrix multiplication involves multiplying a matrix by another matrix.

To perform the multiplication, each element of the resulting matrix is computed by taking the dot product of the rows of the first matrix with the columns of the second matrix. 
Specifically, if you have a matrix $\mathbf{A}$ of size $m\times n$ and a matrix $\mathbf{B}$ of size $n\times p$, the resulting matrix $\mathbf{C}$ will have size $m\times p$:
\[\mathbf{C}=\mathbf{AB}\]
The entry $c_{ij}$ of the resulting matrix is calculated as:
\[c_{ij}=\sum_{k=1}^na_{ik}b_{kj}\]
Here, $a_{ik}$ are the elements of matrix $\mathbf{A}$ and $b_{kj}$ are the elements of matrix $\mathbf{B}$.
\begin{algorithm}[H]
    \caption{Matrix-matrix multiplication}
    \begin{algorithmic}[1]
        \State Global read $x \leftarrow \mathbf{a}_i$ \Comment{Read corresponding rows of matrix $\mathbf{A}$}
        \State Global read $y \leftarrow \mathbf{b}_i$ \Comment{Read corresponding columns of matrix $\mathbf{B}$}
        \State Compute $w = x y$ \Comment{Multiply matrix $\mathbf{A}$ row with matrix $\mathbf{B}$ column}
        \State Global write $w \rightarrow \mathbf{u}_i$ \Comment{Write result to corresponding row of output matrix $\mathbf{u}$}
    \end{algorithmic}
\end{algorithm}  
The performance measures of this algorithm are shown in the following table:
\renewcommand*{\arraystretch}{2}
\begin{table}[H]
    \centering
    \begin{tabular}{|c|cc|}
    \hline
    \textbf{Measure} & $T_1$ & $T_p$ \\ \hline
    \textbf{Complexity} & $\mathcal{O}(n^3)$ & $\mathcal{O}\left(\dfrac{n^3\log n}{p}\right)$ \\ \hline
    \end{tabular}
\end{table}
\renewcommand*{\arraystretch}{1}