\section{Threads}

A thread is an independent unit of execution within a process, capable of running concurrently with other threads. 
Each thread has its own local data, but can access the resources shared by the parent process. 
Unlike a full process, which includes information about resources and execution state, a thread is more lightweight, with minimal overhead. 
A single process can spawn multiple threads, each of which operates as an independent stream of instructions.

Threads are managed by the operating system, which schedules them for execution. 
They can run in parallel, allowing for more efficient use of resources. 
A key distinction between threads and processes is that threads within the same process share most of the process's resources, including memory. 
Changes made by one thread to shared resources, such as closing a file or modifying a variable, are immediately visible to all other threads within the same process.

In this sense, threads provide a form of implicit communication, as they can read and write shared variables. 
However, because multiple threads can access the same memory location, this requires explicit synchronization to prevent conflicts. 
Without proper synchronization, concurrent threads may cause unpredictable behavior, particularly when they attempt to modify the same data simultaneously.

Threads can be created dynamically during execution, and are generally more efficient than creating multiple processes since they share resources, avoiding the overhead associated with full process management. 
Since threads operate independently but share the same memory space, synchronization mechanisms such as locks or semaphores must be used to prevent conflicting operations.

\subsection{Race condition}
A race condition occurs when two or more threads access the same variable concurrently, and at least one of them performs a write operation. 
Since these accesses are not synchronized, there is a risk that the threads may interfere with each other, leading to inconsistent or incorrect results. 

To prevent race conditions, synchronization mechanisms are required, which ensure that only one thread can modify the shared resource at a time.
This can be achieved through various methods, such as using mutexes, locks, or atomic operations. 
The programmer is responsible for managing the synchronization of threads, often using libraries, compiler directives, or other tools designed to handle parallelism and ensure the integrity of shared data.