\section{Regularization}

\subsection{Early stopping}
Early stopping may be applied in the following way: 
\begin{lstlisting}[style=Python]
es_model = build_model(input_shape)
es_model.summary(expand_nested=True, show_trainable=True)
tfk.utils.plot_model(es_model, expand_nested=True, show_trainable=True, show_shapes=True, dpi=70)

# Define the patience value for early stopping
patience = 100

# Create an EarlyStopping callback
early_stopping = tfk.callbacks.EarlyStopping(
    monitor='val_mse',
    mode='min',
    patience=patience,
    restore_best_weights=True
)

# Store the callback in a list
callbacks = [early_stopping]

# Train the model and store the training history
es_history = es_model.fit(
    x = X_train,
    y = y_train,
    validation_data = (X_val, y_val),
    batch_size = batch_size,
    epochs = epochs,
    callbacks = callbacks
).history

# Set the number of initial data points to ignore
ignore = 0

# Create a figure for loss visualization
plt.figure(figsize=(21, 4))

# Plot training and validation loss for early-stopped model
plt.plot(es_history['loss'][ignore:], label='Training loss', alpha=.2, color='#ff7f0e')
plt.plot(es_history['val_loss'][ignore:], label='Validation loss', alpha=.8, color='#ff7f0e')
plt.title('Loss')
plt.legend()
plt.grid(alpha=.3)

# Create a figure for Mean Squared Error visualization
plt.figure(figsize=(21, 4))

# Plot training and validation MSE for early-stopped model
plt.plot(es_history['mse'][ignore:], label='Training MSE', alpha=.2, color='#ff7f0e')
plt.plot(es_history['val_mse'][ignore:], label='Validation MSE', alpha=.8, color='#ff7f0e')
plt.title('Mean Squared Error')
plt.legend()
plt.grid(alpha=.3)

# Display the plots
plt.show()

metadata = evaluate_and_plot_model(
    es_model,
    X_val,
    y_val,
    X_test,
    y_test,
    metadata,
    history=es_history,
    patience=patience,
    model_name='Baseline (es)',
    color='#ff7f0e',
    plot_baseline=True
    )


# Calculate the final validation mse
final_val_mse = round(es_history['val_mse'][-(patience+1)], 4)
print(final_val_mse)

# Save the trained model to a file with the mse included in the filename
es_model_filename = f'Feedforward_es_{final_val_mse}.keras'
es_model.save(es_model_filename)

# Delete the model to free up memory resources
del es_model

plot_histories(metadata, baseline=False)
\end{lstlisting}

\subsection{Ridge regression}
Ridge regression may be applied in the following way: 
\begin{lstlisting}[style=Python]
l2_lambda = 5e-4

l2_model = build_model(input_shape, l2_lambda=l2_lambda)
l2_model.summary(expand_nested=True, show_trainable=True)
tfk.utils.plot_model(l2_model, expand_nested=True, show_trainable=True, show_shapes=True, dpi=70)

# Create an EarlyStopping callback
early_stopping = tfk.callbacks.EarlyStopping(
    monitor='val_mse',
    mode='min',
    patience=patience,
    restore_best_weights=True
)

# Store the callback in a list
callbacks = [early_stopping]

# Train the model and store the training history
l2_history = l2_model.fit(
    x = X_train,
    y = y_train,
    validation_data = (X_val, y_val),
    batch_size = batch_size,
    epochs = epochs,
    callbacks = callbacks
).history

# Set the number of initial data points to ignore
ignore = 0

# Create a figure for loss visualization
plt.figure(figsize=(21, 4))

# Plot training and validation loss for L2-regularized model
plt.plot(l2_history['loss'][ignore:], label='Training loss', alpha=.2, color='#2ca02c')
plt.plot(l2_history['val_loss'][ignore:], label='Validation loss', alpha=.8, color='#2ca02c')
plt.title('Loss')
plt.legend()
plt.grid(alpha=.3)

# Create a figure for Mean Squared Error visualization
plt.figure(figsize=(21, 4))

# Plot training and validation MSE for L2-regularized model
plt.plot(l2_history['mse'][ignore:], label='Training MSE', alpha=.2, color='#2ca02c')
plt.plot(l2_history['val_mse'][ignore:], label='Validation MSE', alpha=.8, color='#2ca02c')
plt.title('Mean Squared Error')
plt.legend()
plt.grid(alpha=.3)

# Display the plots
plt.show()

metadata = evaluate_and_plot_model(
    l2_model,
    X_val,
    y_val,
    X_test,
    y_test,
    metadata,
    history=l2_history,
    patience=patience,
    model_name='Ridge',
    color='#2ca02c'
    )

# Calculate the final validation mse
final_val_mse = round(l2_history['val_mse'][-(patience+1)], 4)

# Save the trained model to a file with the mse included in the filename
l2_model_filename = f'Feedforward_l2_{final_val_mse}.keras'
l2_model.save(l2_model_filename)

# Delete the model to free up memory resources
del l2_model
\end{lstlisting}

\subsection{Dropout}
Dropout may be applied in the following way: 
\begin{lstlisting}[style=Python]
dropout_rate = 1/2

dropout_model = build_model(input_shape, dropout_rate=dropout_rate)
dropout_model.summary(expand_nested=True, show_trainable=True)
tfk.utils.plot_model(dropout_model, expand_nested=True, show_trainable=True, show_shapes=True, dpi=70)

# Create an EarlyStopping callback
early_stopping = tfk.callbacks.EarlyStopping(
    monitor='val_mse',
    mode='min',
    patience=patience,
    restore_best_weights=True
)

callbacks = [early_stopping]

dropout_history = dropout_model.fit(
    x = X_train,
    y = y_train,
    validation_data = (X_val, y_val),
    batch_size = batch_size,
    epochs = epochs,
    callbacks = callbacks
).history

# Set the number of initial data points to ignore
ignore = 0

# Create a figure for loss visualization
plt.figure(figsize=(21, 4))

# Plot training and validation loss for model with dropout layers
plt.plot(dropout_history['loss'][ignore:], label='Training loss', alpha=.2, color='#9467bd')
plt.plot(dropout_history['val_loss'][ignore:], label='Validation loss', alpha=.8, color='#9467bd')
plt.title('Loss')
plt.legend()
plt.grid(alpha=.3)

# Create a figure for Mean Squared Error visualization
plt.figure(figsize=(21, 4))

# Plot training and validation MSE for model with dropout layers
plt.plot(dropout_history['mse'][ignore:], label='Training MSE', alpha=.2, color='#9467bd')
plt.plot(dropout_history['val_mse'][ignore:], label='Validation MSE', alpha=.8, color='#9467bd')
plt.title('Mean Squared Error')
plt.legend()
plt.grid(alpha=.3)

# Display the plots
plt.show()

metadata = evaluate_and_plot_model(
    dropout_model,
    X_val,
    y_val,
    X_test,
    y_test,
    metadata,
    history=dropout_history,
    patience=patience,
    model_name='Dropout',
    color='#9467bd'
    )


# Calculate the final validation mse
final_val_mse = round(dropout_history['val_mse'][-(patience+1)], 4)

# Save the trained model to a file with the mse included in the filename
dropout_model_filename = f'Feedforward_do_{final_val_mse}.keras'
dropout_model.save(dropout_model_filename)

# Delete the model to free up memory resources
del dropout_model
\end{lstlisting}

\subsection{Comparison}
We can compare the models in the following way.
\begin{lstlisting}[style=Python]
# Compare all the trainings
plot_histories(metadata, baseline=True)

# Create a bar chart for validation MSE of different models
plt.figure(figsize=(21, 6))
for m in metadata.keys():
    plt.bar(m, metadata[m]['val_score'], color=metadata[m]['color'], alpha=.8)
plt.ylim(0.01, .02)
plt.title('Validation MSE')
plt.grid(alpha=.3, axis='y')
plt.show()
\end{lstlisting}