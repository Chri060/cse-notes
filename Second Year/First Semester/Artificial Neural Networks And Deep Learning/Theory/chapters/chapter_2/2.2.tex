\section{Activation functions}

Typical activation functions are: 
linear: $g(a)=a$ with a derivative of $g^\prime (a)=1$
sigmoid $g(a)=\dfrac{1}{1+e^{-a}}$ with a derivative of $g(a)(1-g(a))$ (veary easy and convenient). 
hyperbolic tangent $g(a)=\dfrac{e^a-e^{-a}}{e^a+e^{-a}}$ with a derivative of $1-g(a)^2$.
the activation function is a design choice. 


\subsection{Output layer}
In regression the output spans thr $\mathbb{R}$ domain: use a linear activation function for the output neuron.

In binary classification, chose according to their coding:
\begin{enumerate}
    \item Two classes $\Omega_0 = -1$, $\Omega_1 = +1$ then use Tanh output activation.
    \item Two classes $\Omega_0 = 0$, $\Omega_1 = 1$ then use Sigmoid output activation (it can be interpreted as class posterior probability)
\end{enumerate}
When dealing with multiple classes (K) use as many neuron as classes
Classes are coded as $\Omega_0 = 0 0 1$ , $\Omega_1 = 0 1 0$ , $\Omega_2 = [1 0 0]$
utput neurons use a softmax unit: 
\[y_k=\dfrac{e^{z_k}}{\sum_ke^{z_k}}=\dfrac{e^{\sum_jw_{kj}h_j(\sum_i^Iw_{ji}x_i)}}{\sum_{k=1}^Ke^{\sum_jw_{kj}h_j(\sum_i^Iw_{ji}x_i)}}\]
Here, $z_k$ is the activation value of the $k$ element.
This function normalize the vector, interpretable as a probability vector.

\subsection{Hidden layer}
Hidden layer 
For all hidden neurons use sigmoid or hyperbolic tangent.
\begin{theorem}
    A single hidden layer feedforward neural network with S shaped activation functions can approximate any measurable function to any desired degree of accuracy on a compact set.
\end{theorem}
Regardless the function we are learning, a single layer can represent it:
Doesn't mean a learning algorithm can find the necessary weights!
In the worse case, an exponential number of hidden units may be required
The layer may have to be unfeasibly large and may fail to learn and generalize
Classification requires just one extra layer







