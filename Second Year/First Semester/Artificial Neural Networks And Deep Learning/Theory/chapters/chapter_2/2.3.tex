\section{Training}

Recall learning a parametric model $y(x_n|\theta)$ in regression or classification.
Given a training set $\mathcal{D}=<x_1,t_1>\cdots<x_N,t_N>$ we want to find model parameters such that for new data $y_n(x_n|\theta)\sim t_n$.
In case of a neural network this can be rewritten as $g(x_n|w)\sim t_n$. 
For this we can minimize 
\[E=\sum_n^N\left(t_n-g(x_n|w)\right)^2\]
Here, $E$ is the sum of squared errors. 
This error With feedforwad neural networks is non linear.

\paragraph*{Nonlinear optimization}
To find the minimum of a generic function, we compute the partial derivatives of the function and set them to zero
\[\dfrac{\partial J(w)}{\partial w}=0\]
Closed-form solutions are practically never available so we can use
iterative solutions (gradient descent):
\begin{enumerate}
    \item Initialize the weights to a random value
    \item Iterate until convergence: 
        \[w^{k+1}=w^k-\eta\dfrac{\partial J(w)}{\partial w}\Bigg|_{w^k}\]
\end{enumerate}
In a function with multiple minima, we iterate starting from an initial
random configuration: 
\[w^{k+1}=w^k-\eta\dfrac{\partial E(w)}{\partial w}\Bigg|_{w^k}\]
The solution in this case depends on where we start from.
Also, to avoid local minima can use momentum: 
\[w^{k+1}=w^k-\eta\dfrac{\partial E(w)}{\partial w}\Bigg|_{w^k}-\alpha\dfrac{\partial E(w)}{\partial w}\Bigg|_{w^k}\]
Use multiple restarts to seek for a proper global minimum.

\begin{example}
    SLIDE 25 to 27
\end{example}

Using all the data points (batch) might be unpractical. 
So, instead of usinf a batch gradient descent: 
SLIDE 28
We use a stochastic gradient descent: 
SLIDE 28 
Use a single sample,
unbiased, but with
high variance
Or you could use a min-batch gradient descent
SLIDE 28










