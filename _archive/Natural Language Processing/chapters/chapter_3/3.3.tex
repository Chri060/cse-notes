\section{Text normalization}

When ranking documents, it's important to normalize for document length. 
Longer documents tend to have a larger vocabulary, which makes it more likely they will contain the query terms.
However, this doesn't necessarily mean they are more relevant to the user's search. In fact, shorter documents with the same term count should often be preferred.

\subsection{Document length normalization}
One simple way to normalize for document length is to divide the term frequency by the document length. 
However, the most common method of normalization uses the L2 norm (also called the Euclidean norm) instead of the L1 norm (which is simply dividing by the document length).

\subsection{Cosine similarity normalization}
In the Vector Space Model, each document is represented as a vector of term frequencies weighted by their inverse document frequency. 
The vector for a document might look like this:
\[\mathbf{d}=(\text{tf}_{1,d}-\text{idf}_1,\dots,\text{tf}_{1,d}-\text{idf}_n)\]
\noindent To compute the similarity between a query and a document, we measure the cosine of the angle between their vectors. 
The cosine similarity formula is:
\[\text{similarity}(\mathbf{d}_1,\mathbf{d}_2)=\dfrac{\mathbf{d}_1\cdot\mathbf{d}_2}{\left\lVert \mathbf{d}_1\right\rVert\left\lVert \mathbf{d}_2\right\rVert  }\]
Here $\mathbf{d}_1$ represents the query vector, and $\mathbf{d}_2$ represents the document vector.
The cosine of the angle is used because it produces a similarity value in the range $[0,1]$. 
To calculate the cosine similarity, the vectors are normalized by their Euclidean (L2) norm, rather than the length of the document in terms of tokens (which would be an L1 norm).

There have been many studies into alternative methods of length normalization: 
\begin{itemize}
    \item \textit{Pivoted Length Normalization} (PVL): aims to retain the beneficial information from longer documents while preventing them from being unfairly favored. 
        The idea behind PVL is that longer documents generally contain more information, but simple length normalization could lose valuable length information. 
        Instead, PLN adjusts the normalization to account for both the document length and the average document length in the corpus. 
    \item \textit{Best Match 25}: builds upon the ideas of TF-IDF and length normalization. 
        The influence of a term on the document's score decreases as its frequency increases. 
        There are parameters which allow fine-tuning based on the corpus.
\end{itemize}