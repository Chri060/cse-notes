\section{Large Language Models}

Language Models (LMs) are models designed to predict the next token in a sequence. As the name suggests, Large Language Models (LLMs) are simply very large versions of these models.
After the Transformer architecture demonstrated its power for language modeling tasks, a race began to build increasingly larger models. 
Although the ultimate limits of scaling are not fully understood, empirical results show that the performance of GPT-style architectures improves approximately logarithmically with both the amount of training data and the total training time.

\subsection{Chatbot}
While large language models (LLMs) and chatbots are closely related, they are not the same. 
LLMs are primarily trained to predict the next token in a sequence of text, while chatbots are fine-tuned specifically for conversational interaction with users.

One major technique for turning an LLM into a chatbot is Reinforcement Learning from Human Feedback (RLHF). 
In this process, the model is exposed to many conversations with real users, receiving feedback on which responses were most appropriate, helpful, or satisfying. 
Over time, the model adjusts its behavior, learning to generate answers that better align with human preferences. 
Rather than simply continuing text, chatbots are designed to produce responses that are more engaging, polite, and useful.

Another important concept is instruction tuning, where a model is trained to perform a variety of tasks by following natural language instructions. 
Instead of optimizing only for token prediction, the model learns to interpret prompts framed as explicit commands or questions. 
This approach makes the model more versatile and better able to generalize across a wide range of user requests.

\subsection{Prompting}
During fine-tuning, language models are trained to engage in structured conversations with users. 
This involves recognizing special tokens that delineate different parts of the dialogue. 
Conversations are typically made up of three types of messages: 
\begin{itemize}
    \item System messages, which define how the chatbot should behave. 
    \item User messages, which contain the user's input or requests
    \item Assistant messages, which represent the chatbot's responses.
\end{itemize}
\noindent Although LLMs fundamentally operate as text-in/text-out systems, all parts of the conversation—past and present—must be serialized into a single text sequence. 
This is done by concatenating messages with the appropriate formatting, often using chat templates that define the structure of the conversation at the token level.

A key component in this setup is the system prompt, which sets the rules for how the chatbot should respond. 
It guides the model on what behaviors to adopt and which topics or tones to avoid. This prompt plays a crucial role in ensuring safe and aligned responses, helping prevent the model from producing offensive or harmful content. 
System prompts are often proprietary and not publicly disclosed, though users have had varying success inferring them by querying models directly.

\subsubsection{Chain-of-Thought reasoning}
Introduced in 2022, chain-of-thought (CoT) prompting helps models perform better on tasks that require step-by-step reasoning. 
By encouraging the model to explain its reasoning as it answers, CoT prompting often leads to more accurate and interpretable results.

Even without providing examples, a simple phrase can trigger this behavior (an approach known as zero-shot CoT prompting). 
Taking it further, users have found that slightly extending this prompt or adding analogies can yield even better performance.

To improve answer quality, some methods involve generating multiple responses and selecting the most common one. 
Alternatively, the model can critique its own responses, regenerating them if it deems its previous answer incorrect. 
This form of self-reflection enhances both performance and trustworthiness.

\subsubsection{Test-Time compute scaling}
Recent models, such as Deepseek-R1, take a more structured approach to reasoning by separating the thinking and answering phases. 
Their outputs are formatted with dedicated tags, allowing the model to explicitly allocate time to reasoning before producing a final response.
This technique, known as test-time compute scaling, encourages the model to spend more computational effort. 

The training process involves exposing the model to difficult questions paired with answers, but without showing the steps needed to solve them. 
Over time, the model learns to generate increasingly detailed reasoning as part of its response, effectively training itself to think when necessary.

In some setups, a second model (possibly larger or more specialized) can be used to verify the final answer or evaluate the quality of the reasoning. 
This additional layer of oversight helps ensure that responses are not only accurate but also well justified—especially useful when ground-truth answers aren't available.

\subsection{Limitations}
The major limitations of LLMs are: 
\begin{itemize}
    \item \textit{Hallucinations}: LLMs are known to sometimes produce fabricated or inaccurate information. 
        These models are primarily optimized to generate content that is engaging and human-like, which can sometimes conflict with the objective of delivering strictly factual and truthful information.
        Hallucinations can manifest in many forms, including content that is not faithful to known facts, incoherent with provided data, or not logically derivable from available information.
    \item \textit{Limited reasoning}: earlier versions of LLMs exhibited significant reasoning limitations. 
        These issues sometimes arose from tokenization errors, or from inherent limitations related to the model's architecture. 
        As a result, the models could struggle with multi-step logical reasoning or fail to maintain consistency across complex tasks.
    \item \textit{Lack of robustness}: prompt sensitivity remains an active area of research. 
        Small variations in prompt wording can lead to disproportionately large differences in model performance, making the behavior of LLMs sometimes unpredictable and frustrating for users.
        Generally, the clearer, more structured, and less ambiguous the prompt, the more reliable the model's output. 
        However, ensuring consistent performance across a wide range of tasks is still a major challenge.
    \item\textit{Jailbreak}: some users attempt to bypass the chatbot's safeguards. 
        Ethics researchers may do this to test whether the model can be prompted to generate harmful or inappropriate content despite its safety constraints. 
        Security researchers, on the other hand, may try to elicit memorized training data, which poses a significant privacy and security risk. 
        This risk is exacerbated when user-provided content is incorporated into model fine-tuning.
\end{itemize}

\subsection{Scaling laws}
Numerous studies have explored how the performance of large language models scales with various factors. 
A near-linear relationship has been observed between model performance and the logarithm of: computation time, training dataset size, and number of model parameters.
One key insight comes from the Chinchilla scaling law, which provides guidance for training Transformer-based language models efficiently. 
It suggests that, given a fixed computational budget (measured in FLOPs), optimal performance is achieved when the number of model parameters ($N$) and the number of training tokens ($D$) are scaled in roughly equal proportions.

Additionally, when training large models, it is crucial to adjust the learning rate appropriately. 
As model size increases, the learning rate should typically decrease proportionally. 
Various techniques and heuristics exist to estimate an optimal learning rate schedule based on model size and training dynamics.

\subsection{Efficiency improvements}
Recent research has introduced several architectural and algorithmic enhancements to improve the efficiency and scalability of Transformer-based models:
\begin{itemize}
    \item \textit{Layer Input Normalization}: normalization is applied to the input of each sub-layer (self-attention and feed-forward blocks), rather than to the residual stream, which can improve training stability and model performance.
    \item \textit{Positional Embeddings}: improvements form Absolute Positional Embeddings to Rotational Positional Embeddings:
        \begin{itemize}
            \item \textit{Absolute Positional Embeddings}: the original Transformer used sinusoidal position embeddings added to token embeddings. 
                Learned absolute embeddings were also explored but showed limited gains.
                Absolute encodings fix the maximum sequence length, reducing generalization to longer contexts.
            \item \textit{Relative Positional Embeddings}: used in models like T5, a learned bias is added to the query-key similarity score, enabling better generalization to varying context lengths.
                 However, this approach can slow down self-attention and complicate caching mechanisms.
            \item \textit{Rotational Positional Embeddings}: RoPE encodes position by rotating token embeddings. 
                This method combines the computational simplicity of absolute encodings with the benefits of relative positioning, as vector dot products become dependent only on relative positions.
        \end{itemize}
    \item \textit{Grouped Self-Attention}: a memory-efficient variant where queries are shared across subsets of attention heads, reducing the number of parameters and computational cost.
    \item \textit{Enhanced MLP Layers}: new architectures add an additional up-projection matrix with a linear activation, creating high-dimensional hidden representations (typically $\sim4d$) that are summed before projection back to the original dimensionality. 
        This more complex feed-forward structure improves model expressiveness.
    \item \textit{Mixture of Experts}: techniques like the Switch Transformer use a routing mechanism to select a subset of expert FFNNs for each token. 
        This allows the model to scale to billions of parameters without a corresponding increase in computation per token, thus improving performance while maintaining efficiency.
    \item \textit{Sliding Window Attention for Longer Contexts}: recent methods use a sliding window approach to attention, enabling models to process much longer contexts. 
        This allows for full-document understanding and greatly expands application domains.
\end{itemize}
\noindent For the efficient hardware deployment we have the following techniques:
\begin{itemize}
    \item \textit{Low-Bit quantization}: transformers typically require GPUs for fast inference, but GPU memory is limited. 
        Model weights are commonly quantized to lower precisions to save memory and improve inference speed.
        While quantization reduces accuracy slightly, it enables deployment of larger models on constrained hardware.
    \item \textit{Low-Rank Adaptation}: fine-tuning large models is memory-intensive and often infeasible on standard hardware. 
        LoRA addresses this by learning low-rank updates to the weight matrices. 
        A weight matrix $W$ is adjusted via a low-rank decomposition: $W^\prime = W + AB$, where $A$ and $B$ are much smaller matrices (with $B$ initially zero). 
        This drastically reduces the number of trainable parameters and memory required for fine-tuning, making it practical for smaller datasets and limited compute environments.
\end{itemize}

\subsection{Integration}
\paragraph*{Retrieval-Augmented Generation (RAG)}
Traditional LLM-based chatbots are limited by the information encoded within their model parameters during training. 
This creates challenges when responding to: domain-specific queries requiring expert knowledge, and questions about recent events or updates unavailable at training time.
RAG models address this limitation by augmenting LLMs with real-time retrieval capabilities. 
They can search external knowledge sources and then generate responses grounded in the retrieved content. 
This allows LLMs to remain up-to-date and provide more accurate, context-specific answers.

\paragraph*{Agentic AI}
Agentic AI builds on the LLM's ability to reason about tasks and goals, enabling models to make decisions and take actions in pursuit of user-defined objectives. 
These systems plan and execute actions to achieve a goal and can modify external systems—for instance, updating a user's address in a database based on a natural language request.
Frameworks such as \texttt{LangChain} facilitate the development of such agent-based applications by integrating LLMs with tools, memory, and actions in a programmable environment.