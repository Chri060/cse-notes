\section{Data architecture}

A well-defined data schema ensures typing, coherence, and uniformity within a system, serving as the foundation for reliable data management. 
Central to this are transactions, which provide the mechanism to perform atomic and consistent operations on the database.
\begin{definition}[\textit{Transaction}]
    A transaction is the smallest unit of work executed by an application within a database system. 
\end{definition}
\noindent A transaction ensures that operations are executed in a controlled and reliable manner.
Transactions are demarcated by two key commands:
\begin{itemize}
    \item \texttt{BEGIN TRANSACTION}: marks the start of the transaction.
    \item \texttt{END TRANSACTION}: concludes the transaction, after which one of the following outcomes occurs: 
        \begin{itemize}
            \item \texttt{COMMIT}: finalizes the transaction, making all changes permanent.
            \item \texttt{ROLLBACK}: aborts the transaction, reverting the system to the state it was in before the transaction began.
        \end{itemize}
\end{itemize}
\noindent Online Transaction Processing systems play a crucial role in managing and executing transactions for multiple, concurrent applications.
\begin{definition}[\textit{Online Transaction Processing}]
    An Online Transaction Processing system is a platform that defines and processes transactions on behalf of various applications running simultaneously.
\end{definition}

\subsection{Partitioning}
Data partitioning is an essential technique for achieving scalability and efficient distribution of data in large-scale database systems. 
By dividing the data into smaller chunks, the system can distribute the load across multiple storage nodes, thus enhancing performance and enabling horizontal scaling. 
The primary goal of partitioning is to optimize data retrieval and improve the overall efficiency of database operations.

There are two primary methods for partitioning data:
\begin{itemize}
    \item \textit{Horizontal partitioning} (sharding): different rows are stored across separate nodes.
    This is particularly useful in large-scale systems, where distributing data across multiple machines helps spread the load, enabling the system to scale out horizontally.
    \item \textit{Vertical partitioning}: different columns of a table are stored on different nodes.
    This method is beneficial when certain columns are accessed more frequently than others, allowing for better optimization of data retrieval.
\end{itemize}
By distributing the data, both read and write operations can be performed more quickly, as each node handles a smaller subset of the total data.
Additionally, partitioning reduces memory overhead, as it ensures that only relevant portions of the data are loaded into memory at any given time. 
Partitioning also facilitates scalability, enabling a system to grow by adding more nodes to handle increasing data volume or traffic.

One of the major risks is potential data loss or inconsistency, particularly if partitions are not managed properly. 
Node failures or mismanagement of partitions can lead to problems, especially when ensuring that the data remains consistent across different nodes. 
Moreover, managing partitions adds complexity to the database system, requiring sophisticated strategies to maintain data integrity and fault tolerance.

\subsection{Replication}
Data replication is a key strategy for ensuring fault-tolerance and reliability within distributed database systems. 
The core objective of replication is to maintain multiple copies of the database across various nodes in the system.
This redundancy provides robust protection against data loss, as the failure of one node or even multiple nodes does not result in the loss of the data, since other copies are available.

One of the main advantages of replication is that it enhances data read performance. 
Since multiple copies of the data are distributed across different nodes, read requests can be served from the nearest or least-loaded node, leading to faster response times. 
Additionally, replication increases the overall reliability of the system, as the risk of losing all copies of the data is greatly diminished.

Replication introduces higher network overhead, as nodes must continuously synchronize with one another to ensure that all copies remain consistent.
Furthermore, replication increases memory overhead because each node in the system stores a full duplicate of the dataset. 

\subsection{Scalability}
Scalability is a fundamental goal when designing systems to handle varying loads. 
In modern systems, scalability often ties directly to the concept of elasticity.
\begin{definition}[\textit{Elasticity}]
    Elasticity refers to a system's ability to automatically adjust resources based on demand, either scaling up to accommodate higher loads or scaling down when demand decreases.
\end{definition}
\noindent This capability ensures that the system operates efficiently without wasting resources or compromising performance. 
Elastic systems are essential in environments where workloads fluctuate, as they allow for the dynamic allocation of resources to maintain consistent performance while optimizing costs.

\subsection{Data ingestion}
Data ingestion is the process by which data is collected, imported, transferred, and loaded into a system for storage and future analysis. 
It involves extracting data from various sources, and then loading it into a storage system, often after making adjustments to ensure it is in a format that enhances storage efficiency and accessibility.
This process may require transforming data to meet the system's specific requirements, which can include normalization, encoding, or formatting changes to ensure consistency across the dataset.

\paragraph*{Wrangling}
Data wrangling is a critical step in the data ingestion pipeline that focuses on cleaning and transforming raw, unstructured data into a structured and usable format. 
This process involves several stages, including identifying and correcting errors, handling missing values, filtering out irrelevant information, and converting data into a format that is ready for analysis. 
The goal of data wrangling is to prepare the data for analysis by ensuring its quality, consistency, and structure.