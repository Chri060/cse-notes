\subsection{Online SLAM}

For Extended Kalman Filter SLAM, we define a Belief matrix. 
Considering a map with $N$ landmarks, we need to define a $(3+2N)$-dimensional Gaussian with the following Belief matrix:
\[\text{Bel}(x_t,m_t)=\left\langle \begin{bmatrix}
    x \\ y \\ \theta \\ l_1 \\ l_2 \\ \vdots \\ l_N
\end{bmatrix},\begin{bmatrix}
    \sigma_x^2 & \sigma_{xy} & \sigma_{x\theta} & \sigma_{xl_1} & \sigma_{xl_2} & \cdots & \sigma_{xl_N} \\
    \sigma_{yx} & \sigma_{y}^2 & \sigma_{y\theta} & \sigma_{yl_1} & \sigma_{yl_2} & \cdots & \sigma_{yl_N} \\
    \sigma_{\theta x} & \sigma_{\theta y} & \sigma_{\theta}^2 & \sigma_{\theta l_1} & \sigma_{\theta l_2} & \cdots & \sigma_{\theta l_N} \\
    \sigma_{xl_1} & \sigma_{yl_1} & \sigma_{\theta l_1} & \sigma_{l_1}^2 & \sigma_{l_1l_2} & \cdots & \sigma_{l_1l_N} \\
    \sigma_{xl_2} & \sigma_{yl_2} & \sigma_{\theta l_2} & \sigma_{l_1l_2} & \sigma_{l_2}^2 & \cdots & \sigma_{l_2l_N} \\
    \vdots & \vdots & \vdots & \vdots & \vdots & \ddots & \vdots \\
    \sigma_{xl_N} & \sigma_{yl_N} & \sigma_{\theta l_N} & \sigma_{l_1l_N} & \sigma_{l_2l_N}& \cdots & \sigma_{l_N}^2
\end{bmatrix}\right\rangle \]
Here, $\begin{bmatrix} x & y & \theta \end{bmatrix}^T$ represents the robot's position, and $\begin{bmatrix} l_1 & \cdots & l_N \end{bmatrix}^T$ represents the positions of landmarks.
The top left and bottom right parts represent the covariances of the robot poses and the landmark positions, respectively. 
The other two sub-matrices represent the covariances between the robot and the landmarks.
The covariance between the robot pose and landmark position is not null since the uncertainty lies in the measurements made from the robot itself.

Additionally, we assume the robot's position to be a linear function $x_t=\mathbf{A}_tx_{t-1}+\text{B}_tu(t)+\varepsilon_t$. 
The measurement is also assumed to be a linear function $z_t=\mathbf{C}_tx_t+\delta_t$. 
Therefore, the belief is updated as follows: 
\[\begin{cases}
    \Pr(x_t|u_t,x_{t-1})=\mathcal{N}(x_t;\mathbf{A}_tx_{t-1}+\text{B}_tu(t),\mathbf{R}_t) \\
    \Pr(z_t|x_t)=\mathcal{N}\left(z_t;\mathbf{C}_tx_t,\mathbf{Q}_t\right)
\end{cases}\]

\subsection{Bayes filter algorithm for EKF SLAM}
The Bayes filter algorithm for Simultaneous Localization and Mapping mirrors the one used for localization.

\subsection{Kalman filter algorithm for EKF SLAM}
The Kalman filter algorithm for Simultaneous Localization and Mapping closely resembles the one used for localization. 
The key difference lies in the handling of the Belief matrix, which contains all the covariances and can be updated dynamically during the algorithm's execution. 
In terms of complexity, there isn't much deviation from standard EKF, but the state dimension increases.

\paragraph*{Extended Kalman filter algorithm}
To address complexity concerns, we can approximate the SLAM posterior with a high-dimensional Gaussian using the Extended Kalman filter algorithm.

\paragraph*{Properties of Kalman Filter SLAM}
The linear Kalman filter for SLAM is characterized by the following theorems.
\begin{theorem}
    The determinant of any sub-matrix of the map covariance matrix decreases monotonically as successive observations are made.
\end{theorem}
\begin{theorem}
    In the limit, the landmark estimates become fully correlated.
\end{theorem}
These theorems have several implications:
\begin{itemize}
    \item Quadratic complexity in the number of landmarks: $\mathcal{O}(n^2)$.
    \item Convergence outcomes are established for the linear scenario.
    \item Divergence may occur when dealing with significant nonlinearities.
    \item Successful applications have been observed in large-scale environments.
    \item Computational complexity is mitigated through the use of approximations.
\end{itemize}
Currently, EKF SLAM finds its niche in sparse landmark-based maps, where it exhibits its highest efficiency.

\paragraph*{Monocular SLAM}
Monocular SLAM is a technique used in robotics and computer vision to create maps of an environment while simultaneously determining the position and orientation of the observer within that environment using a single camera.
This is achieved by correlating visual features in successive frames of video to track movement and construct a map of the surroundings.
Initially, monocular SLAM algorithms relied on Extended Kalman Filter (EKF) SLAM to estimate the pose of the camera. 
This technique necessitates that the images captured contain identifiable landmarks or features.

\subsection{Summary}
While EKF-SLAM demonstrates effectiveness, it relies on linearized models of nonlinear motion and observation, thus inheriting various limitations. 
Computational demands increase significantly, scaling quadratically with the number of landmarks. 
Several potential solutions have been proposed:
\begin{itemize}
    \item Employing local sub-maps. 
    \item Utilizing sparse links (correlations).
    \item Implementing sparse extended information filters.
    \item Applying Rao-Blackwellisation (FastSLAM), which represents nonlinear processes and non-Gaussian uncertainty, thereby reducing computational burden.
\end{itemize}