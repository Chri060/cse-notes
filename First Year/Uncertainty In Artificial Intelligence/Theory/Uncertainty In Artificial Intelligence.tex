\documentclass[12pt, a4paper]{report}
\usepackage{graphicx, array, amsthm, amssymb, amsmath, algorithm, algpseudocode, float, xcolor, thmtools, thmbox}
\usepackage[english]{babel}

\makeatletter
\renewcommand\thmbox@headstyle[2]{\bfseries #1}
\makeatother
\newtheorem[style=M,bodystyle=\normalfont]{theorem}{Theorem}
\newtheorem[style=M,bodystyle=\normalfont]{corollary}{Corollary}
\newtheorem[style=M,bodystyle=\normalfont]{lemma}{Lemma}
\newtheorem[style=M,bodystyle=\normalfont]{definition}{Definition}


\title{Uncertainty In Artificial Intelligence \\ \textit{Theory}}
\author{Christian Rossi}
\date{Academic Year 2023-2024}

\begin{document}

\maketitle

\newpage

\begin{abstract}
    The topics of the course are:
    \begin{itemize}
        \item Uncertainty sources that affect models: typology, issues, and modeling approaches.
        \item Measure-based uncertainty modeling.
        \item Logic-based uncertainty modeling.
        \item Fuzzy models: fuzzy sets, fuzzy logic, fuzzy rules, motivations for fuzzy modeling, tools for fuzzy systems, design 
            of fuzzy systems, applications.
        \item Bayesian networks: basics, design, learning, evaluation, applications.
        \item Hidden Markov Models: basics, design, learning, evaluation, applications.
        \item Applications: motivations, choices, models, case studies.
    \end{itemize}
\end{abstract}

\newpage

\tableofcontents

\newpage

\chapter{Introduction}
    \section{Definition}
    \begin{definition}[Uncertainty]
        \emph{Uncertainty} refers to epistemic situations involving imperfect or unknown information. It applies to predictions 
        of future events, to physical measurements that are already made, or to the unknown. 
    \end{definition}
    Uncertainty arises in partially observable or stochastic environments, as well as due to ignorance, indolence, or both. It arises 
    in any number of fields, including insurance, philosophy, physics, statistics, economics, finance, medicine, psychology, sociology, 
    engineering, metrology, meteorology, ecology and information science.
    
    The lack of certainty, a state of limited knowledge where it is impossible to exactly describe the existing state, a future outcome,
    or more than one possible outcome. This puts in evidence that uncertainty is related to the need of describing a piece of reality.

    \section{Modelling}
    Modelling is at the base of our life: the way we interact with the world is through models that interpret data coming from sensors
    and generate knowledge and actions. Modelling is also the way we may represent entities in a computer and possibly making it reasoning
    on them.
    \begin{definition}[Model]
        A \emph{model} is a representation of some entity, defined for a specific purpose. A model captures only the aspects of the entity
        modelled that are relevant for the purpose. A model is necessarily different from the modelled entity. So, intrinsic to modelling
        are all sort of uncertainties.
    \end{definition}
    All Artificial Intelligence applications are based on models, either defined by somebody or learned. These models are represented In
    different ways, but share uncertainty issue mainly on inputs. 

    \section{Uncertainty classification}
    The uncertainty can be of two main types: 
    \begin{itemize}
        \item Epistemic uncertainty: it is due to things one could in principle know but does not in practice. This may because a 
            measurement is not accurate, because the model neglects certain effects, or because particular data have been deliberately
            hidden. It is also known as systematic uncertainty and can in principle be reduced by enriching the model.      
        \item Aleatoric uncertainty: it is representative of unknown unknowns that differ each time we run the same experiment. 
            Aleatoric uncertainty is also known as statistical uncertainty, since only statistical information can describe it. This may
            also depend on the way we get and elaborate data. In general, it is present when the model is missing some aspects.
    \end{itemize}
    The sources of uncertainty can be: 
    \begin{itemize}
        \item Parameter: it comes from the model parameters, whose exact values are unknown to experimentalists and cannot be controlled
            in experiments, or whose values cannot be inferred by statistical methods. 
        \item Parametric variability: it comes from the variability of input variables of the model. 
        \item Structural: also known as model inadequacy, model bias, or model discrepancy, this comes from the lack of knowledge of the
            problem.
        \item Algorithmic: also known as numerical uncertainty, or discrete uncertainty. This type comes from numerical errors and
            numerical approximations in the implementation of the computer model. 
        \item Experimental: also known as observation error, this comes from the variability of experimental measurements.
        \item Interpolation: this comes from a lack of variable data collected from computer model simulations and/or experimental 
            measurements. 
    \end{itemize}

    \section{Uncertainty modelling}
    The type of uncertainty model depends on the type of uncertainty, its sources and the information we have in uncertainty and mostly
    has to do with qualification and quantification of uncertainty. The possible models for uncertainty are: statistical, logical and
    cognitive.

    Artificial Intelligence and Machine Learning technologies are based on models that include uncertainty models of these sorts, essential
    not only for the implementation of effective models, but also to define learning models able to cope with complex situations, and 
    to evaluate the quality of learned/developed models. 
    There are two major types of problems in uncertainty quantification: 
    \begin{itemize}
        \item Forward propagation of uncertainty: the various sources of uncertainty are propagated through the model to predict the overall
            uncertainty in the system response:
            \begin{itemize}
                \item To evaluate low-order moments of the outputs (mean and variance).
                \item To evaluate the reliability of the outputs.
                \item To assess the complete probability distribution of the outputs. 
            \end{itemize}
            This is what is done also in Bayesian networks and graphical models. 
        \item Inverse assessment of model uncertainty and parameter uncertainty, where the model parameters are calibrated simultaneously
            using test data: given some experimental measurements of a system and some results from its mathematical model, inverse 
            uncertainty quantification estimates the discrepancy between the experiment and the mathematical model (bias correction) and
            estimates the values of unknown parameters in the model if there are any (parameter calibration).
    \end{itemize}
    The models used in Artificial Intelligence can be classified in three main types: 
    \begin{itemize}
        \item Symbolic models: elements of the models are expressed as terms related to entities to be modelled. The state of the world is
            represented by facts expressed in formal languages close to natural languages.
        \item Sub-symbolic models: elements of the models are expressed by code. 
        \item Black-box models: the model can be computed and possibly investigated, but it is only regarded as a computational way to map
            inputs to outputs. 
    \end{itemize}
    For symbolic models a fact is true in a model if it is possible to collect enough evidence to support it. The only really true facts 
    are the ones true by definition. All the others may be supported by evidence. 
    
    \section{Ignorance management}
    There are many potential sources of ignorance when reasoning in the real world:
    \begin{itemize}
        \item Insufficient data.
        \item Biased data: data are collected by sensors affected by errors. 
        \item Variable data: data are collected by imprecise sensors.
        \item Reliability of data. 
        \item Fuzzyness. 
        \item Reliability of the model: depends on the model design, implementation and parametrization. 
        \item Incompleteness of the model. 
    \end{itemize}
    \begin{example}
        Let's consider the sentence "The elephant weighs 2 tons". This can be interpreted in various ways, each slightly different:
        \begin{itemize}
            \item The elephant weighs exactly 2 tons.
            \item The elephant weighs 2 tons ± 10 kg, given the resolution of the weight scales of the instrument.
            \item The elephant weighs approximately 2 tons, but we cannot say anything more precise.
            \item We are not sure about any previous sentence because we do not have enough evidence.
        \end{itemize}
    \end{example}
    \begin{figure}[H]
        \centering
        \includegraphics[width=1\linewidth]{images/smithson.png}
        \caption{Smithson's taxonomy of ignorance and uncertainty}
    \end{figure}
    To model ignorance most often it is decided to associate measures of some aspects. Let's distinguish between two aspects: 
    \begin{itemize}
        \item The type of representation: numbers, labels, intervals, \dots
        \item The represented ignorance that we would like to model: i.e, probability, reliability, subjective evaluation, \dots
    \end{itemize}

    The probability is represented with numbers between zero and one, and a well-established set of rules and properties are associated 
    to its management, among which, given a set of alternative hypothesis: 
    \begin{itemize}
        \item The sum of their probabilities should be one. 
        \item The probability a posteriori of a hypothesis $h_i$ given some evidence $e$ is given by the Bayes theorem:
            \[P(h_i \mid e)=\frac{P(e \mid h_i)P(h_i)}{P(e)}\]
    \end{itemize}
    Probability was used, for example, in the MYCIN that was one of the first expert systems, aimed at diagnosing blood illness. They 
    modeled certainty by considering two numerical factors: 
    \begin{itemize}
        \item Measure of increased Belief: $MB=\frac{P(\frac{h}{e})-P(h)}{1-P(h)}$.
        \item Measure of decreased Disbelief: $MD=\frac{P(h)-P(\frac{h}{e})}{P(h)}$.
    \end{itemize}
    The measure of a statement is given by the certain factor:
    \[CF=MB-MD \in [-1;1]\]
    The main hypothesis for this solution is that the number given as $MB$ and $MD$ are not statistical probabilities, but subjective
    probabilities, provided by different experts and combined by rules (this may be ambiguous). 

    Compared to probabilities, linguistic terms are less ambiguous than numbers. Using a limited set of labels it is possible to associate
    to statements subjective evaluation, on which it is relatively easy to make subjective judgements converge. Then, a computational 
    mechanism is needed to define how to combine labels. This is done by using fuzzy systems, that are a representation of truth  of a 
    statement in linguistic terms, as evaluation of its fuzzyness. 
    
    \newpage

    \chapter{Fuzzy sets}
    \section{History}
    Fuzzy sets have been defined by Lotfi Zadeh in 1965 as a tool to model approximate concepts. In 1972 the first linguistic fuzzy
    controllers has been implemented. Around 1980 the fuzzy were used frequently worldwide. In the Nineties there were a massive 
    diffusion of fuzzy controllers in various end-user goods. Today, fuzzy systems are the kernel of many intelligent devices. 

    \section{Fuzzy membership function}
    A "crisp" set is defined by a boolean membership function on some property on the considered elements. Instead, a "fuzzy" set is
    a set whose membership function that ranges in the values between zero and one.
    \begin{definition}[Membership function]
        A \emph{membership function} defines a set, by defining the degree of membership of an element of the universe of discourse 
        to the set. A name is given to the set to make it possible to refer to it: this is usually called \emph{label}. Fuzzy sets can 
        also be defined with a variable with discrete values. 
    \end{definition}
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.50\linewidth]{images/function.png}
        \caption{Example of a membership function}
    \end{figure}
    To define a membership function we have to (according to the purpose of the model and the available data):
    \begin{enumerate}
        \item Select a variable on which the membership function will be defined. 
        \item Define the range of the variable.
        \item Identify the fuzzy sets needed for the application and define the labels. 
        \item For each fuzzy set identify characteristic points for the membership function.
        \item Define the shape of the membership function.
        \item Check if the membership function is correct.
    \end{enumerate}
    The shapes of the membership function can be chosen arbitrarily. The choice of the shape modify the smoothness of the transition 
    between two labels (i.e., in intervals (horizontal shape) the transition is immediate). 
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.75\linewidth]{images/shape.png}
        \caption{Possible shapes for a membership function}
    \end{figure}
    \begin{definition}
        A set of fuzzy sets fully covering the universe of discourse is called \emph{frame of cognition}. The properties of this set are:
        \begin{itemize}
            \item Coverage: each element of the universe of discourse us assigned to at least one granule with membership greater or equal 
                than zero.
            \item Uni-modality of fuzzy sets: there is a unique set of values for each granule with maximum membership. 
        \end{itemize}
    \end{definition}
    \begin{definition}
        A frame of cognition for which the sum  of the membership values of each value of the base variable is equal to one is called 
        a \emph{fuzzy partition}. 
    \end{definition}
    \begin{definition}
        The \emph{$\alpha$-cut} of a fuzzy set is the "crisp" set of the values of $x$ such that $\mu(x) \geq \alpha$:
        \[\alpha_\mu(x)=\{x \mid \mu(x) \geq \alpha\}\]
    \end{definition}
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.5\linewidth]{images/alpha.png}
        \caption{Alpha-cut of a membership function}
    \end{figure}
    \begin{definition}[H]
        The \emph{support} of a fuzzy set is the "crisp" set of values $x$ such that $\mu_f(x)>0$ is the \emph{support} of the fuzzy set
        $f$ on the universe $X$.
    \end{definition}
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.5\linewidth]{images/support.png}
        \caption{Support of a membership function}
    \end{figure}
    \begin{definition}
        The height $h_f$ of a fuzzy set $f$ on the universe $X$ is the highest membership degree of an element of $X$ to the fuzzy set:
        \[h_f(X)=\max_{x \in X}\mu_f(x)\]
        A fuzzy set is normal if, and only if, $h_f(X)=1$.
    \end{definition}
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.5\linewidth]{images/height.png}
        \caption{Height of a membership function}
    \end{figure}
    \begin{definition}
        A fuzzy set is \emph{convex} if and only if 
        \[\mu[\lambda x_1+(1-\lambda)x_2] \geq \min [\mu(x_1),\mu(x_2)]\]
        for any $(x_1,x_2) \in \mathbb{R}$ and any $\lambda \in [0,1]$.
    \end{definition}
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.75\linewidth]{images/convex.png}
        \caption{Graphical difference between a convex and a not convex set}
    \end{figure}
    The particular fuzzy sets are: singleton (a fuzzy set with exactly one member) and interval (a fuzzy set whose members have all membership
    equals to one). The possible operations on the fuzzy sets are: 
    \begin{itemize}
        \item Complement: $\mu_{\bar{f}}(x)=1-\mu_f(x)$.
        \item Union: $\mu_{f_1 \cup f_2}(x)=\max [\mu_{f_1}(x),\mu_{f_2}(x)]$.
        \item Intersection: $\mu_{f_1 \cap f_2}(x)=\min [\mu_{f_1}(x),\mu_{f_2}(x)]$.
    \end{itemize}

    \newpage

    \chapter{Fuzzy logic}
    \section{Introduction}
    Logic is a tool that has been used since a thousand of years to formally represent knowledge. There are many types of logic: 
    \begin{itemize}
        \item Propositional: truth values for proposition.
        \item First order: truth values for predicates (with variables and quantifiers).
        \item Second order: predicates of predicates.
    \end{itemize}
    These types of logic are binary. We may notice that the meaning of the terms in these logics is not defined together with the formalism,
    and this is not needed to make the logic work.

    \section{Propositional logic}
    Propositional logics are concerned with propositional operators which may be applied to one or more propositions giving new propositions.
    The accent is on the truth value of propositions and on how these truth values are composed.
    \begin{definition}
        A logic is \emph{truth functional} if the truth value of a compound sentence depends only on the truth values of the consistent atomic 
        sentences, not on their meaning or structure. For such a logic the only important question about propositions is what truth values may
        have.
    \end{definition}
    In a classical, boolean or two-valued logic every proposition is either true or false and no other feature of the proposition is relevant.

    The main operators in the propositional logics are: conjunction ($\land$), disjunction ($\lor$) and negation ($\lnot$).

    \section{First order predicate logic}
    The first order logic is the same as propositional logic augmented with the possibility to define predicates on variables. Furthermore, 
    existential ($\exists$) and universal ($\forall$) quantifiers are defined. 
    In predicate logics it is possible to infer the truth value of a proposition by inferential mechanisms, such as Modus Ponens. 
    \begin{example}[Inference]
        Given the sentences: "All man are mortal" and "Socrates is a man" we can infer that "Socrates is mortal".
    \end{example}
    Inference is used to model a mechanism that we have in our minds to store a reduced amount of information and set a mechanism that can be 
    applied to derive from information other information, to face everyday situations.
    \begin{definition}
        Information and potential relationship together compose what we call \emph{knowledge}.
    \end{definition}
    
    \section{Many-valued logics}
    Aristotle already had put in evidence problems about the validity of classical logic as a knowledge representation tool. For instance, it is
    difficult to state the truth value of a proposition in the future. To solve this problem, let's introduce a third value (i.e., 0.5) for the 
    undefined situation and define a three-valued logic. From this to an infinite set of truth values there is just a small step.
    
    Infinite-value logics considers a continuum of truth values between zero and one for example. 

    \begin{example}[Logic L1, Łukasiewicz(1930)]
        The main rules in this type of infinite-value logic are: 
        \begin{itemize}
            \item $T(\lnot a)=1-T(a)$.
            \item $T(a \land b)=\min (T(a),T(b))$.
            \item $T(a \lor b)=\max (T(a),T(b))$.
            \item $T(a \implies b)=\min (1, 1+T(b)-T(a))$.
            \item $T(a \Leftrightarrow b)=1-\left\lvert T(a)-T(b) \right\rvert$.
        \end{itemize}
    \end{example}

    This innovations bring a change in the society: things are no longer stated as true or false, probability (kolmogorov, 1929) and 
    stochasticity (Markov, 1906) became the way to represent the new approach to science and life. 

    The difference between classical logic L2 and many-valued logic L1 are the following: 
    \begin{itemize}
        \item L1 is isomorphic to the fuzzy set theory with standard operators as the classical logic L2 is isomorphic to the set theory.
        \item Tautologies are true by definition, and are used to prove theorems, so to prove the truth of an inferential chain. 
            Some tautologies valid in L2 are no longer valid in L1, for example:
            \begin{itemize}
                \item Third excluded law ($T(a \lor \lnot a)=1$)
                \item Non-contradiction law ($T(a \land \lnot a)=0$).
            \end{itemize}
    \end{itemize}

    The sentence "I'm a liar" would be a paradox in classical logic, if we give a meaning to the term "liar", since no formula can have the
    same truth value of its negation. This may not be so in many-valued logics. In Łukasiewicz logic, for instance, it can be that the truth 
    value of a sentence is 0.5, and that its negation is the same, so the proposition is consistent with the axioms, and it is not a paradox. 

    \section{Fuzzy logic}
    Fuzzy logic is an infinite-valued logic, with truth values in $[0 \dots 1]$ and prepositions are expressed as: 
    \[A \: is \: L\]
    where: 
    \begin{itemize}
        \item $A$ is a linguistic variable.
        \item $L$ is a label denoting a fuzzy set.
    \end{itemize}

    Formally, a linguistic variable is defined by a 5-tuple $(X,T(X),U,G,M)$, where: 
    \begin{itemize}
        \item $X$ is the name of the variable.
        \item $T(X)$ is the set of term for $X$, each corresponding to a fuzzy variable denoted by $T(X)$ and ranging on $U$.
        \item $U$ is the universe of discourse defined on a base variable $u$.
        \item $G$ is the syntactic rule used to generate the interpretation $X$ of each value $u$.
        \item $M$ is the semantic rule used to associate to $X$ its meaning.
    \end{itemize}
    \begin{example}[Linguistic variable for age]
        We can define a linguistic variable for the age in the following way:
        \begin{itemize}
            \item $X$ is a linguistic variable labelled "age".
            \item $U=[0 \dots 100]$.
            \item $T(X)={old, middle-aged, young, \dots}$.
            \item $u=[0 \dots +\infty]$.
            \item $M$ is the definition in terms of fuzzy sets of the values of $X$.
            \item $G$ is the fuzzy matching interpretation of $u$.
        \end{itemize}
    \end{example}

    Now that we have defined the linguistic variable it is possible to write a simple proposition in the following way: 
    \[p\: : \: X \: is \: F\]
    where:
    \begin{itemize}
        \item $X$ is a linguistic variable.
        \item $F$ is the label of a fuzzy set, defined on $U$, which represent a fuzzy predicate.
        \item $\mu_F(x)$ is the membership function defining $F$, and it is interpreted as truth value for the preposition $p$ ($T(p)=\mu_F(x)$).
    \end{itemize}
    Therefore, the truth value of the preposition $P$ is a fuzzy set defined on $[0 \dots 1]$.
    \begin{example}
        Given the simple proposition "p:temperature is high", where $X$ is temperature and $F$ is high we can find the truth value of this 
        preposition using the graph of the membership function given:
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.5\linewidth]{images/temperature.png}
        \end{figure}
        So, the truth value of the given proposition is $0.75$.
    \end{example}

    It is also possible to define qualified, non-conditional propositions with this syntax: 
    \[p \: : \: (X \: is \: F) \: is \: S\]
    where:
    \begin{itemize}
        \item $S$ is a fuzzy truth qualifier.
        \item $F$ is a fuzzy set.
        \item $p$ is truth qualified.
    \end{itemize}
    \begin{example}
        Given the conditional proposition "p:age of Tina is young is very true", where $X$ is age, $F$ is young and $S$ is very true we can 
        find the truth value of this preposition using the graph of the membership function given:
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.75\linewidth]{images/age.png}
        \end{figure}
    \end{example}

    In the fuzzy logic it is possible to use fuzzy modifiers to modify the truth values of the propositions.     
    The modifiers can be of two main types: 
    \begin{itemize}
        \item Strong ($m(a) \leq a \: \forall a \in [0 \dots 1]$): they make the predicate stronger, so they reduce the truth of the preposition.
        \item Weak($m(a) \geq a \: \forall a \in [0 \dots 1]$): they make the predicate weaker, so they increase the truth of the preposition.
    \end{itemize}
    The properties of the fuzzy modifiers are:
    \begin{itemize}
        \item $m(0)=0$ and $m(1)=1$.
        \item $m$ is a continuous function. 
        \item If $m$ is strong $m^{-1}$ is weak, and the other way around.
        \item Given another modifier $g$, the composition of $g$ and $m$ and the other way round are modifiers, too, and, if both are strong
            (or weak), so it's their composition.
    \end{itemize}
    \begin{example}
        The sentence "x is young" actually means "(x is young) is true". This sentence can be modified in the following ways with fuzzy modifiers:
        \begin{itemize}
            \item "x is very young is true".
            \item "x is young is very true".
            \item "x is very young is very true". 
        \end{itemize}
        Graphically we can draw the modified membership function as: 
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.4\linewidth]{images/modifiers.png}
        \end{figure}
        where: 
        \begin{itemize}
            \item $\mu_{very \: a}(x)=\mu_a(x)^2$.
            \item $\mu_{fairly \: a}(x)=\mu_a(x)^{\frac{1}{2}}$.
        \end{itemize}
    \end{example}

    \section{Inference rules}
    \begin{definition}
        An \emph{inference rule} is a model. In other words it is a way to define a mapping from input to output. 
        Rules are used to represent inferential relationships among pieces of knowledge.
    \end{definition}
    We consider forward chaining rules, having the shape:
    \[IF \: antecedent \: THEN \: consequent\]
    where:
    \begin{itemize}
        \item $antecedent$ is a set of clauses related by logical operators.
        \item $consequent$ is a set of clauses related by logical operators.
    \end{itemize}
    The clauses used in the inference rules are either a proposition (sequence of symbols) or a pattern (sequence of symbols and variables). 

    Inference rules are used to implement Knowledge-Based Systems, among which Expert Systems are mostly known as successful Artificial
    Intelligence applications. An Expert System is designed upon the experience of somebody to replicate, or improve his performance in solving
    a problem. Knowledge Acquisition is a complex process bringing to the definition of rule-based systems, implemented and running on computers.

    A system can generate new information using rules and other related information using these steps:
    \begin{enumerate}
        \item Pattern matching: identify the rules whose antecedents match the known facts (saved in the fact base). These can be considered for
            activation, given the corresponding assignment to variables.
        \item Selection of the rules to be activated: among the rules identified with pattern matching (candidate rules), select the rules that
            should be activated.
        \item Activation of the selected rules: assert the consequent of the selected rules in the fact base. 
    \end{enumerate}
    \begin{example}
        Suppose that the rule base contains the following four rules:
        \begin{enumerate}
            \item "If X croaks and X eats flies, then X is a frog".
            \item "If X chirps and X sings, then X is a canary".
            \item "If X is a frog, then X is green".
            \item "If X is a canary, then X is yellow".
        \end{enumerate}
        Now suppose to observe the following facts (fact base):
        \begin{itemize}
            \item Fritz croaks.
            \item Fritz eats flies.
        \end{itemize}
        From rule 1 and facts a and b we can add to the fact base the fact: 
        \[Fritz \: is \: a \: frog\]
        Given the new fact base, we can use rule 3 to deduce the fact: 
        \[Fritz \: is \: green\]
    \end{example}

    \section{Fuzzy rules}
    \begin{definition}
        A \emph{fuzzy rule} is a rule whose clauses have the shape
        \[(V \: is \: L)\]
        where $V$ is a linguistic variable and $L$ is a label, a value for $V$ associated to a fuzzy set. This is a \emph{linguistic clause}.
    \end{definition}
    Often, clauses in the antecedent are only related by the AND operator which is not explicitly written. 
    The antecedent is usually matched against facts that are represented as values of the base variables corresponding to the linguistic variables.
    The consequent may be one of two types:
    \begin{itemize}
        \item Linguistic rules: the consequent is a conjunction of linguistic clauses. These rules can be considered as a mapping between
            the interpretation of an input configuration and a symbolic description of the desired output. The general formula is:
            \[IF \: (A \: is \: LA_i) \: AND \: (B \: is \: LB_k) \: AND \: \dots \: THEN \: (U \: is \: LU_m) \: AND \: \dots\]
        \item Model rules: bind a model to the linguistic interpretation of its applicability conditions. This can be considered as a mapping 
            between the interpretation of an input configuration and a model to be applied to the input real values to obtain the output. 
            The general formula is:
            \[IF \: (A \: is \: LAn) \: AND \: (B \: is \: LBk) \: AND \: \dots \: THEN \: U \: is \: f(A,B)\]
    \end{itemize}
    The steps to use the fuzzy rules are the following: 
    \begin{enumerate}
        \item Input matching.
        \item Combination of matching degrees.
        \item Combination with rule weight, if present.
        \item Aggregation of output from different rules. 
        \item Eventual defuzzyfication of output.
    \end{enumerate}
    To defuzzyficate the output it is possible to consider various operators other than the weighted mean, for example: centroid, bisector,
    average of maxima, the lowest maximum, the highest maximum, center of the highest area, \dots Depending on the choice the system change the 
    output and the level of optimization. 
    \begin{example}[Linguistic rules]
        Let's consider:
        \begin{itemize}
            \item Two input variables (fuzzy partition) $A$ and $B$ equally distributed from Negative Large to Positive Large. 
            \item One output variable $U$ (equally distributed fuzzy set) from Negative Large to Positive Large. The fuzzy sets are all singletons.
        \end{itemize} 
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.75\linewidth]{images/rules.png}
        \end{figure}
        We now have to define the rules of the rule base (each one with a weight) as follows: 
        \begin{enumerate}
            \item IF $A$ is $PL$ AND $B$ is $PS$ THEN $X$ is $PM$ (weight 1).
            \item IF $A$ is $PM$ AND $B$ is $PS$ THEN $X$ is $PS$ (weight 0.5).
            \item IF $A$ is $PL$ AND $B$ is $PM$ THEN $X$ is $PM$ (weight 1).
        \end{enumerate}

        Let's now set $A=22$ and $B=140$. The steps used to calculate the output value are the following:
        \begin{enumerate}
            \item For the first step we have to check the corresponding value for each label. In this case we have:
                \begin{itemize}
                    \item ($A$ is $PL$) has a truth value of $0.2$.
                    \item ($B$ is $PS$) has a truth value of $0.6$.
                    \item ($A$ is $PM$) has a truth value of $0.8$.
                    \item ($B$ is $PM$) has a truth value of $0.4$.
                \end{itemize}
            \item To consider the degree of truth of each predicate we simply take the minimum between the two values (because there is an AND 
                operator). So, the result will be $0.2$ for the first rule, $0.6$ for the second and $0.4$ for the third one.
            \item Now we have to consider the rule weight. To do so we simply select the minimum between the selected value and the weight value.
                So the final value for the consequent are: $0.2$ for the first rule, $0.5$ for the second and $0.4$ for the third one. 
            \item  Now we have to aggregate the output. If we have a repeated expression we take the maximum value between the possible ones. 
                In this case we obtain that ($X$ is $PM$) has a truth value of $0.4$ (maximum between $0.2$ and $0.4$) and ($X$ is $PS$) has a 
                truth value of $0.5$. This result can be visualized graphically by cutting the initial graph as follows.
                \begin{figure}[H]
                    \centering
                    \includegraphics[width=0.5\linewidth]{images/cut.png}
                \end{figure}
            \item Now we can defuzzyficate the result by obtaining a number. We need to do this operation because the input is a number and the 
            desired output needs to be a number. To calculate the exact value of the output it is possible to use a simple weighted mean: 
            \[X=\frac{10 \cdot 0.5 + 20 \cdot 0.4}{0.5+0.4}=14.44\]
        \end{enumerate}
    \end{example}
    \begin{example}[Model rules]
        The variables are the same as the previous example. The rules we are going to consider are the following: 
        \begin{enumerate}
            \item IF $A$ is $PL$ and $B$ is $PS$ THEN $X$ is $A+2B$.
            \item IF $A$ is $PM$ and $B$ is $PS$ THEN $X$ is $A+3$. 
            \item IF $A$ is $PL$ and $B$ is $PM$ THEN $X$ is $A+B$.
        \end{enumerate}
        The models considered in these rules are all linear. Pattern matching is the same as the previous example, and so we have that the 
        subsequent have the following degree of truth: the first one has a value of $0.2$, the second a value of $0.5$ and the third a value of
        $0.4$. For the output aggregation we consider again the weighted mean and use the initial value of $A=22$ and $B=140$ in the resulting 
        formula:
        \[X=\frac{0.2 \cdot (A+2B)+0.5 \cdot (A+3)+ 0.4 \cdot (A+B)}{0.2+0.5+0.4}=125.18\]
    \end{example}

    \section{Fuzzy system design}
    To design a fuzzy system we have to follow those steps:
    \begin{enumerate}
        \item Problem definition.
        \item Parametrization of the model: concepts.
        \item Mapping definition: rules.
        \item Implementation.
        \item Testing.
    \end{enumerate}
    In the problem definition phase we have to choose all the input and output variables and the goal of the model. In principle, input variables
    are numerical or ordinal (like colors) variables so that it is possible to define fuzzy sets on them. Variables can be either: 
    \begin{itemize}
        \item Perceived values coming directly from sensors, data, or users. 
        \item Computed from perceived variables. 
    \end{itemize}
    The selection of the variables is up to designers, so there aren't best or worst input variables to select. Output variables are the result
    of the models, so come directly from the modeler needs. The goals of the fuzzy models depend on the specification. The goals should always be
    stated in advance and guide the design. 

    The system parametrization is based on:
    \begin{itemize}
        \item Selection of the membership functions for all variables. These function can be defined by a:
            \begin{itemize}
                \item Single expert, with objective evaluation or interviews.
                \item Multiple expert, which is more reliable.
                \item Automatic systems working on data (like Neural Networks).
            \end{itemize}
            The number of membership functions for each variable varies between three and seven. 
            Any point in the range of input variables has to be covered by at least one fuzzy set participating to at least one rule and boundaries
            should be covered with maximum value. 
        \item Selection of the inferential mechanism. The inferential engine depends on the operators selected for:
            \begin{itemize}
                \item AND of antecedent clauses (minimum: the worst degree of matching is the most relevant; product: all the degrees of matching
                    are relevant).
                \item Detachment: combination with the rule weight (minimum or product).
                \item Aggregation of the degrees of the same consequent (max: the best degree is the most relevant; probabilistic sum: all the
                    knowledge is considered).
            \end{itemize}
        \item Selection of eventual fuzzyfication and defuzzyfication. 
    \end{itemize}

    The rules can be defined:
    \begin{itemize}
        \item From experience.
        \item From another model.
        \item By using Machine Learning, or self-tuning techniques (like Neural Networks).
    \end{itemize}

    The testing can be done with: dynamic simulation, static simulation or directly on the process, possibly under safe conditions. 

    \section{Applications of fuzzy systems}
    The fuzzy controls are systems able to control the behavior of another system. In most cases it is a PID controller, where the output
    depends on the difference between the desired and the observed behavior. 
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.5\linewidth]{images/control.png}
        \caption{General schema of a fuzzy control system}
    \end{figure}
    The main features of a fuzzy control system are: 
    \begin{itemize}
        \item Robustness with reference to noise.
        \item Control rules defined over a wide range of applicability.
        \item Possibility to model heuristics from experts.
        \item Smoothness of action.
        \item Non-linearity.
    \end{itemize}

    The fuzzy system can also be used to make database queries that are flexible with human like sensibility. For example with fuzzy sets it
    is possible to make queries like "Give me the names of the people that have recently invested a lot" and give a meaning to "recently" and 
    "a lot". 

    The fuzzy systems can be also used in Artificial Intelligence systems like Expert Systems, scheduling, and Decision Support Systems.
\end{document}