\section{Direct methods}

\subsection*{LU factorization method}
Let $A \in \mathbb{R}^{n \times n}$. 
Suppose there exist two appropriate matrices $L$ and $U$, which are lower triangular and upper triangular, respectively, such that $A=LU$. 
This is referred to as an LU factorization of $A$.
In the event that $A$ is non-singular, both $L$ and $U$ are also non-singular, and consequently, their diagonal elements are non-zero.
Under these circumstances, solving $Ax=B$ can be reduced to solving two triangular systems: $Ly=B$ and $Ux=y$. 
\subsubsection*{Forward substitution algorithm}
The initial equation can be efficiently solved using the forward substitution algorithm with a time complexity of $O(n^2)$. 
The algorithm can be outlined as follows:
\begin{algorithm}[H]
    \caption{Forward substitution algorithm}
        \begin{algorithmic}
            \State $y_1\leftarrow\dfrac{b_1}{l_{11}}$
            \State $y_n\leftarrow\dfrac{1}{l_{11}}\left( b_i-\sum_{j=1}^{i-1}{l_{ij}y_j} \right) \:\:\:\:\:\: i=2,\dots,n$
        \end{algorithmic}
\end{algorithm}
\subsubsection*{Backward substitution algorithm}
Similarly, the second equation can be effectively solved using the backward substitution algorithm, also with a time complexity of $O(n^2)$. 
The algorithm can be summarized as follows:
\begin{algorithm}[H]
    \caption{Backward substitution algorithm}
        \begin{algorithmic}
            \State $x_n\leftarrow\dfrac{y_n}{u_{nn}}$
            \State $x_i\leftarrow\dfrac{1}{u_{ii}} \left( y_i-\sum_{j=i+1}^{n}{u_{ij}x_j} \right) \:\:\:\:\:\: i=n-1,\dots,1$
        \end{algorithmic}
\end{algorithm}

The elements within matrices $L$ and $U$ satisfy the system of nonlinear equations, represented as:
\[\sum_{r=1}^{\min{(i,j)}}{l_{ir}u_{rj}}=a_{ij} \:\:\:\:\:\: i,j=1,\dots,n\]
This system is underdetermined because there are $n^2$ equations and $n^2+n$ unknowns.
As a result, the LU factorization is not unique. 
However, by enforcing that the $n$ diagonal elements of $L$ are set to 1, the previous system becomes a determined one.
This determined system can be efficiently solved using the Gauss algorithm.
\subsubsection*{Gauss algorithm}
\begin{algorithm}[H]
    \caption{Gauss algorithm}
        \begin{algorithmic}[1]
            \For {$k=1,\dots,n-1$}
                \For {$i=k+1,\dots,n$}
                    \State $l_{ik} \leftarrow \dfrac{a_{ik}^{(k)}}{a_{kk}^{(k)}}$
                    \For {$j=k+1,\dots,n$}
                        \State $a_{ij}^{(k+1)} \leftarrow a_{ij}^{(k)}-l_{ik}a_{kj}^{(k)}$
                    \EndFor
                \EndFor
            \EndFor
        \end{algorithmic}
\end{algorithm}
The elements denoted as $a_{kk}^{(k)}$ must all possess non-zero values and are referred to as pivot elements.
After completing this procedure, the upper triangular matrix $U$ is composed of elements represented by $u_{ij}$, while the coefficients $l_{ij}$ generated by this algorithm make up the matrix $L$.
The determination of the elements within $L$ and $U$ necessitates approximately $\dfrac{2}{3}n^3$ operations. 

\begin{proposition}
    For a given matrix $A \in \mathbb{R}^{n \times n}$, its LU factorization exists and is unique if and only if the principal submatrices $A_i$ of $A$ of order $i=1,\dots,n-1$ are non-singular. 
\end{proposition}
The conditions outlined in this proposition are satisfied by specific classes of matrices:
\begin{itemize}
    \item Strictly dominant matrices. 
    \item Real symmetric and positive definite matrices. 
    \item Complex definite positive matrices. 
\end{itemize}
\begin{example}
    Given the following system: 
    \[  \begin{cases}
            x_1+3x_2=b_1 \\
            2x_1+2x_2+2x_3=b_2 \\
            3x_1+6x_2+4x_3=b_3
        \end{cases}\]
    Find the lower triangular and the upper triangular matrices using the Gauss algorithm. The initial matrix is: 
    \[A^{(0)}=            
        \begin{bmatrix}
            1 & 0 & 3 \\
            2 & 2 & 2 \\
            3 & 6 & 4
        \end{bmatrix}\]
    For the first iteration we compute the values for the lower matrices using the first column: 
    \[l_{21}=\dfrac{a_{21}}{a_{11}}=\dfrac{2}{1}=2\]
    \[l_{31}=\dfrac{a_{31}}{a_{11}}=\dfrac{3}{1}=3\]
    So, it is now possible to compute the new second and third rows with these formulas: 
    \[r_2 \leftarrow r_2 - l_{21}r_1\]
    \[r_3 \leftarrow r_3 - l_{31}r_1\]
    The new matrix now becomes: 
    \[A^{(1)}=            
    \begin{bmatrix}
        1 & 0 & 3 \\
        2-2 & 2-0 & 2-6 \\
        3-3 & 6-0 & 4-9
    \end{bmatrix} =
    \begin{bmatrix}
        1 & 0 & 3 \\
        0 & 2 & -4 \\
        0 & 6 & -5
    \end{bmatrix}\]
    For the first iteration we compute the values for the lower matrices using the first column (always from $A^{(0)}$): 
    \[l_{32}=\dfrac{a_{32}}{a_{22}}=\dfrac{6}{2}=3\]
    So, it is now possible to compute the new third row with this formula: 
    \[r_3 \leftarrow r_3 - l_{32}r_2\]
    The new matrix now becomes: 
    \[A^{(2)}=            
    \begin{bmatrix}
        1 & 0 & 3 \\
        0 & 2 & -4 \\
        0-0 & 6-6 & -5+12
    \end{bmatrix}=
    \begin{bmatrix}
        1 & 0 & 3 \\
        0 & 2 & -4 \\
        0 & 0 & 7
    \end{bmatrix}\]
    Now we have found that the lower triangular matrix is: 
    \[L=
    \begin{bmatrix}
        1 & 0 & 0 \\
        2 & 1 & 0 \\
        3 & 3 & 1
    \end{bmatrix}\]
    and that the upper triangular matrix is: 
    \[U=
    \begin{bmatrix}
        1 & 0 & 3 \\
        0 & 2 & -4 \\
        0 & 0 & 7
    \end{bmatrix}\]
\end{example}

\subsection*{Cholesky factorization}
When $A \in \mathbb{R}^{n \times n}$ is symmetric and positive definite, it's possible to establish a specialized factorization:
\[A=R^TR\]
Here, $R$ is an upper triangular matrix with positive diagonal elements. 
This factorization is known as the Cholesky factorization and involves approximately $\dfrac{1}{3}n^3$ operations. 
It's noteworthy that, due to symmetry, only the upper part of $A$ is stored, and $R$ can be accommodated in the same memory space.

The elements of $R$ can be computed using the following algorithm:
\begin{algorithm}[H]
    \caption{Cholesky factorization algorithm}
        \begin{algorithmic}
            \State $r_{jj}\leftarrow\sqrt{a_{jj}-\sum_{k=1}^{j-1}r_{kj}^2}$
            \State $r_{ij}\leftarrow\dfrac{1}{r_{ii}}\left(a_{ij}-\sum_{k=1}^{i-1}r_{ki}r_{kj}\right) \:\:\:\:\:\: i = 1,\dots,j-1$
        \end{algorithmic}
\end{algorithm}
In some instances, it is possible to adjust the matrix $A$ to apply this factorization by utilizing an appropriate matrix $P$:
\[P^TAP=R^TR\]

\subsection*{The pivoting technique}
The pivoting technique allows us to achieve the LU factorization for any non-singular matrix.
By performing a suitable row permutation of the original matrix $A$, we can make the entire factorization process feasible, even if the conditions of the proposition are not met, as long as $\det(A) \neq 0$. 
The decision regarding which row to permute can be made at each step $k$, when a zero diagonal element $a_{kk}^{(k)}$ is generated.
Since a row permutation involves changing the pivot element, this method is known as row pivoting.
The factorization obtained in this manner preserves the original matrix, albeit with a row permutation. Specifically, we have:
\[PA=LU\]
Here, $P$ is a suitable permutation matrix, initially set as the identity matrix. 

Whenever during the procedure, two rows of $A$ are permuted, the same permutation must be applied to the corresponding rows of $P$.
In the end, we need to solve the following systems:
\[Ly=PB\]
\[Ux=y\]

In addition to zero pivots, small elements $a_{kk}^{(k)}$ can also pose problems.
In such cases, potential round-off errors affecting the coefficients $a_{kj}^{(k)}$ can be significantly amplified.
Therefore, it is advisable to implement pivoting at each step of the factorization process by searching among all potential pivot elements $a_{ik}^{(k)}$, with $i=k,\dots,n$, for the one with the maximum modulus.
\begin{algorithm}[H]
    \caption{Gauss algorithm with pivoting}
        \begin{algorithmic}[1]
            \For {$k=1,\dots,n-1$}
                \State find $\overline{r}$ such that $\left\lvert a_{\overline{r}k}^{(k)} \right\rvert=\max_{r=k,\dots,n}\left\lvert a_{rk}^{(k)} \right\rvert$
                \State exchange row $k$ with row $\overline{r}$ in both $A$ and $P$
                \For {$i=k+1,\dots,n$}
                    \State $l_{ik} \leftarrow \dfrac{a_{ik}^{(k)}}{a_{kk}^{(k)}}$
                    \For {$j=k+1,\dots,n$}
                        \State $a_{ij}^{(k+1)} \leftarrow a_{ij}^{(k)}-l_{ik}a_{kj}^{(k)}$
                    \EndFor
                \EndFor
            \EndFor
        \end{algorithmic}
\end{algorithm}

\subsection*{Linear system solution accuracy}
When solving a linear system $Ax=B$ numerically, we are, in fact, seeking the exact solution $\widetilde{x}$ to a perturbed system:
\[\left( A + \delta A\right)\widetilde{x}=B+\delta b\]
Here, $\delta A$ and $\delta b$ are, respectively, a matrix and a vector that depend on the specific numerical method being employed.
Let's begin by considering the case where $\delta A=0$ and $\delta b\neq 0$, which is simpler than the more general case.
Suppose $\delta A \in \mathbb{R}^{n \times n}$ and $\delta b \in \mathbb{R}^{n}$, with $\left\lVert \delta A \right\rVert_2 \leq \varepsilon$ and $\left\lVert \delta b \right\rVert_2 \leq \varepsilon$, where $\varepsilon > 0$. 
By comparing the previous equation with the general system formula, we can derive the following:
\[A\left(x-\widetilde{x}\right)=\delta b\Rightarrow\Vert x-\widetilde{x} \Vert_2=\Vert A^{-1}-\delta b \Vert_2 \leq \Vert A^{-1} \Vert_2 \Vert \delta b \Vert_2\]
We can also establish that:
\[\Vert Ax\Vert_2=\Vert B\Vert_2 \Rightarrow \Vert A \Vert_2\Vert x \Vert_2 \geq \Vert Ax\Vert_2=\Vert B\Vert_2\]
By dividing these two equations, we derive:
\[\dfrac{\left\lVert \widetilde{x}-x \right\rVert_2}{\left\lVert x \right\rVert_2} \leq \dfrac{\left\lVert A \right\rVert_2\left\lVert A^{-1} \right\rVert_2\left\lVert \delta b \right\rVert_2}{\left\lVert B \right\rVert_2}=k_2(A)\dfrac{\left\lVert \delta b \right\rVert_2}{\left\lVert B \right\rVert_2}\]
Since the matrix is symmetric positive definite we have that: 
\[k_2(A)=\dfrac{\lambda_{max}(A)}{\lambda_{min}(A)}\]

If the value of the constant $k_2(A)$ is excessively high (on the order of 1000), we need to correct the result using the residual correction algorithm.
\begin{algorithm}[H]
    \caption{Residual correction algorithm}
        \begin{algorithmic}[1]
            \State solve $ Ax=B$
            \State $r \leftarrow B- Ax$
            \State solve $\boldsymbol{Ad}=r$
            \State $x \leftarrow x+\boldsymbol{d}$
        \end{algorithmic}
\end{algorithm}