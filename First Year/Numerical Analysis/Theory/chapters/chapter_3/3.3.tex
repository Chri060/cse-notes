\section{Iterative methods}

A common approach to formulate an iterative method involves a matrix $A$ being split into:
\[A=P-\left(P-A\right)\]
Here, $P$ represents a suitable non-singular matrix referred to as the preconditioner of $A$.

We will denote with $\rho(B)$ the spectral radius of $B$, that is, the maximum modulus of eigenvalues of $B$. 
If $B$ is symmetric positive definite then $\rho(B)$ coincides with the largest eigenvalue of $B$. 

\subsection*{Jacobi method}
If the diagonal entries of matrix $A$ are non-zero, we can set $P=D$, where $D$ is the diagonal matrix containing the diagonal entries of $A$. 
Consequently, we have:
\[Dx^{(k+1)}=B-\left(A-D\right)x^{(k)} \:\:\:\:\:\: k \geq 0\]
Alternatively, when considering individual components, the Jacobi method can be expressed as follows:
\begin{algorithm}[H]
    \caption{Jabobi method algorithm}
        \begin{algorithmic}[1]
            \State $x_i^{(k+1)}=\dfrac{1}{a_{ii}}\left( b_i-\sum_{j=1,j \neq i}^{n}a_{ij}x_j^{(k)}\right) \:\:\:\:\:\: i=1,\dots,n $
        \end{algorithmic}
\end{algorithm}
The stopping criteria for the Jacobi method include:
\[\dfrac{\left\lVert x^{(k+1)}-x^{(k)} \right\rVert_2}{\left\lVert x^{(k)} \right\rVert_2} \leq \varepsilon \:\:\: \lor \:\:\: \dfrac{\left\lVert B- Ax^{(k+1)} \right\rVert_2}{\left\lVert B \right\rVert_2} \leq \varepsilon_r\]
\begin{proposition}
    If the matrix $A \in \mathbb{R}^{n \times n}$ is strictly diagonally dominant by row, then the Jacobi method is guaranteed to converge.
\end{proposition}

\subsection*{Gauss-Seidel method}
The Gauss-Seidel method iteratively computes the result using the following formula:
\[\left(D-E\right)x^{(k+1)}=B+Fx^{(k)}\]
Here, $-F$ represents the upper triangular part of the matrix, excluding the diagonal, $-E$ is the lower triangular part of the matrix, excluding the diagonal, and $D$ is the diagonal matrix containing the diagonal entries of matrix $A$.
\begin{algorithm}[H]
    \caption{Gauss-Seidel algorithm}
        \begin{algorithmic}[1]
            \State $x_i^{(k+1)}=\dfrac{1}{a_{ii}}\left( b_i-\sum_{j=1}^{i-1}a_{ij}x_j^{(k+1)}-\sum_{j=i+1}^{n}a_{ij}x_j^{(k)}\right) \:\:\:\:\:\: i=1,\dots,n $
        \end{algorithmic}
\end{algorithm}
\begin{proposition}
    Let $A \in \mathbb{R}^{n \times n}$ be a tridiagonal non-singular matrix whose diagonal elements are all non-null. 
    Then the Jacobi method and the Gauss-Seidel method are either both divergent or both convergent. 
    In the latter case, the Gauss-Seidel method is faster than Jacobi's. 
\end{proposition}
It's worth noting that when $P=D-E$, it is referred to as the Gauss-Seidel method, whereas when $P=D-F$, it is known as the inverted Gauss-Seidel method.

\subsection*{Richardson method}
The general form of the Richardson method is expressed as:
\[P(x^{(k+1)}-x^{(k)})=\alpha_kr^{(k)} \:\:\:\:\:\: k \geq 0\]
In this method, we refer to it as "stationary" when $\alpha_k=\alpha$ for any $k \geq 0$, and "dynamic" when $\alpha_k$ may vary during the iterations. 
When $P \neq I$, it is known as the preconditioned Richardson method, and the formula becomes:
\[x^{(k+1)}=x^{(k)}+\alpha_k r^{(k)}\]
\begin{proposition}
    Let $A \in \mathbb{R}^{n \times n}$. For any non-singular matrix $P \in \mathbb{R}^{n \times n}$ the stationary Richardson method converges if and only if: 
    \[\left\lvert \lambda_i\right\rvert < \dfrac{2}{\alpha} \textnormal{Re}\left[\lambda_i\right] \:\:\:\:\:\:\forall i=1,\dots,n\]
    Here, $\lambda_i$ are the eigenvalues of $(P^{-1}A)$. If the latter are all real, it converges if and only if: 
    \[0 < \alpha\lambda_i<2 \:\:\:\:\:\:\forall i=1,\dots,n\]
    If both $A$ and $P$ are symmetric positive definite matrices, the stationary Richardson method converges for any possible choice of 
    $x^{(0)}$ if and only if: 
    \[0 < \alpha < \dfrac{2}{\lambda_{max}}\]
    Here, $\lambda_{max} >0$ is the maximum eigenvalue of $(P^{-1} A)$. 
    Moreover, the spectral radius $\rho (B_{\alpha})$ of the iteration matrix $B_{\alpha}=I-\alpha P^{-1}A$ is minimized for $\alpha=\alpha_{opt}$, where: 
    \[\alpha_{opt}=\dfrac{2}{\lambda_{min}+\lambda_{max}}\]
    $\lambda_{min}$ being the smallest eigenvalue of $P^{-1}A$. 
    If I choose $\alpha_{opt}$, then: 
    \[\left\lVert e^{(k+1)}\right\rVert_A= \left\lVert x^{(k+1)}-x^{(k)}\right\rVert_A \leq \dfrac{k_2(P^{-1}A)-1}{k_2(P^{-1}A)+1}\left\lVert e^{(k)}\right\rVert_A\]
\end{proposition}

\subsection*{Gradient method}
If matrix $A$ is symmetric positive definite, solving the equation $Ax=B$ is equivalent to finding the unique minimum of the function $\phi(y)=\dfrac{1}{2}y^TAy-B^Ty$. 
Given an initial guess $x^{(0)}$, we compute the values as follows:
\[x^{(k+1)}-\alpha_{k}\nabla\phi(x^{(k)})=x^{(k)}+\alpha_kr^{(k)}\]
In the preconditioned version, we replace $r^{(k)}$ with the preconditioned residual $Z^{(k)}$, which is the solution of $=Pz^{(k)}r^{(k)}$: 
\[x^{(k+1)}-\alpha_{k}\nabla\phi(x^{(k)})=x^{(k)}+\alpha_kz^{(k)}\]
Let's define the function $\varphi(\alpha)=\phi(x^{(k)}+\alpha r^{(k)})$. 
The minimum of this function converges to $\alpha_k$, which is given by:
\[\alpha_k=\dfrac{\left\lVert r^{(k)} \right\rVert^2}{r^{(k)T}Ar^{(k)}}\]
The convergence ratio of the gradient method is once again determined by:
\[\left\lVert e^{(k+1)}\right\rVert_A\leq \dfrac{k_2(P^{-1}A)-1}{k_2(P^{-1}A)+1}\left\lVert e^{(k)}\right\rVert_A\]

\subsection*{Conjugate gradient method}
When both matrices $A$ and $P$ in $\mathbb{R}^{n \times n}$ are symmetric positive definite, the conjugate gradient method can be employed.
In exact arithmetic, the conjugate gradient method converges for any initial approximation $x^{(0)}$ within at most $n$ iterations.
The stopping criteria for this method include monitoring the residual, controlling the step size, and imposing a maximum of $n$ iterations.

\subsection*{Generalized minimal residual method}
Among the iterative methods applicable to general matrices, the Generalized Minimal Residual (GMRES) stands out as one of the most significant. 
The primary objective of GMRES is to progressively decrease the norm of the residual at each iteration.

However, due to the increasing computational cost with each iteration, it is often more efficient to employ a modified version known as "restarted GMRES". 