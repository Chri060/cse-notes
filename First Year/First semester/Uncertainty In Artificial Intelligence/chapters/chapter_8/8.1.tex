\section{Learning classification}

A Bayesian network is characterized by its graphical structure and the parameters associated with each conditional probability density.
These components can be specified either through expert knowledge or learned from available data. 
It is important to note the following:
\begin{itemize}
    \item The process of learning the network structure is more challenging compared to learning its parameters.
    \item Learning in scenarios where certain nodes are concealed or when data is incomplete is considerably more difficult than situations where all observations are available.
\end{itemize}
Learning approaches can be categorized based on both structure and observability, yielding the following classifications:
\begin{table}[H]
    \centering
    \begin{tabular}{ccc}
                                            & \multicolumn{2}{c}{\textbf{Observability}}                                              \\ \cline{2-3} 
    \multicolumn{1}{c|}{\textbf{Structure}} & \textit{Full}                 & \multicolumn{1}{c|}{\textit{Partial}}                   \\ \hline
    \multicolumn{1}{|c|}{\textit{Known}}    & Maximum likelihood estimation & \multicolumn{1}{c|}{Expectation Maximization}           \\
    \multicolumn{1}{|c|}{\textit{Unknown}}  & Search through model space    & \multicolumn{1}{c|}{EM with search through model space} \\ \hline
    \end{tabular}
\end{table}

\subsection{Known structure with full observability}
In this scenario, our objective is to determine the values of parameters in the conditional probability distributions that maximize the likelihood of the observed data, denoted as $d \in D$: 
\[L=\sum_i^N\sum_d^D \log \Pr(X_i\mid \text{parents}(X_i),d)\]
To simplify computation, we utilize the log-likelihood. 
The log-likelihood decomposes based on the graph structure, enabling the maximization of each node's contribution independently. 
Sparse data challenges are addressed by incorporating Dirichlet priors.
For Gaussian nodes, we compute the sample mean and variance, employing linear regression to estimate the weight matrix.
\begin{example}
    Let's consider the sprinkler example with Wet being true.
    For the Wet node, given a set of training data, we count the occurrences of Wet when it is Raining and the Sprinkler is on (for all combinations):
    \[\Pr(W=1\mid R=1,S=1)=\dfrac{\#(W,S,R)}{\#(S,R)}\]
    However, since this specific combination might never occur, we adjust the denominator to prevent it from being null:
    \[\Pr(W=1\mid R=1,S=1)=\dfrac{\#(W,S,R)}{\#(\overline{W},S,R)+\#(W,S,R)}\]
    To account for the possibility of some combinations never occurring, we introduce Dirichlet priors. 
    This rule ensures that each combination occurs at least once by adding one to the total number of observed evidence instances.
\end{example}

\subsection{Known structure with partial observability}
When certain nodes are hidden, the Expectation Maximization algorithm can be employed to derive a locally optimal Maximum likelihood estimate, as follows:
\begin{itemize}
    \item \textit{E-Step}: compute expected values for unobserved variables using an inference algorithm. 
        Treat these expected values as if they were observed.
    \item \textit{M-Step}: consider the model as fully observable and apply the algorithm designed for full observability.
\end{itemize}
Given the expected counts, iteratively maximize parameters and recalculate expected counts. 
The Expectation Maximization algorithm converges to a local maximum likelihood.
\begin{example}
    Let's consider the sprinkler example with Wet being true. 
    For the Wet node, observed counts are replaced with the expected occurrences of each event:
    \[\Pr(W=1\mid R=1,S=1)=\dfrac{\mathbb{E}\left[\#(W,S,R)\right]}{\mathbb{E}\left[\#(S,R)\right]}=\dfrac{\sum_d^D\Pr(W,S,R\mid d)}{\sum_d^D\Pr(S,R\mid d)}\]
\end{example}
However, it's worth noting that this algorithm becomes slow for large networks due to the necessity of applying inference for each data point at each iteration.

\subsection{Unknown structure with full observability}
Structure learning aims to discover a Directed Acyclic Graph that provides the most accurate explanation for the given data. 
Here are key points about structure learning:
\begin{itemize}
    \item In general, it poses an $\mathcal{NP}$-hard problem, with the number of DAGs on $N$ variables growing super-exponentially in $N$.
    \item If the node ordering is known, the parent set for each node can be learned independently.
\end{itemize}
The approach involves initializing the model structure and then conducting a local search.
This search evaluates the scores of neighboring structures, moving to the best one until a local optimum is reached.
Several algorithms can be employed for this task, including the Tabu search algorithm, genetic algorithms (for global optimization), and multiple restarts (for finding both global optimum and learning a model ensemble).

Start with an initial guess of the model structure, then perform local search, evaluating the score of neighboring structures, and move to the best one until reaches a local optimum.
The possible algorithms to to this are: Tabu search algorithm, genetic algorithms (to find a global optimum), and use multiple restarts (to find global optimum and to learn a model ensemble). 

\paragraph*{Maximum likelihood model}
The Maximum likelihood model, denoted as $G_{MLE}$, corresponds to a complete graph. 
It possesses the maximum number of parameters, offering the best data fit. 
However, being a joint distribution, it tends to overfit.

\paragraph*{Maximum a posteriori model}
To address overfitting, the maximum a posteriori model is considered. 
It involves the probability of the graph given the data:
\[\Pr(G\mid D)=\dfrac{\Pr(G\mid D)\Pr(G)}{\Pr(D)}\]
For computational simplicity, the formula is often expressed in logarithmic terms:
\[\Pr(G\mid D)=\log\Pr(G\mid D)+\log\Pr(G)-\log\Pr(D)\]
While $\Pr(G)$ could be used to penalize complex models, it's not always necessary, as the term:
\[\Pr(D\mid G)=\int_{\theta} \Pr(D\mid G,\theta)\]
already has a similar effect.

\subsection{Unknown structure with partial observability}
This situation is typically challenging due to the unknown structure and the presence of hidden variables and/or missing data. 
To address this, learning employs posterior approximation using the Bayesian information criterion:
\[\log\Pr(G\mid D) \approx \log\Pr(D\mid G,\widehat{\Theta}_G)-\dfrac{N\log R}{2}\]
Here, $R$ represents the number of samples, $\widehat{\Theta}_G$ denotes the model parameters, and $N$ is the number of variables.

In cases of full observability, the model's dimension is determined by the number of free parameters.
However, in models with hidden variables, this dimension might be lower.

The Bayesian information criterion breaks down into a sum of local terms.
Despite this decomposition, local search remains computationally expensive, involving Expectation Maximization at each step to compute $\widehat{\Theta}_G$, with the local search executed in the M-step.