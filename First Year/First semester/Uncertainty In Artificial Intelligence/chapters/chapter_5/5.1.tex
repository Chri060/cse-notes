\section{Basic probability}

\begin{definition}[\textit{Boolean-valued random variable}]
    A random variable, denoted as $A$, is considered a boolean-valued random variable when it represents an event, and there exists a level of uncertainty regarding whether this event will occur.
\end{definition}
\begin{definition}[\textit{Probability}]
    The concept of probability associated with $A$ is defined as the proportion of possible outcomes or worlds in which the event represented by $A$ is true.    
\end{definition}
\begin{theorem}
    The fundamental axioms of probability theory are: 
    \[0 \leq \Pr(A) \leq 1\]
    \[\Pr(A=true) = 1 \: \land \Pr(A=false) = 0\]
    \[\Pr(A \lor B)=\Pr(A)+\Pr(B)-\Pr(A \land B)\]
\end{theorem}
Starting with these axioms, it becomes possible to derive various other formulas in probability theory, such as:
\begin{enumerate}
    \item $\Pr(\overline{A})=1-\Pr(A)$.
    \item $\Pr(A)=\Pr(A \land B)+\Pr(\overline{A} \land B)$
\end{enumerate}
\begin{definition}[\textit{Multivalued random variable}]
    A random variable $A$ is classified as a multivalued random variable of arity $k$ when it can assume one of the values from the set $\{v_1, v_2, v_3, \dots, v_k\}$. 
\end{definition}
\begin{theorem}
    The axioms for multivalued random variable are: 
    \[\Pr(A=v_i \land A=v_j) \:\:\:\:\:\: i \neq j\]
    \[\Pr(A=v_1 \lor A=v_2 \lor A=v_3 \lor \dots \lor A=v_k)=1\]
\end{theorem}
These new axioms enable the derivation of other valuable formulas applicable to multivalued variables:    \begin{enumerate}
    \item $\Pr(A=v_1 \lor A=v_2 \lor \dots \lor A=v_i)=\sum_{j=1}^{i}{\Pr(A=v_j)}$.
    \item $\sum_{j=1}^{k}{\Pr(A=v_j)}=1$. 
    \item $\Pr(B \land (A=v_1 \lor A=v_2 \lor \dots \lor A=v_i))=\sum_{j=1}^{i}{B \land A=v_j}$. 
    \item $\Pr(B)=\sum_{j=1}^{k}{\Pr(B \land A=v_j)}$.
\end{enumerate}
\begin{definition}[\textit{Conditional probability}]
    The conditional probability of event $A$ given event $B$ represents the proportion of possible scenarios where event $B$ is true, and event $A$ is true as well.
\end{definition}
Inference can primarily be accomplished using the following rules:
\begin{itemize}
    \item \textit{Chain rule}: $\Pr(A \land B)=\Pr(A\mid B)\Pr(B)$
    \item \textit{Bayes theorem}: $\Pr(A\mid B)=\dfrac{\Pr(B\mid A)\Pr(A)}{\Pr(B)}$.
    \item \textit{Sum rule (marginalization)}: $\Pr(A)=\sum_{b}{(A \land B=b)}$.
\end{itemize}
\begin{definition}[\textit{Idipendent random variables}]
    Let's assume that $A$ and $B$ are boolean random variables. $A$ and $B$ are considered independent, denoted as $A \perp B$, if and only if:
    \[\Pr(A\mid B)=\Pr(A)\]
\end{definition}
\begin{definition}[\textit{Joint distribution of random variables}]
    When we have two random variables, $A$ and $B$, the joint distribution of $A$ and $B$ is represented by $\Pr(A, B)$ and encompasses the combined distribution of both variables.    
\end{definition}
We can represent a joint distribution of $m$ binary variables through the following steps:
\begin{enumerate}
    \item Create a truth table listing all possible combinations of values (a total of $2^m$ entries).
    \item Calculate the probability for each combination.
    \item Verify that the sum of all probabilities equals one.
\end{enumerate}