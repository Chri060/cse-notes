\documentclass[12pt, a4paper]{report}
\usepackage{graphicx, array, amsthm, amssymb, amsmath, algorithm, algpseudocode, float, xcolor, thmtools, thmbox}
\usepackage[english]{babel}

\makeatletter
\renewcommand\thmbox@headstyle[2]{\bfseries #1}
\makeatother
\newtheorem[style=M, bodystyle=\normalfont]{operation}{Operation}
\newtheorem[style=M, bodystyle=\normalfont]{theorem}{Theorem}
\newtheorem[style=M, bodystyle=\normalfont]{corollary}{Corollary}
\newtheorem[style=M, bodystyle=\normalfont]{lemma}{Lemma}
\newtheorem[style=M, bodystyle=\normalfont]{definition}{Definition}


\title{Formal Languages And Compilers \\ \textit{Theory}}
\author{Christian Rossi}
\date{Academic Year 2023-2024}

\begin{document}

\maketitle

\newpage

\begin{abstract}
    The lectures are about those topics: 
    \begin{itemize}
        \item Definition of language, theory of formal languages, language operations, regular expressions, regular languages, finite deterministic and non-deterministic automata, 
            BMC and Berry-Sethi algorithms, properties of the families of regular languages, nested lists and regular languages.
        \item Context-free grammars, context-free languages, syntax trees, grammar ambiguity, grammars of regular languages, properties of the families of context-free languages, 
            main syntactic structures and limitations of the context-free languages.
        \item Analysis and recognition (parsing) of phrases, parsing algorithms and automata, push down automata, deterministic languages, bottom-up and recursive top-down syntactic 
            analysis, complexity of recognition.
        \item Translations: syntax-driven, direct, inverse, syntactic. Transducer automata, and syntactic analysis and translation. Definition of  semantics and semantic properties. Static flow analysis of programs. Semantic translation driven by syntax, semantic functions and attribute grammars, one-pass and 
            multiple-pass computation of the attributes.
    \end{itemize}
    The laboratory sessions are about those topics: 
    \begin{itemize}
        \item Modelisation of the lexicon and the syntax of a simple programming language (C-like).
        \item Design of a compiler for translation into an intermediate executable machine language (for a register-based processor).
        \item Use of the automated programming tools Flex and Bison for the construction of syntax-driven lexical and syntactic analyzers and translators.
    \end{itemize}
\end{abstract}

\newpage

\tableofcontents

\newpage

\chapter{Regular Languages}
    \subsection{Formal language theory}
    A \emph{formal language} consists of words whose letters are taken from an alphabet and are well-formed according to a specific set of rules.
    \begin{definition}
        An \emph{alphabet} is a finite set of elements called terminal symbols or \emph{characters}. 
        The \emph{cardinality} of an alphabet \[\Sigma =\{ a_1,a_2,\dots, a_k \}\] is the number of characters that it contains: $\left\lvert \Sigma \right\rvert = k$. 
        A \emph{string} or word is a sequence of characters. 
    \end{definition}
    \begin{example}
        The alphabet $\Sigma =\{ a,b \}$ has a cardinality of two. Some possible languages derived from this alphabet can be:
        \begin{itemize}
            \item $L_1=\{aa,aaa\}$
            \item $L_2=\{aba,aab\}$
            \item $L_3=\{ab,ba,aabb,abab,\dots,aaabbb,\dots\}$
        \end{itemize}
    \end{example}
    \begin{definition}
        Given a language, a string belonging to it is called a \emph{sentence} or \emph{phrase}. The \emph{cardinality} or size of a language is the number of sentence it contains.
        If the cardinality is finite, the language is called \emph{vocabulary}. 
    \end{definition}
    \begin{example}
        Given the language (that is a vocabulary) $L_2=\{ bc,bbc \}$ we have that its cardinality is equal to two. 
    \end{example}
    \begin{definition}
        The number of repetitions of a certain letter in a word is called \emph{number of occurrences}. The \emph{length} of a string is the number of its elements. 
        Two strings are \emph{equal} if and only if: 
        \begin{itemize}
            \item They have the same length.
            \item Their elements, from left to right, coincide. 
        \end{itemize}
    \end{definition}
    \begin{example}
        The number of occurrences of $a$ and $c$ in $aab$ is indicated with:
        \[{\left\lvert aab \right\rvert}_a = 2\]
        \[{\left\lvert aab \right\rvert}_c = 0\]
        The length of the string $aab$ is equal to: 
        \[\left\lvert aab \right\rvert = 3\]
    \end{example}
    
    \subsection{Operations on strings}
    \begin{operation}[Concatenation]
        Given two strings $x=a_1a_2\dots a_h$ and $y=b_1b_2\dots b_k$ the \emph{concatenation} is defined as:
        \[x \cdot y = a_1a_2\dots a_h b_1b_2\dots b_k\]
    \end{operation}
    Concatenation is non-commutative and associative ($x(yz)=(xy)z$). The length of the result is the sum of the length of the concatenated strings
    ($\left\lvert xy \right\rvert = \left\lvert x \right\rvert + \left\lvert y \right\rvert$). 
    \begin{operation}[Empty string]
        The \emph{empty string} $\varepsilon$ is the neutral element for concatenation that satisfies the identity:
        \[x\varepsilon=\varepsilon x=x\]
    \end{operation}
    It is important to note that $\left\lvert \varepsilon \right\rvert = 0$ and that the set that contains this operator is not the empty set. 
    \begin{operation}[Substring]
        Let string $x=xyv$ be written as the concatenation of three, possibly empty, strings $x,y$ and $v$. Then, strings $x,y$ and $v$ are \emph{substrings} of $x$. 
    \end{operation}
    Moreover, string $u$ is a prefix of $x$ and $v$ is a suffix of $x$. A non-empty substring is called proper if it does not coincide with string $x$. 
    \begin{operation}[Reflection]
        The \emph{reflection} of a string $x=a_1a_2\dots a_h$ is:
            \[x^R=a_ha_{h-1}\dots a_1\]
    \end{operation}
    The following identities are immediate: 
    \[(x^R)^R=x \:\:\:\:\:\: (xy)^R=y^Rx^R \:\:\:\:\:\: \varepsilon^R=\varepsilon\]
    \begin{operation}[Repetition]
        The \emph{repetition} is the $m$-th power $x^m$ of a string $x$ is the concatenation of $x$ with himself $m-1$ times. The formal definition o the following: 
        \[x^m=x^{m-1}x \:\: for m \geq 1 \:\:\:\:\:\: x^0=\varepsilon\]
    \end{operation}
    Repetition and reflection take precedence over concatenation. 

    \subsection{Operations on languages}
    Operations are typically defined on a language by extending the string operation to all its phrases. 
    \begin{operation}[Reflection]
        The \emph{reflection} $L^R$ of a language $L$ is the finite set of strings that are the reflection of a sentence of $L$: 
        \[L^R = \{ x | \exists y \left( y \in L \land x=y^R \right)\}\]
    \end{operation}
    \begin{operation}[Prefix]
        The set of \emph{prefixes} of a language $L$ is defined as: 
        \[Prefixes(L)=\{y | y \neq \varepsilon \land \exists x \exists z \left( x \in L \land x=yx \land z \neq \varepsilon \right)\}\]
    \end{operation}
    A language is prefix-free if none of the proper prefixes of its sentences is in the language. 
    \begin{operation}[Concatenation]
        Given languages $L^{'}$ and $L^{''}$ we have that \emph{concatenation} is defined as: 
            \[L^{'}L^{''}=\{ xy | x \in L^{'} \land y \in L^{''} \}\]
    \end{operation}
    \begin{operation}[Repetition]
        The \emph{repetition} is redefined as: 
        \[L^m=L^{m-1}L \:for \: m \geq 1 \:\:\:\:\:\: L^0=\{ \varepsilon \}\]
    \end{operation}
    The identity now became: 
    \[\varnothing ^0 = \{ \varepsilon \} \:\:\:\:\:\: L.\varnothing=\varnothing .L=\varnothing \:\:\:\:\:\: L.\{\varepsilon\}=\{\varepsilon\} .L=L\]
    The power operator allows one to define concisely the language of strings whose length is not greater than a given integer $K$. 
    \begin{operation}[Set operations]
        Since a language is a set, the classical set operation of union ($\cup$), intersection ($\cap$), difference ($ \setminus $), inclusion ($ \subseteq $), strict inclusion 
        ($ \subset $), and equality ($=$). 
    \end{operation}
    \begin{operation}[Universal language]
        The \emph{universal language} is defined as the set of all the strings, over an alphabet $\Sigma$, of any length including zero: 
        \[L_{universal}=\Sigma ^0 \cup \Sigma ^1 \cup \Sigma ^2 \cup \dots \]
    \end{operation}
    \begin{operation}[Complement]
        The \emph{complement} of a language $L$ over an alphabet $\Sigma$, denoted by $\lnot L$, is the set difference: 
            \[ \lnot L = L_{universal} - L\]
    \end{operation}
    That is, the set of the strings over the alphabet $\Sigma$ that are not in $L$. Note that: 
    \[L_{universal} = \lnot \varnothing\]
    The complement of a finite language is always infinite. The complement of an infinite one is not necessarily finite.   

    Given a set A and a relation $R \subseteq A \times A$, $(a_1, a_2) \in R$ is also denoted as $a_1Ra_2$. $R^{*}$ is a relation defined by:
    \begin{itemize}
        \item $xR^{*}x \:\: \forall x \in A$ (reflexive property). 
        \item $x_1Rx_2 \land x_2Rx_3 \land \dots x_{n-1}Rx_n \implies x_1R^{*}x_n$ (transitive property). 
    \end{itemize}
    \begin{example}
        Given $R = \{(a, b), (b, c)\}$, the transitive closure will be: 
        \[R^{*} = \{(a, a), (b, b), (c, c), (a, b), (b, c), (a, c) \}\]
    \end{example}
    Given a set A and a relation $R \subseteq A \times A$, $(a_1, a_2) \in R$ is also denoted as $a_1Ra_2$. $R^{+}$ is a relation defined by: 
    $x_1Rx_2 \land x_2Rx_3 \land \dots x_{n-1}Rx_n \implies x_1R^{*}x_n$ (transitive property). 

    \begin{example}
        Given $R = \{(a, b), (b, c)\}$, the transitive closure will be: 
        \[R^{+} = \{ (a, b), (b, c), (a, c)\}\]
    \end{example}
    \begin{operation}[Star operator]
        The \emph{star operator} (also called Kleene star) is the reflexive transitive closure under the concatenation operation. It is defined as the union of all the powers of the 
        base language: 
        \[L^{*}=\bigcup_{h=0\dots\infty}L^h=L^0 \cup L^1 \cup L^2 \cup \dots = \varepsilon \cup L^1 \cup L^2 \cup \dots\]
    \end{operation}
    \begin{example}
        Given the language $L=\{ab,ba\}$ we have that the star operation gives the following language: 
        \[L^{*}=\{\varepsilon, ab, ba, abab, abba, baab, baba, \dots\}\]
        It is possible to see that $L$ is finite and $L^{*}$ is infinite. 
    \end{example}
    Every string of the star language $L^{*}$ can be chopped into substrings in $L$. The star language $L^{*}$ can be equal to the base language $L$. If we take $\Sigma$ as the base 
    language, then $\Sigma^{*}$ contains all the strings built on that alphabet (it is the universal language of alphabet $\Sigma$). We often say that $L$ is a language on alphabet
    $\Sigma$ by writing $L \subseteq \Sigma$. 
    \begin{table}[H]
        \centering
        \begin{tabular}{cc}
        \hline
        \textbf{Property}                                      & \textbf{Meaning}            \\ \hline
        $L \subseteq L^{*}$                                    & Monotonicity                \\
        if $x \in L^{*} \land y \in L^{*}$ then $xy \in L^{*}$ & Closure by concatenation    \\
        $(L^{*})^{*}=L^{*}$                                    & Idempotence                 \\
        $(L^{*})^R=(L^R)^{*}$                                  & Commutativity with reversal \\ \hline
        \end{tabular}
    \end{table}
    Furthermore, if $L^{*}$ is finite we have $\varnothing^{*}=\{\varepsilon\}$ and that $\{\varepsilon\}^{*}=\{\varepsilon\}$. 
    \begin{operation}[Cross operator]
        The \emph{cross operator} is the transitive closure under the concatenation operation. It is defined as the union of all the powers of the 
        base language except the first power $L^0$: 
        \[L^{+}=\bigcup_{h=1\dots\infty}L^h=L^1 \cup L^2 \cup \dots\]
    \end{operation}
    \begin{example}
        Given the language $L=\{ab,ba\}$ we have that the star operation gives the following language: 
        \[L^{*}=\{ab, ba, abab, abba, baab, baba, \dots\}\]
    \end{example}
    \begin{operation}[Language quotient]
        The \emph{quotient operator} shortens the phrases of $L_1$ by cutting off a suffix that belongs to $L_2$:
        \[L=L_1/L_2=\{y|\exists x \in L_1 \exists y \in L_2 (x=yz)\}\]
    \end{operation}
    \begin{example}
        Given the languages $L_1=\{a^{2n}b^{2n}|n>0\}$ and $L_2=\{b^{2n+1}|n \geq 0\}$ the quotient language is: 
        \[L=L_1/L_2=\{aab,aaaab,aaaabbb\}\]
    \end{example}

    \subsection{Regular expressions and languages}
    The family of regular languages is our simplest formal language family. It can be defined in three ways: algebraically, by means of generative grammars, and by means of 
    recognizer automata. 
    \begin{definition}
        A \emph{regular expression} is a string $r$ containing the terminal characters of the alphabet $\Sigma$ and the following meta-symbols: union ($\cup$), concatenation ($.$), 
        star ($^{*}$), empty string ($\varepsilon$), and parenthesis in accordance with the following rules:
        \begin{table}[H]
            \centering
            \begin{tabular}{|cc|}
            \hline
            $r=\varepsilon$ & Empty string                 \\
            $r=a$           & Unitary language             \\
            $r=s \cup t$    & Union of expressions         \\
            $r=(st)$        & Concatenation of expressions \\
            $r=s^{*}$       & Iteration of an expression   \\ \hline
            \end{tabular}
        \end{table}
        where the symbols $s$ and $t$ are regular sub-expression. 
    \end{definition}
    For expressivity, the metasymbol cross is allowed. The operators precedence is: star, concatenation, and union. 
    \begin{definition}
        A \emph{regular language} is a language denoted by a regular expression. 

        The \emph{family of regular languages} (REG) is the collection of all regular languages. 

        The \emph{family of finite languages} (FIN) is the collection of all languages having a finite cardinality
    \end{definition}
    We have that every finite language is regular because it is the union of a finite number of strings each one being the concatenation of a finite 
    number of alphabet symbols. The family of regular languages also includes languages having infinite cardinality (hence $FIN \subset REG$)
    
    The union and repetition operators correspond to possible choices. One obtains a sub-expression by making a choice that identifies a sub-language. Given a regular expression 
    one can derive another one by replacing any outermost sub-expression with another that is a choice of it. 
    \begin{definition}
        We say that a regular expression $e^{'}$ \emph{derives} a regular expression $e^{''}$, written $e^{'} \implies e^{''}$, if the two regular expressions can be factorized as 
        \[e^{'}=\alpha \beta \gamma \:\:\:\:\:\: e^{''}=\alpha \delta \gamma\]
        where $\delta$ is a choice of $\beta$.
    \end{definition}
    The derivation relation can be applied repeatedly, yielding relation $\implies^{n}$ ($n$ steps), $\implies^{*}$ ($n \geq 0$ steps), and $\implies^{+}$ ($n > 0$ steps). 
    \begin{definition}
        Two regular expressions are \emph{equivalent} if they define the same language. 

        A regular expression is \emph{ambiguous} if the language of the numbered version $f^{'}$ includes two distinct strings $x$ and $y$ that coincide when numbers are erased. 
    \end{definition}

    \newpage 

    \chapter{Grammars}
        \section{Context-free generative grammars}
        Regular expressions are very practical for describing lists but fall short of the capacity needed to define other frequently occurring constructs. 
        For defining other useful languages, regular or not, we move to the formal model of generative grammars.  A generative grammar or syntax is a set of multiple 
        rules that can be repeatedly applied in order to generate all and only the valid strings. 
        \begin{definition}
            A \emph{context-free grammar} $G$ is defined by four entities: 
            \begin{enumerate}
                \item $V$ non-terminal alphabet, is the set of non-terminal symbols.
                \item $\Sigma$ terminal alphabet, is the set of the symbols of which phrases or sentences are made.
                \item $P$ is the set of rules or productions.
                \item $S \in V$ is the specific non-terminal, called the axiom ($S$), from which derivations start. 
            \end{enumerate}
        \end{definition}
        A rule of set $P$ is an order pair $X \rightarrow \alpha$, with $X \in V$ and $\alpha \in (V \cup \Sigma)^{*}$. Two or more rules: 
        \[X \rightarrow \alpha_1 \:\:\:\: X \rightarrow \alpha_2 \:\:\:\: \dots \:\:\:\: X \rightarrow \alpha_n\]
        with the same left part $X$ can be concisely groped in:
        \[X \rightarrow \alpha_1 | \alpha_2 | \dots | \alpha_n\]
        We say that the strings $\alpha_1,\alpha_2,\dots,\alpha_n$ are the alternative of $X$. 
    
        \subsection{Conventional grammar representation}
        In professional practice, different styles are used to represent terminals and non-terminals. We usually adopt these conventions: 
        \begin{itemize}
            \item Lowercase Latin letters $\{a,b,\dots\}$ for terminal characters. 
            \item Uppercase Latin letters $\{A,B,\dots\}$ for non-terminal symbols. 
            \item Lowercase Latin letters $\{r,s,\dots,z\}$ for strings over the alphabet $\Sigma$. 
            \item Lowercase Greek letters $\{r,s,\dots,z\}$ for both terminals and non. 
            \item $\sigma$ only for non-terminals. 
        \end{itemize}
        The classification of grammar rule forms is the following. 
        \begin{figure}[H]
            \centering
            \includegraphics[width=1\linewidth]{images/grammars.png}
        \end{figure}
    
        \subsection{Derivation and Language Generation}
        We reconsider and formalize the notion of string derivation. Let $\beta=\delta A \eta$ be a string containing a non-terminal, where $\delta$ and $\eta$ are any, 
        possibly empty strings. Let $A \rightarrow \alpha$ be a rule of $G$ and let $\gamma=\delta\alpha\eta$ be the string obtained replacing in $\beta$ non-terminal $A$with 
        the right part $\alpha$. The relation between such two strings is called derivation. We say that $\beta$ derives $\gamma$ for grammar $G$, written:
        \[\beta \implies \gamma\]
        $A\rightarrow \alpha$ is applied in such derivation and string $\alpha$ reduced to non-terminal $A$. The possible closures are: power ($\implies^n$), 
        reflexive ($\implies^{*}$), and transitive ($\implies^{+}$). 
        \begin{definition}
            If $A \implies^{*} \alpha$ we have that $\alpha \in (V \cup \Sigma)$ is called \emph{string form} generated by $G$. 
    
            If $S \implies^{*} \alpha$ we have that $\alpha$ is called \emph{sentential} or phrase form.
    
            If $A \implies^{*} s$ we have that $s \in \Sigma^{*}$ is called \emph{phrase} or sentence. 
    
            Language is \emph{context-free} if a context-free grammar exists that generates it. 
            
            Two grammars $G$ and $G^{'}$ are \emph{equivalent} if they generate the same language. 
        \end{definition}
    
        \subsection{Erroneous grammars and useless rules}
        When writing a grammar attention should be paid that all non-terminals are defined and that each one effectively contributes to the production of some sentence. 
        In fact, some rules may turn out to be unproductive. 
        \begin{definition}
            A grammar $G$ is called \emph{clean} (or reduced) under the following conditions:
            \begin{enumerate}
                \item Every non-terminal $A$ is reachable from the axiom.
                \item Every non-terminal $A$ is well-defined.
            \end{enumerate}
        \end{definition}
        It is often straightforward to check by inspection whether a grammar is clean. The following algorithm formalizes the checks. The algorithm operates in two phases, 
        first pinpointing the undefined non-terminals, then the unreachable ones. Lastly the rules containing non-terminals of either type can be canceled. The phases are: 
        \begin{enumerate}
            \item Compute the set $DEF\subseteq V$ of well-defined non-terminals. The set $DEF$ is initialized with the non-terminals of terminal rules, those having a 
                terminal string as right part:
                \[DEF:=\{A|( A \rightarrow u ) \in P,with u \in \Sigma^{*}\}\]
                Then the next transformation is applied until convergence is reached:
                \[DEF:=DEF \cup B|( B \rightarrow D_1D_2\dots D_n)\in P\]
                where every $D_i$ is a terminal or a non-terminal symbol present in $DEF$. At each iteration two outcomes are possible:
                \begin{itemize}
                    \item A new non non-terminal is found having as right part a string of symbols that are well-defined non-terminals or terminals. 
                    \item The termination condition is reached
                \end{itemize}
                The non-terminals belonging to the complement set $V-DEF$ are undefined and should be eliminated.
            \item A non-terminal is reachable from the axiom, if, and only if, there exists a path in the following graph, which represents a relation between non-terminals,
                called product:
                \[A \rightarrow^{produce} B\]
                saying that $A$ produces $B$ if, and only if, there exists a rule $A \rightarrow \alpha B \beta$, where $A,B$ are non-terminals and $\alpha,\beta$ are any strings.
                Clearly $C$ is reachable from $S$ if, and only if, in this graph there exists an oriented path from $S$ to $C$. The unreachable non-terminals are the complement
                with respect to $V$. They should be eliminated because they do not contribute to the generation of any sentence.
        \end{enumerate}
        Quite often the following requirement is added to the above clearness conditions: $G$ should not permit circular deviations $A \implies^{+} A$. This is done to avoid 
        ambiguity. We observe that a grammar, although clean, may still contain redundant rules. 
    
        \subsection{Recursion and language infinity}
        An essential property of most technical languages is to be infinite. We study how this property follows from the form of grammar rules. In order to generate an 
        unbound number of strings, the grammar must be able to derive strings of unbound length. To this end, recursive rules are necessary, as next argued. An $n \geq 1$ 
        steps derivation $A \implies^{n}xAy$ is called recursive (immediately recursive if $n=1$); similarly non-terminal $A$ is called recursive. If $x$ is empty, the recursion 
        is termed left.
    
        Let $G$ be a grammar clean and avoid of circular deviations. The language $L ( G )$ is infinite if, and only if, $G$ has a recursive derivation.
    
        \subsection{Syntax trees and canonical derivations}
        \begin{definition}
            A \emph{tree} is an oriented and ordered graph not containing a circuit, such that every pair of nodes is connected by exactly one oriented path.
            
            An \emph{arc} $\langle N_1,N_2 \rangle$ define the $\langle \textnormal{father,son} \rangle$ relation, customarily visualized from top to bottom as in genealogical 
            trees. The sides of a node are ordered from left to right. 
    
            The \emph{degree} of a node is the number of its siblings. 
            
            A \emph{tree} contains one node without father, termed root.
    
            Consider an internal node $N$: the subtree with root $N$ is the tree having $N$ as root and containing all descendants of $N$. Nodes without sibling are termed leaves or \emph{terminal nodes}. 
        
            The sequence of all leaves, read from left to right, is the \emph{frontier} of the tree.
    
            A \emph{syntax tree} has as root the axiom and as frontier a sentence.
        \end{definition}
        A syntax tree of a sentence $x$ can also be encoded in a text, by enclosing each subtree between brackets. Brackets are subscribed with the non-terminal symbol. The representation can be simplified 
        by dropping the non-terminal labels, thus obtaining a skeleton tree. A further simplification of the skeleton tree consists in shortening non bifurcating paths, resulting in the condensed skeleton tree. 
    
    
        \subsection{Left and right derivations}
        We can have right (expands at each step the rightmost non-terminal) and left derivation (expands at each step the leftmost non-terminal). 
        However, for a fixed syntax tree of a sentence, there exist a unique right derivation, and a unique left derivation matching that tree. Right and left derivation are useful to define parsing algorithms. 
    
        \subsection{Parenthesis languages}
        Many  artificial  languages  include  parenthesized  or  nested  structures,  made  by matching pairs of opening/closing marks. Any such occurrence may contain other matching pairs.
        The marks are abstract elements that have different concrete representations indistinct settings.
        \begin{definition}
            When a marked construct may contain another construct of the same kind, it is called \emph{self-nested}.
        \end{definition}
        Self-nesting is potentially unbounded in artificial languages, whereas in natural languages its use is moderate, because it causes difficulty of comprehension by breaking the flow of discourse. 
        Abstracting from concrete representation and content, this paradigm is known as a Dyck language. The terminal alphabet contains one or more pairs of opening/closing marks. 
        Dyck sentences are characterized by the following cancelation rule that checks parentheses are well nested: given a string, repeatedly substitute the empty string for a pair of adjacent matching parentheses:
        \[[\:]\implies\varepsilon \:\:\:\:\:\: (\:)\implies\varepsilon\]
        Thus obtaining another string. Repeat until the transformation no longer applies; the original string is correct if, and only if, the last string is empty.
        \begin{definition}
            Let $G=(V,\Sigma,P,S)$ be a grammar with an alphabet $\Sigma$ not containing parentheses. The \emph{parenthesized grammar} $G_p$ has alphabet $\Sigma \cup \{'(',')'\}$ and rules:
            \[A \rightarrow (\alpha) \textnormal{ where } A \rightarrow (\alpha) \textnormal{ is a rule of } G\]
            The grammar is distinctly parenthesized if every rule has form:
            \[A \rightarrow (_A \alpha)_A \:\:\:\:\:\: B \rightarrow (_B \alpha)_B\]
            where $(_A$ and $)_A$ are parentheses subscripted with the non-terminal name.
        \end{definition}
        Clearly each sentence produced by such grammars exhibits parenthesized structure. A notable effect of the presence of parentheses is to allow a simpler checking of string correctness. 
    
        \subsection{Regular composition of context-free languages}
        If the basic operations of regular languages, union, concatenation, and star, are applied to context-free languages, the result remains a member of the CF family. Let $G_1=(\Sigma_1,V_1,P_1,S_1)$
        and $G_2=(\Sigma_2,V_2,P_2,S_2)$ be the grammars defining languages $L_1$ and $L_2$. We need the not restrictive hypothesis that non-terminal sets are disjoint. Moreover, we stipulate that symbol $S$, 
        to be used as axiom of the grammar under construction, is not used by either grammar, $S \notin (V_1 \cup V_2)$.
        \begin{operation}[Union]
            The union $L_1 \cup L_2$ is defined by the grammar containing the rules of both grammars, plus the initial rules $S\rightarrow S_1|S_2$. In formulas, the grammar is: 
            \[G=\left(\Sigma_1 \cup \Sigma_2,\{S\} \cup V_1 \cup V_2,\{S\rightarrow S_1|S_2\} \cup P_1 \cup P_2,S\right)\]
        \end{operation}
        \begin{operation}[Concatenation]
            The concatenation $L_1L_2$ is defined by the grammar containing the rules of both grammars, plus the initial rule $S\rightarrow S_1S_2$. The grammar is: 
            \[G=\left(\Sigma_1 \cup \Sigma_2,\{S\} \cup V_1 \cup V_2,\{S\rightarrow S_1S_2\} \cup P_1 \cup P_2,S\right)\] 
        \end{operation}
        \begin{operation}[Star]
            The grammar $G$ of the starred language $(L1)^{*}$ includes the rules of $G_1$ and rules $S \rightarrow SS_1|\varepsilon$.
        \end{operation}
        \begin{operation}[Cross]
            From the identity $L^{+}=L.L^{*}$, the grammar of the cross language could be written applying the concatenation construction to $L$ and $L^{*}$, but it is better to produce the grammar directly. 
            The grammar $G$ of language $(L1)^{+}$ contains the rules of $G_1$ and rules $S \rightarrow SS_1|S1$. 
        \end{operation}
        The family CF of context-free languages is closed by union, concatenation, star, and cross. Examining the effect of string reversal on the sentences of a CF language, one immediately sees the family 
        is closed with respect to reversal (the same as family REG). Given a grammar, the rules generating the mirror language are obtained reversing every right part of a rule.

        \subsection{Ambiguity}
        The common linguistic phenomenon of ambiguity in natural language shows up when a sentence has two or more meanings. Ambiguity is of two kinds, semantic or syntactic.
        \begin{definition}
            A sentence $x$ defined by grammar $G$ is \emph{syntactically ambiguous}, if it is generated with two different syntax trees. Then the grammar too is called ambiguous.

            The \emph{degree of ambiguity} of a sentence $x$ of  language $L(G)$ is the number of distinct syntax trees deriving the sentence. For a grammar the degree of ambiguity is the maximum degree of 
            any ambiguous sentence.
        \end{definition}
        The ambiguity can be: 
        \begin{itemize}
            \item From bilateral recursion
        \end{itemize}


        PAG 47 A 79

        \subsection{Grammar transformations and normal forms}
        The grammars can be transformed in the following ways: 
        \begin{itemize}
            \item 
        \end{itemize}

\newpage

\chapter{Finite state automata}
    \section{Recognition algorithms and automata}
    To check if a string is valid for a specified language, we need a recognition algorithm, a type of algorithm producing a yes/no answer, commonly referred to in 
    computational complexity studies as a decision algorithm. For the string membership problem, the input domain is a set of strings of alphabet $\Sigma$. The 
    application of a recognition algorithm $\alpha$ to a given string $x$ is denoted as $\alpha(x)$. We say string $x$ is recognized or accepted if $\alpha(x)=yes$, 
    otherwise it is rejected. The language recognized, $L(\alpha)$, is the set of accepted strings:
    \[L(\alpha)=\{x \in \Sigma^{*}|\alpha(x)=yes\}\]
    The algorithm is usually assumed to terminate for every input, so that the membership problem is decidable. However, it may happen that, for some string $x$,the 
    algorithm does not terminate. In such case we say that the membership problem for $L$ is semi-decidable, or also that $L$ is recursively enumerable. In practice, 
    we do not have to worry about such decidability issues because in language processing the only language families of concern are decidable. 
    \subsection{A general automaton}
    An automaton or abstract machine is an ideal computer featuring a very small set of simple instructions. In its more general form a recognizer it is composed by three 
    parts: input tape, control unit, and (auxiliary) memory. The control unit has a limited store, to be represented as a finite set of states;the auxiliary memory, on the 
    other hand, has unbounded capacity. The upper tape contains the given input or source string, which can be read but not changed. Each case of the tape contains a 
    terminal character; the cases to the left and right of the input contain two delimiters, the start of text mark $\vdash$ and the end of text mark or terminator $\dashv$.
    A peculiarity of automata is that the auxiliary memory is also a tape containing symbols of another alphabet. The automaton examines the source by performing a series
    of moves; the choice of a move depends on the current two symbols (input  and  memory) and on the current state. A move may have some of the following effects:
    \begin{itemize}
        \item Shift the input head left or right by one position. 
        \item Overwrite the current memory symbol with another one, and shift the memory head left or right by one position. 
        \item Change the state of the control unit.
    \end{itemize}
    \begin{definition}
        A machine is \emph{unidirectional} if the input head only moves from left to right. 
    \end{definition}
    At any time the future behavior of the machine depends on a three-tuple, called configuration: the suffix of the input string still to be read, the contents of the 
    memory tape and the position of the head. 
    \begin{definition}
        The \emph{initial configuration} has: the input head positioned on character $a_1$, the control unit in an initial state, and the memory containing a specific 
        symbol.
    \end{definition}
    Then the machine performs a computation. If for a configuration at most one move can be applied, the change of configuration is deterministic. A non-deterministic
    automaton is essentially a manner of representing an algorithm that in some situation may explore alternative paths. 
    \begin{definition}
        A configuration is \emph{final} if the control is in a state specified as final, and the input head is on the terminator. 
    \end{definition}
    The source string $x$ is accepted if the automaton, starting in the initial configuration with $x\dashv$ as input, performs a computation leading to a final configuration. 
    The language accepted or recognized by the machine is the set of accepted strings.

    Notice a computation terminates either when the machine has entered a final con-figuration or when in the current configuration no move can be applied. In the latter 
    case the source string is not accepted by that computation. 
    \begin{definition}
        Two automata accepting the same language are called \emph{equivalent}.
    \end{definition}

    \section{Introduction to finite automata}
    Conforming to the general scheme, a finite automaton comprises: the input tape with the source string $x \in \Sigma^{*}$, the control unit, and the reading head scanning 
    the string until its end, unless an error occurs before. Upon reading a character, the automaton updates the state  of the  control unit and advances the reading head. 
    Upon reading the last character, the automaton accepts $x$ if and only if the state is an accepting one.

    A well-known representation of an automaton is by a state-transition diagram or graph. This is a directed graph whose nodes are the states of the control unit. Each arc
    is labeled with a terminal and represents the change of state or transition caused by reading the terminal.

    An automaton may have several final states, but only one initial state. 

    \section{Deterministic finite automata}
    \begin{definition}
        A \emph{finite deterministic automaton M} comprises five items:
        \begin{enumerate}
            \item $Q$, the state set (finite and not empty). 
            \item $\Sigma$, the input or terminal alphabet
            \item $\delta:(Q \times \Sigma) \rightarrow Q$, the transition function.
            \item $q_0 \in Q$, the initial state. 
            \item $F\subseteq Q$, the set of final states.
        \end{enumerate}
    \end{definition}
    Function $\delta$ specifies the moves: the meaning of $\delta(q,a)=r$ is that machine $M$ in the current state $q$ reads $a$ and moves to next state $r$. 
    If $\delta(q,a)$ is undefined, the automaton stops, and we can assume it enters the error state. 

    A special case is the empty string, for which we assume no change of state: 
    \[\forall q \in Q:\delta(q,\varepsilon)=q\]
    \begin{definition}
        The languages accepted by such automata are called \emph{finite-state recognizable}.
        
        Two automata are \emph{equivalent} if they accept the same language.
    \end{definition}
    Observing that for each input character the automaton executes one step, the total number of steps is exactly equal to the length of the input string. Therefore, such
    machines are very efficient as they can recognize strings in real time by a single left-to-right scan.

    \subsection{Error state and total automata}
    If the move is not defined in state $q$ when reading character $a$, we say that the automaton falls into the error state $q_{err}$. The error state is such that for 
    any character the automaton remains in it, thus justifying its other name of sink or trap state. Obviously the error state is not final. The state-transition function 
    can be made total by adding the error state and the transitions from/to it. 

    Clearly any computation reaching the error state gets trapped in it and cannot reach a final state. As a consequence, the total automaton accepts the same language as 
    the original one. It is customary to leave the error state implicit, neither drawing a node nor specifying the transitions for it.

    \subsection{Clean automata}
    An automaton may contain useless parts not contributing to any accepting computation, which are best eliminated.
    \begin{definition}
        A state $q$ is \emph{reachable} from state $p$ if a computation exists going from $p$ to $q$.

        A state is \emph{accessible} if it can be reached from the initial state. 

        A state is \emph{post-accessible} if a final state can be reached from it. 

        A state is called \emph{useful} if it is accessible and post-accessible. 

        An automaton is \emph{clean} if every state is useful.
    \end{definition}
    For every finite automaton there exists an equivalent clean automaton.

    \subsection{Minimal automata}
    For every finite-state language, the deterministic finite recognizer minimal with respect to the number of states is unique.
    \begin{definition}
        The states $p$ and $q$ are \emph{indistinguishable} if, and only if, for every string $x \in \Sigma^{*}$, either both states $\delta(p,x)$ and $\delta(q,x)$ are 
        final, or neither one is. 
        
        The complementary relation is termed \emph{distinguishability}.
    \end{definition}
    Two states $p$ and $q$ are indistinguishable if, starting from them and scanning the same arbitrarily chosen input string $x$, it never happens that a computation 
    reaches a final state and the other does not. Notice that: 
    \begin{enumerate}
        \item The sink state $q_{err}$ is distinguishable from every state $p$, since for any state there exists a string $x$ such that $\delta(p,x) \in F$, while for 
            every string $x$ it is $\delta(q_{err},x)=q_{err}$. 
        \item $p$ and $q$ are distinguishable if $p$ is final and $q$ is not, because $\delta(p,\varepsilon)\in F$ and $\delta(q,\varepsilon) \notin F$. 
        \item $p$ and $q$ are distinguishable if, for some character $a$, the next states $\delta(p,a)$ and $\delta(q,a)$ are distinguishable.
    \end{enumerate}
    In particular, $p$ is distinguishable from $q$ if the set of labels attached to the outgoing arrows from $p$ and the similar set from $q$ are different. 

    Indistinguishability as a relation is symmetric, reflexive, and transitive. 

    \subsection{Construction of minimal automaton}
    The minimal automaton $M^{'}$, equivalent to the given $M$, has for states the equivalence classes of the indistinguishability relation. From this it is a 
    straightforward test to check whether two given machines are equivalent. First minimize both machines; then compare their state-transition graphs to see if 
    they are identical. In practical use, obvious economy reasons make the minimal machine a prefer-able  choice. But the saving is often negligible for  
    the cases of concern in compiler design. What is more, in certain situations state minimization of the recognizer should be avoided. The uniqueness property 
    of the minimal automaton does not hold for the nondeterministic machines. 

    \subsection{From automaton to grammars}
    The grammar $G$ has as non-terminal set the states $Q$ of the automaton, and the axiom is the initial state. For each move $q \rightarrow^{a} r$ the grammar 
    has the rule $q \rightarrow ar$. If state $q$ is final, it has also the terminal rule $q \rightarrow \varepsilon$. It is evident that there exists a bijective 
    correspondence between the computations of the automaton and the derivations of the grammar. 
    \begin{example}
        The correspondence between an automaton and a grammar is shown below. 
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.9\linewidth]{images/autgram.png}
        \end{figure}
    \end{example}
    The conversion from automaton to grammar has been straightforward, but to make the reverse transformation from grammar to automaton, we need to 
    modify the machine definition by permitting nondeterministic behavior.
        
    \section{Nondeterministic automata}
    A right-linear grammar may contain two alternative rules starting with the same character. In this case, converting the rules to machine transitions, two arrows with 
    identical label would exit from the same state $A$ and enter two distinct states $B$ and $C$. This means that in state $A$, reading the character, the machine can 
    choose which one of the next states to enter: its behavior is not deterministic. A machine move that does not read an input character is termed spontaneous or an epsilon 
    move. Spontaneous moves too cause the machine to be nondeterministic. 

    \subsection{Motivation of non-determinism}
    The main advantages of this are: 
    \begin{itemize}
        \item Concision: defining a language with a nondeterministic machine often results in a more read-able and compact definition. 
        \item Left right interchange and language reflection: it is useful when a deterministic machine is used to recognize the reflection. 
        \item Converting regular expressions to automaton. 
    \end{itemize}

    \subsection{Nondeterministic recognizers}
    \begin{definition}
        A \emph{non-deterministic finite automaton} $N$, without spontaneous moves, is defined by: 
        \begin{itemize}
            \item The state set $Q$. 
            \item The terminal alphabet $\Sigma$. 
            \item Two subsets of $Q$: the set $I$ of the initial states and the set $F$ of final states.
            \item The transition relation $\delta$, a subset of the Cartesian product $Q\times\Sigma\times Q$.     
        \end{itemize}
\end{definition} 
    As before, a computation is a series of transitions such that the origin of each one coincides with the destination of the preceding one. The computation
    origin is $q_0$, the termination is $q_n$, and the length is the number $n$ of transitions or moves. A computation of length 1 is just a transition.
    A string $x$ is recognized or accepted by the automaton, if it is the label of a computation originating in some initial state, terminating in some final state, 
    and having label $x$. The language $L(N)$ recognized by automaton $N$ is the set of accepted strings. The moves of a nondeterministic automaton can still be 
    considered as a finite function, but one computing sets of values. For a machine $N=(Q,\Sigma,\delta,I,F)$, devoid of spontaneous moves, the functionality of the 
    state-transition function $\delta$ is the following: 
    \[\delta:Q\times\left(\Sigma\cup\{\varepsilon\}\right)\rightarrow \mathcal{P}(Q)\]
    where symbol $\mathcal{P}(Q)$ indicates the power set of set $Q$. 

    \subsection{Automata with spontaneous moves}
    Another kind of nondeterministic behavior occurs when an automaton changes state without reading a character, thus performing a spontaneous move. In this case 
    the number of steps of the computation can exceed the length of the input string, because of the presence of $\varepsilon$-arcs. As a consequence, the recognition 
    algorithm no longer works in real time. Yet time complexity remains linear, because it is possible to assume that there are no cycles of spontaneous moves in any 
    computation. The family of languages recognized by such nondeterministic automata is also called finite-state.

    The official definition of nondeterministic machine allows two or more initial states, but it is easy to construct an equivalent machine with only one: add to the
    machine anewstateq0, which will be the only initial state, and the $\varepsilon$-arcs going from it to the former initial states of the automaton.

    \subsection{Correspondence between automata and grammars}
    Consider a right-linear grammar $G=(V,\Sigma,P,S)$ and a nondeterministic automaton $N=(Q,\Sigma,\delta,q_0,F)$, which we may assume from the preceding discussion to 
    have a single initial state. First assume the grammar rules are strictly unilinear. The states $Q$ of the automaton match the non-terminals $V$ of the grammar. The 
    initial state corresponds to the axiom. Notice that the pair of alternatives $p\rightarrow aq|ar$ correspond to two nondeterministic moves. A copy rule matches a 
    spontaneous move. A final state  matches a non-terminal having an empty rule.
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.75\linewidth]{images/correspondence.png}
        \caption{Correspondence between automaton and grammar}
    \end{figure}

    \subsection{Ambiguity of automata}
    \begin{definition}
        An automaton is \emph{ambiguous} if it accepts a string with two different computations.
    \end{definition}
    Clearly it follows from the definition that a deterministic automaton is never ambiguous. We also have that an automaton is ambiguous if, and only if, the right-linear 
    equivalent grammar is ambiguous. 

    REG families can be defined also using left-linear grammars. By interchanging left with right, it is simple to discover the mapping between such grammars and automata.

    \section{From automaton to regular expression: the BMC method}
    Suppose for simplicity the initial state $i$ is unique, and no arc enters in it; similarly the final state $t$ is unique and without outgoing arcs. Otherwise, just add
    a new initial state $i$ connected by spontaneous moves to the ex-initial states; similarly introduce a new unique final state $t$. Every state other than $i$ and $t$ is 
    called internal. We construct an equivalent automaton, termed generalized,  which is more flexible as it allows arc labels to be not just terminal characters, but also 
    regular languages. The idea is to eliminate one by one the internal states, while compensating by introducing new arcs labeled with regular expression, until only the initial and final 
    states are left. Then the label of arc $i \rightarrow t$ is the regular expression of the language.
    \begin{example}
        The BMC method applied to a simple automaton: 
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.6\linewidth]{images/brzozowski.png}
        \end{figure}
    \end{example}

    \section{Elimination of non-determinism}
    Every non-deterministic finite automaton can always be transformed into an equivalent deterministic one. Consequently, every right linear grammar always admits an 
    equivalent non-ambiguous right linear one. Thus,  every ambiguous regular expression can always be transformed into a non-ambiguous one. The algorithm to transform a 
    non-deterministic automaton into a deterministic one is structured in two phases: 
    \begin{enumerate}
        \item Elimination of the spontaneous moves. As such moves correspond to copy rules, it suffices to apply the algorithm for removing the copy rules. 
        \item Replacement of the non-deterministic multiple transitions by changing the automaton state set. This is the well known subset construction. 
    \end{enumerate}
    \begin{example}
        Given the following automaton: 
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.5\linewidth]{images/oaut.png}
        \end{figure}
        After applying the algorithm we have: 
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.5\linewidth]{images/faut.png}
        \end{figure}
    \end{example}

    \section{From a regular expression to a finite state automaton}
    There are a few algorithms to transform a regular expression into an automaton, which differ as for automaton characteristic. 
    
    \subsection{Thompson structural method}
    Wit the Thompson structural method, given a regular expression, we analyze it into simple parts, we produce corresponding component automata, and we interconnect them 
    to obtain the complete recognizer. In  this  construction each  component machine is  assumed to  have  exactly one initial state without incoming arcs and one final 
    state without outgoing arcs. 
    \begin{figure}[H]
        \centering
        \includegraphics[width=1\linewidth]{images/thompson.png}
        \caption{Sub-expression to automaton}
    \end{figure}
    The validity of Thompson's method comes from it being an operational reformulation of the closure properties of regular languages under concatenation, union, and star.
    In general the outcome of the Thompson method is a non-deterministic automaton with spontaneous moves. There are various optimizations of the Thompson method that avoid 
    creating redundant states. 

    \subsection{Glushkov-McNaughton-Yamada algorithm}
    The GMY algorithm constructs the automaton equivalent to a given regular expression, with states that are in a one-to-one correspondence with the generators that occur
    in the regular expression. 

    \begin{definition}
        Given a language $L$ over the alphabet $\Sigma$ we can define: 
        \begin{itemize}
            \item The set of initials: $Ini(L)=\{a \in \Sigma | a\Sigma^{*}\cap L \neq \varnothing\}$. 
            \item The set of finals: $Fin(L)=\{a \in \Sigma | \Sigma^{*}a\cap L \neq \varnothing\}$.
            \item The set of digrams: $Dig(L)=\{x \in \Sigma^{2} | \Sigma^{*}x\Sigma^{*} \cap L \neq \varnothing\}$.
            \item The set of forbidden digrams: $\overline{Dig(L)}=\Sigma^{2}-Dig(L)$
        \end{itemize}
        The language $L$ is called \emph{local} or \emph{locally testable}, if and only if it satisfies the following identity: 
        \[L-\{\varepsilon\}=\{x|Ini(x)\in Ini(L) \land Fin(x)\in Fin(L)\land Dig(x)\subseteq Dig(L)\}\]
    \end{definition}
    To design the recognizer of a local language we scan the input string from left to right and check whether: the initial character belongs to the set $Ini$, every 
    digram belongs to the set $Dig$, and the final character belongs to the set $Fin$. The string is accepted if, and only if, all the above checks succeed. 

    We can implement the above recognizer by resorting to a sliding window with a width of two characters, which is shifted over the input string from left to right.
    At each shift step the window contents are checked, and if the window reaches the end of the string and all the checks succeed, then the string is accepted, otherwise
    it is rejected. This sliding window algorithm is simple to implement by means of a non-deterministic automaton. 
    \begin{definition}
        A regular expression is said to be \emph{linear} if there is not any repeated generator. 
    \end{definition}
    The idea of the GMY algorithm, based on the linear regular expressions is the following: 
    \begin{enumerate}
        \item Denumerate the regular expression e and obtain the linear regular expression $e_{\#}$. 
        \item Compute the three characteristic local sets $Ini$, $Fin$ and $Dig$ of $e_{\#}$.
        \item Design the recognizer of the local language generated by $e_{\#}$.
        \item Cancel the indexing and thus obtain the recognizer of $e$.
    \end{enumerate}
    
    \subsection{Berry-Sethi method}
    In order to obtain the deterministic recognizer, we can just apply the subset construction to the non-deterministic recognizer built by the GMY algorithm. However, there 
    is a more direct algorithm called Berry-Sethi. The idea at the base of this algorithm is the following: 
    \begin{enumerate}
        \item From the original regular expression $e$ over alphabet $\Sigma$ derive the linear expression $e^{'}\dashv$, where $e^{'}$ is the numbered version of $e$ 
            and $\dashv$ is a string terminator symbol, with $\dashv \notin \Sigma$.
        \item Build the local automaton recognizing the local language $L(e^{'}\dashv)$: this automaton includes the initial state $q_0$, one non-initial and non-final
            state for each element of $\Sigma_N$, and a unique final state $\vdash$.
        \item Label each state of the automaton with the set of the symbols on its outgoing edges. The initial state $q_0$ is labeled with $Ini(e^{'}\dashv)$, the final 
            state $\dashv$ is labeled with the empty set $\varnothing$. For each non-initial and non-final states $c$, $c \in \Sigma_N$, the set labeling that state is 
            called the set of followers of symbol $c$, $Fol(c)$,in the expression $e^{'}\dashv$; it is derived directly from the local set of digrams as follows:
            $Fol(a_i)=\{b_j|a_ib_j \in Dig(e^{'}\dashv)\}$. $Fol$ is equivalent to the $Dig$ local set and, together with the other two local sets $Ini$ and $Fin$, 
            characterizes a local language.
        \item Merge any existing states of the automaton that are labeled by the same set. The obtained automaton is equivalent to the previous one: since the recognized 
            language is local, states marked with equal sets of followers are indistinguishable
        \item Remove the numbering from the symbols that label the transitions of the automaton: the resulting automaton, which may be nondeterministic, accepts by 
            construction the language $L(e^{'}\dashv)$.
        \item Derive a deterministic, equivalent automaton by applying the construction of Accessible Subsets; label the sets resulting from the union of several states of 
            the previous nondeterministic automaton with the union of the sets labeling the merged states. The resulting deterministic automaton recognizes $L(e^{'}\dashv)$.
        \item Remove from the automaton the final state (labeled by $\varnothing$) and all arcs entering it; define as final states of the resulting automaton those labeled 
            by a set that includes the $\dashv$ symbol; the resulting automaton is deterministic and recognizes $L(e)$.
    \end{enumerate}
    \begin{algorithm}[H]
        \caption{Berry-Sethi algorithm}
            \begin{algorithmic}[1]
                \State $q_0 \leftarrow Ini(e_{\#} \dashv)$
                \State $Q \leftarrow \{q_0\}$
                \State $\delta \leftarrow \varnothing$
                \While {$\exists q \in Q$ such that $q$ is unmarked}
                    \State mark state $q$ as visited
                    \For {each character $c \in \Sigma$}
                        \State $q^{'} \leftarrow \bigcup_{\forall c_{\#} \in \Sigma_{c_{\#}}}Fol(c_{\#})$
                        \If {$q^{'} \neq \varnothing$}
                            \If {$q^{'} \notin Q$}
                                \State set $q^{'}$ as a new unmarked state
                                \State $Q \leftarrow Q \cup \{q^{'}\}$
                            \EndIf
                            \State $\delta \leftarrow Q \cup \{q^{'}\}$
                        \EndIf
                    \EndFor
                \EndWhile
            \end{algorithmic}
    \end{algorithm}
    \begin{example}
        Given the language $L=(a|bb)^{*}(ac)^{+}$ apply the BS algorithm. First we enumerate the string: 
        \[e_{\#}=(a_1|b_2b_3)^{*}(a_4c_5)^{+} \dashv\]
        And with the table we obtain:
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.8\linewidth]{images/BS.png}
        \end{figure}
    \end{example}
    Another use of algorithm BS is as an alternative to the power set construction, for converting a nondeterministic machine $N$ into a deterministic one $M$. The steps are: 
    \begin{enumerate}
        \item Distinctly number the labels of non-$\varepsilon$ arcs of $N$, obtaining automaton $N^{'}$.
        \item Compute the local sets $Ini$, $Fin$, and $Fol$ for the language $L(N^{'})$. These can be easily derived from the transition graph, possibly exploiting the 
            identity $\varepsilon a=a\varepsilon=a$.
        \item Applying the BS construction to the sets $Ini$, $Fin$, and $Fol$, produce the deterministic automaton $M$.
    \end{enumerate}

    \section{Regular expression: complement and intersection}
    Regular expressions may also contain the operators of complement, intersection and set difference, which are very useful to make the regexp more concise. 
    Let $L$ and $L^{'}$ be regular languages. The complement $\lnot L$ and the intersection$L \cap L^{'}$ are regular languages. The deterministic recognizer $\overline{M}$ 
    of the complement language requires to complete the automaton $M$ by adding the error state p and the missing moves: 
    \begin{itemize}
        \item Create the error state $p$, not in $Q$, so the states of $\overline{M}$ are $Q \cup \{ p \}$
        \item The transition function $\delta$ is: 
            \begin{itemize}
                \item $\delta(q,a)=\delta(q,a)$, where $\delta(q,a) \in Q$. 
                \item $\delta(q,a)=p$, where $\delta(q,a)$ is not defined;
                \item $\delta(p,a)=p$, for every character $a \in \Sigma$;
            \end{itemize}
        \item Swap the non-final and final states. 
    \end{itemize}
    \begin{example}
        Find the complement of the given automaton: 
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.8\linewidth]{images/complement.png}
        \end{figure}
    \end{example}
    For the complement construction to work correctly, the original automaton must be deterministic, otherwise the original and complement languages may be not disjoint,
    which fact would be in violation of the complement definition. The complement automaton may contain useless states and may not be in the minimal form either; it should 
    be reduced and minimized, if necessary. 
    \subsection{Product of automata}
    A very common construction of formal languages, where a single automaton simulates the computation of two automata that work in parallel on the same input string. 
    It is very useful to construct the intersection automaton. To obtain the intersection automaton we can resort to the De Morgan theorem. The Cartesian product can 
    also be obtained by a more direct construction. The intersection of the two languages is recognized directly by the Cartesian product of their automata. 
    Suppose both automata do not contain any spontaneous moves. The state set of the product machine is the Cartesian product of the state sets
    of the two automata. Each product state is a pair $\left\langle q^{'},q^{''} \right\rangle $, where the left (right) member is a state of the first (second) machine. 
    The move is: 
    \[\left\langle q^{'},q^{''} \right\rangle \rightarrow^a \left\langle r^{'},r^{''} \right\rangle \textnormal{ if and only if } q^{'} \rightarrow r^{'} \textnormal{ and } q^{''} \rightarrow r^{''}\]
    The product machine has a move if, and only if, the projection of such a move onto the left (right) component is a move of the first (second) automaton. The initial 
    and final state sets are the Cartesian products of the initial and final state sets of the two automata, respectively. The product construction is equivalent to simulating both 
    machines in parallel.
    \begin{example}
        The intersection can be found as follows: 
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.52\linewidth]{images/intersection.png}
        \end{figure}
    \end{example}

    \newpage

    \chapter{Pushdown automata}
    \section{Introduction}
    Any compiler includes a recognition algorithm which is essentially a finite automaton enriched with an auxiliary memory organized as a pushdown or LIFO stack of 
    unbounded capacity, which stores the symbols. The input or source string, delimited on the right end by an end-marker $\dashv$, is: 
    \[a_1a_2\dots a_i\dots a_n\dashv\]
    The following operations apply to a stack:
    \begin{itemize}
        \item Push: places the symbol(s) onto the stack top. 
        \item Pop: removes symbol from the stack top, if the stack is not empty; otherwise reads $Z_0$. 
        \item Stack emptiness test: true if the stack is empty, false otherwise. 
    \end{itemize}
    The symbol $Z_0$ is the stack bottom and can be read but not removed. At each instant the machine configuration is specified by: the remaining portion of the input 
    string still to be read, the current state, and the stack contents. With a move the pushdown automaton: 
    \begin{itemize}
        \item Reads the current character and shifts the input head, or performs a spontaneous move without shifting the input head. 
        \item Reads the stack top symbol and removes it from the top if the stack is not empty, or reads the stack symbol $Z_0$ if the stack is empty. 
        \item Depending on the current character, state and stack top symbol, it goes into the next state and places none, one or more symbols onto the stack top. 
    \end{itemize}
    \begin{definition}
        A pushdown automaton $M$ is defined by:
        \begin{itemize}
            \item $Q$ a finite set of states of the control unit.
            \item $\Sigma$ a finite input alphabet.
            \item $\Gamma$ a finite stack alphabet.
            \item $\delta$ a transition function.
            \item $q_0 \in Q$ the initial state.
            \item $Z_0 \in \Gamma$ the initial stack symbol.
            \item $F \subseteq Q$ a set of final states.
        \end{itemize}
    \end{definition}
    The domain and range of the transition function are made of Cartesian products:
    \begin{itemize}
        \item Domain: $Q \times \left(\Sigma \cup \{\varepsilon\}\right) \times \Gamma$. 
        \item Range: the set of the subsets of $Q \times \Gamma^{*}$. 
    \end{itemize}
    The possible moves are: 
    \begin{itemize}
        \item Reading move: in the state $q$ with symbol $Z_0$ on the stack top, the automaton reads char $a$ and enters one of the states $p_i$ with $1 \leq i \leq n$, 
            after orderly executing the operations pop and push ($\gamma_i$): 
            \[\delta(q,a,Z)=\{(p_1,\gamma_1), (p_2,\gamma_2),\dots,(pn,\gamma_n)\}\]
        \item Spontaneous move: in the state $q$ with symbol $Z_0$ on the stack top, the automaton does not read any input character and enters one of the states $p_i$
            with $1 \leq i \leq n$, after orderly executing the operations pop and push ($\gamma_i$): 
            \[\delta(q,\varepsilon,Z)=\{(p1,\gamma_1), (p2,\gamma_2),\dots,(pn,\gamma_n)\}\]
    \end{itemize}
    There is non-determinism: for a triple (state, input, stack top) there are two or more possible moves that consume none or one input character. 
    \begin{definition}
        The \emph{instantaneous configuration} of a machine $M$ is a 3-tuple: 
        \[(q,y,\eta)\in Q \times \Gamma^{*} \times \Gamma^{+}\]
        which specifies:
        \begin{itemize}
            \item $q$, the current state,
            \item $y$, the remaining portion (suffix) of the source string $x$ to be read.
            \item $\eta$, the stack content.
        \end{itemize}

        The \emph{initial} configuration of machine $M$ is: 
        \[(q_0,x,Z_0)\]

        The \emph{final} configuration of machine $M$ is: 
        \[(q,\varepsilon,\lambda)\]
    \end{definition}
    Applying a move, a transition from a configuration to another occurs, to be denoted as: 
    \[(q,y,\eta)\rightarrow(p,z,\lambda)\]
    Note that a chain of one or more transitions is denoted by $\rightarrow^{+}$. An input string $x$ is accepted by final state if there is the following computation
    \[(q_0,x,Z_0)\mapsto^{*}(q,\varepsilon,\lambda)\]
    where $q \in F$ and $\lambda\in\Gamma^{*}$, whereas there is not any specific condition for $\lambda$; sometimes $\lambda$ happens to be the empty string, but this 
    is not necessary. 

    \subsection{State-transition diagram for PDA}
    The transition function of a finite automaton can be graphically presented, although its readability is somewhat lessened by the need to specify stack operations.
    \begin{example}
        The language $L=\{uu^R|u \in \{a,b\}^{*}\}$ of the palindromes of even length is accepted with final state by the pushdown recognizer. 
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.5\linewidth]{images/PDA.png}
        \end{figure}
    \end{example}

    \subsection{From a grammar to a PDA}
    Grammar rules can be viewed as the instructions of a non-deterministic pushdown automaton. Intuitively such an automaton works in a goal-oriented way and uses the 
    stack as a notebook of the sequence of actions to undertake in the next future. The stack symbols can be both terminals and non-terminals of the grammar. If the
    stack contains the symbol sequence $A_1 \dots A_k$, then the automaton executes first the action associated with $A_k$, which should recognize if in the input string 
    from the position of the current character $a_i$ there is a string w that can be derived from $A_k$; if it is so, then the action shifts the input head of 
    $\left\lvert w\right\rvert $ positions. An action can be recursively divided into a series of sub-actions, if to recognize the non-terminal symbol $A_k$ it is 
    necessary to recognize other non-terminals. 

    The initial action is the grammar axiom: the pushdown recognizer must check if the source string can be derived from the axiom. Initially the stack contains
    only the symbol $Z_0$ and the axiom $S$, and the input head is positioned on the initial character of the input string. At every step the automaton
    chooses (non-deterministically) one applicable grammar rule and executes the corresponding move. The input string is recognized accepted when,
    and only when, it is completely scanned and the stack is empty. 
    \begin{figure}[H]
        \centering
        \includegraphics[width=1\linewidth]{images/gramPDA.png}
        \caption{Correspondence between a grammar and a PDA}
    \end{figure}

    The family of free languages generated by free grammars coincides with the family of the languages recognized by one-state pushdown automata. 

    Unfortunately in general the resulting pushdown automaton is non-deterministic, as it explores all the moves applicable at any point and has an exponential time
    complexity with respect to the length of the source string. There are more efficient algorithms. 

    \subsection{Varieties of pushdown automata}
    The acceptance modes can be: 
    \begin{enumerate}
        \item By final state: accepts when enters a final state independently of the stack contents. 
        \item By empty stack: accepts when the stack gets empty independently of the current state. 
        \item Combined: by final state and empty stack.
    \end{enumerate}
    For the family of (non-deterministic) pushdown automata with states, the three acceptance modes listed above are equivalent. 

    A generic pushdown automaton may execute an unlimited number of moves without reading any input character. This happens if, and only if, it enters a loop made only
    of spontaneous moves. Such a behaviour prevents it of completely reading the input string, or causes it to execute an unlimited number of moves before deciding whether
    to accept or reject the string. Both behaviours are undesirable in the practice. It is always possible to build an equivalent automaton with no spontaneous loops. 

    A pushdown automaton operates in on-line mode if it decides whether to accept or reject the string as soon as it reads the last character of the input string, and
    then it does not execute any other move. Clearly from a practical perspective the on-line mode is a desirable behavior. It is always possible to build an equivalent 
    automaton that works in on-line mode. 

    \subsection{One family for context-free languages and PDA}
    The family CF of context-free languages coincides with that of the languages recognized by unrestricted pushdown automata. 

    And more specifically the family CF of (context-) free languages coincides with that of the languages recognized by the one-state non-deterministic pushdown automata.

    \subsection{Intersection of regular and free languages}
    It is easy to justify that the intersection of a free and a regular language is free as well. Given a grammar $G$ and a finite state automaton $A$, the pushdown 
    automaton $M$ that recognizes the intersection $L(G) \cap L(A)$ can be obtained as follows: 
    \begin{enumerate}
        \item Construct the one-state pushdown automaton $N$ that recognizes $L (G)$ by empty stack. 
        \item Construct the pushdown automaton $M$ (with states), the state-transition graph of which is. The Cartesian product of those of $N$ and $A$, by the Cartesian 
            product construction so that the actions of $M$ on the stack are the same as those of $N$. 
    \end{enumerate}
    The obtained pushdown automaton $M$: 
    \begin{enumerate}
        \item As its states, has pairs of states of the component machines $N$ and $A$. 
        \item Accepts by final state and empty stack (combined acceptance mode). 
        \item The states that contain a final state of $A$ are themselves final. 
        \item Is deterministic, if both component machines $N$ and $A$ are so. 
        \item Accepts by final state all and only the strings that belong to the intersection language.
    \end{enumerate}
    \begin{example}
        The intersection of the automaton is: 
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.9\linewidth]{images/PDAint.png}
        \end{figure}
    \end{example}

    \section{Deterministic PDA and languages}
    Nondeterminism is absent if the transition function $\delta$ is one-valued and if $\delta (q, a, A)$ is defined then $\delta(q, \varepsilon, A)$ is undefined and 
    if $\delta(q, \varepsilon, A)$ is defined then $\delta (q, a, A)$ is undefined for every $a \in \Sigma$. If the transition function does not exhibit any form of 
    non-determinism, then the automaton is deterministic and the recognized language is deterministic, too. 

    The family DET of the deterministic free languages is strictly contained in the family CF of all the free languages. 

    If we denote by $L$, $D$ and $R$ a language that belongs to the family CF, DET, and REG we have the following closure properties: 
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.8\linewidth]{images/PDAclosure.png}
    \end{figure}

    \newpage

    \chapter{Syntax analysis}
    \section{Top-down and bottom-up constructions}
    Consider a grammar $G$. If a source string is in the language $L(G)$, a syntax analyzer or parser scans the string and computes a derivation or syntax tree; 
    otherwise it stops and prints the configuration where the error was detected (diagnosis); afterwards it may resume parsing and skip the substrings contaminated 
    by the error (error recovering), in order to offer as much diagnostic help as possible with a single scan of the source string. If the source string is ambiguous, 
    the result of the analysis is a set of trees, also called tree forest. 

    We know the same syntax tree corresponds to many derivations. Depending on the derivation being leftmost or rightmost and on the order it is constructed, we obtain 
    two important parser classes: 
    \begin{itemize}
        \item Top-down analysis: constructs the leftmost derivation by starting from the axiom.
        \item Bottom-up analysis constructs the rightmost derivation but in the reversed order.
    \end{itemize}

    \section{Grammar as network of finite automata}
        Let $\Sigma$ and $V=\{S,A,B,\dots\}$ be, respectively, the terminal alphabet and non-terminal alphabet, and $S$ be the axiom of an extended context-free grammar $G$.

        For each non-terminal $A$ there is exactly one (extended) grammar rule $A\rightarrow\alpha$ and the right part $\alpha$ of the rule is a regular expression over the 
        alphabet $\Sigma \cup V$.
        
        Let the grammar rules be denoted by $S\rightarrow\sigma,A\rightarrow\alpha,B\rightarrow\beta,\dots$. The symbols $R_S,R_A,R_B,\dots$ denote the regular languages 
        over the alphabet $\Sigma \cup V$, definedby the regular expression $\sigma,\alpha,\beta,\dots$, respectively. 
        
        The symbols $M_S,M_A,M_B,\dots$ are the names of the (finite deterministic) machines accepting the corresponding regular languages $R_S,R_A,\dots$ The set of all 
        such machines is denoted by symbol $\mathcal{M}$.
        
        To prevent confusion, the names of the states of any two machines are made disjoint, say, by appending the machine name as a subscript. The state set of a machine
        $M_A$ is denoted $Q_A={0_A,\dots,q_A,\dots}$, its only initial state is $0_A$ and its set of final states is $F_A\subseteq Q_A$. The state set $Q$ of a net $\mathcal{M}$
        is the union of all states:
        \[Q=\bigcup_{M_a \in \mathcal{M}}{Q_A}\]
        The transition function of all machines will be denoted by the same name $\delta$ as for the individual machines, at no risk of confusion as the machine state sets 
        are all disjoint.
        
        For a state $q_A$, the symbol $R(M_A,q_A)$ or for brevity $R(q_A)$, denotes the regular language over the alphabet $\Sigma \cup V$, accepted by the machine $M_A$ 
        starting from state $q_A$. For the initial state, we have $R(0_A)\equiv R_A$.
        
        It is convenient to stipulate that for every machine $M_A$, there is no arc as with $c \in \Sigma \cup V$, which enters the initial state $0_A$. Such a normalization 
        ensures that the initial state is not visited twice within a computation that does not leave machine $M_A$.

        We need to consider also the terminal language defined by a generic machine $M_A$, when starting from a state possibly other than the initial one. For any state
        $q_A$,not necessarily initial, we write as: 
        \[L(M_A,q_A)=L(q_A)=y \in \Sigma^{*}|\eta\in R(q_A) \land \eta^{*} \implies y\]
        The formula above contains a string $\eta$ over terminals and non-terminals, accepted by machine $M_A$ when starting in the state $q_A$. The derivations 
        originating from $\eta$ produce all the terminal strings of language $L(q_A)$. In particular, from previous stipulations it follows that: 
        \[L(M_A,0_A)=L(0_A)\equiv L_A(G)\]
        and for the axiom it is:
        \[L(M_S,0_S)=L(0_S)=L(M)\equiv L(G)\]

    \newpage 

    \chapter{Bottom-up deterministic analysis}
    To systematically construct a bottom-up syntax analyzer we have: 
    \begin{enumerate}
        \item Construction of the pilot graph: the pilot drives the PDA. In each macro-state the pilot incorporates all the information about any possible phrase form that
            reaches the $m$-state (with lookahead). 
        \item The $m$ states are used to build a few analysis threads in the stack, which correspond to possible derivations: computations of the machine network, or paths 
            with $\varepsilon$-arcs at each machine change, labeled with the scanned string. 
        \item Verification of determinism conditions on the pilot graph: shift-reduce conflicts, reduce-reduce conflicts, and convergence conflicts. 
        \item If the determinism test is passed, the PDA can analyze the string deterministically.
        \item The PDA uses the information stored in the pilot graph and in the slack. 
    \end{enumerate}
    \begin{definition}
        The \emph{set of initials} is the set of chars found starting from state $q_A$ of machine $M_A$ of the net $M$.
        
        An \emph{item} is: 
        \[\left\langle q_B,a\right\rangle \textnormal{ in } Q \times (\Sigma \cup \{\dashv\})\]

        The function \emph{closure} computes a kind of closure of a set $C$ of items with look-ahead. 

        The \emph{shift operation} is defined as: 
        \[
        \begin{cases}
            \theta(\left\langle p_A,\rho\right\rangle,X)=\left\langle q_A,\rho\right\rangle \textnormal{ if the arc } p_a \rightarrow^X q_a \textnormal{ exists} \\
            \textnormal{the empty set otherwise}
        \end{cases}    
        \]
    \end{definition}

    The pilot is a DFA, named $\mathcal{P}$, defined by the following entities:
    \begin{itemize}
        \item The set $R$ of $m$-states. 
        \item The pilot alphabet is the union $\Sigma\cup V$ of the terminal and non-terminal alphabets, to be also named the grammar symbols.
        \item The initial $m$-state, $I_0$, is the set $I_0=closure(\left\langle 0_S,\dashv \right\rangle )$. 
        \item The $m$-state set $R={I_0,I_1,\dots}$ and the state-transition function $\theta:R \times (\Sigma \cup V) \rightarrow R$ are computed starting from $I_0$. 
    \end{itemize}

    


\newpage 

\chapter{Flex, Bison and ACSE}
    \section{Regular expressions}
    The basic character set of regular expression is: 
    \begin{table}[H]
        \centering
        \begin{tabular}{cc}
        \hline
        \textbf{Syntax} & \textbf{Matches}                               \\ \hline
        $x$             & The x character                                \\
        $.$             & Any character except newline                   \\
        $[xyz]$         & $x$ or $y$ or $z$                              \\
        $[a-z]$         & Any character between $a$ and $z$              \\
        $[^a-z]$        & Any character except those between $a$ and $z$ \\ \hline
        \end{tabular}
    \end{table}
    The composition rules are the following: 
    \begin{table}[H]
        \centering
        \begin{tabular}{cc}
        \hline
        \textbf{Syntax} & \textbf{Matches}                                  \\ \hline
        R               & The R regular expression                          \\
        RS              & Concatenation of R and S                          \\
        R$|$S             & Either R or S                                     \\
        R*              & Zero or more occurrences of R                     \\
        R+              & One or more occurrences of R                      \\
        R?              & Zero or one occurrence of R                       \\
        R\{$m,n$\}      & A number of R occurrences ranging from $n$ to $m$ \\
        R\{$n,$\}       & $n$ or more occurrences of R                      \\
        R\{$n$\}        & Exactly $n$ occurrences of R                      \\ \hline
        \end{tabular}
    \end{table}
    Other utilities for regular expressions are: 
    \begin{table}[H]
        \centering
        \begin{tabular}{cc}
        \hline
        \textbf{Syntax}                                         & \textbf{Matches}                                                                   \\ \hline
        (R)                                                     & Override precedence / capturing group                                              \\
        \textasciicircum{}R                                     & R at beginning of a line                                                           \\
        R\$                                                     & R at the end of a line                                                             \\
        \textbackslash{}t                                       & Tab character (just like in C)                                                     \\
        \textbackslash{}n                                       & Newline (just like in C)                                                           \\
        \textbackslash{}w                                       & A word (same as {[}a-zA-Z0-9\_{]})                                                 \\
        \textbackslash{}d                                       & A digit (same as {[}0-9{]})                                                        \\
        \textbackslash{}s                                       & Whitespace (same as {[} \textbackslash{}t\textbackslash{}r\textbackslash{}n{]})    \\
        \textbackslash{}W, \textbackslash{}D, \textbackslash{}S & Complement of \textbackslash{}w, \textbackslash{}d, \textbackslash{}s respectively \\ \hline
        \end{tabular}
    \end{table}

    \section{Flex}
    A lexical analysis must recognize tokens in a stream of characters and possibly decorate tokens with additional info. Flex is a scanner generators based on regular 
    expression description. A scanner is just a big finite state automaton. In a compiler, instead, the scanner prepares the input for the parser:
    \begin{itemize}
        \item Detects the tokens of the language. 
        \item Cleans the input. 
        \item Adds information to the tokens. 
    \end{itemize}
    The input of the lexical analyzer generator called flex is a specification file of the scanner, while the output is a C source code file that implements the scanner. 
    A flex file is structured in three sections separated by $\%\%$:
    \begin{itemize}
        \item Definitions: declare useful regular expressions. The definition associates a name to a set of characters using regular expressions, and are usually employed 
            to define simple concepts. They are recalled by putting their name in curly braces
        \item Rules: bind regular expressions combinations to actions. A rule represents a full token to be recognized, and it is defined with a regular expression. They 
            define a semantic action to be made at each match. The semantic actions are executed every time the rule is matched, and can access matched textual data. 
            Simple applications put the business logic directly inside semantic actions. More complex applications that also use a separate parser instead assign a value 
            to the recognized token, and return the token type. 
        \item User code: C code (generally helper functions). This code is copied to the generated scanner as is. It usually contains the main function and any other 
            routine called by actions. 
    \end{itemize}
    The scanner generated by flex is called "lex.yy.c". The yylex() function parses the file yyin until a semantic action returns or the file ends (return value 0). 

    Flex requires you to implement a single function "int yywrap(void)" that is called when the file ends. It gives the opportunity to open another file and continue 
    scanning from there. It must return 0 if the parsing should continue or 1 if the parsing should stop. If you dont want this, you must put the following line
    in the scanner source: "$\%$option noyywrap"

    Some last important rules to remember:
    \begin{itemize}
        \item Longest matching rule: if more than one matching string is found, the rule that generates the longest one is selected. 
        \item First rule: if more than one string with the same length is matched, the rule listed first will be triggered. 
        \item Default action: if no rules are found, the next character in input is considered matched implicitly and printed to the output stream as is.
    \end{itemize}

    The generated parser implements a non-deterministic finite state automaton that tries to match all possible tokens at the same time, and as soon as one is recognized:
    \begin{enumerate}
        \item The semantic action is executed. 
        \item The stream skips past the end of the token. 
        \item The automaton reboots
    \end{enumerate}
    Actually, the NFA is translated into a deterministic automaton using a modified version of the Berry-Sethi algorithm. 

    \subsection{Multiple scanners}
    Sometimes is useful to have more than one scanner together. In order to support multiple scanners: rules should be marked with the name of the associated scanner (start 
    condition), and we need to have special actions to switch between scanners. A start condition S: is used to mark rules with as a prefix 
    $\left\langle S \right\rangle$ RULE, and it marks rules as active when the scanner is running the $S$ scanner. Moreover:
    \begin{itemize}
        \item The * start condition matches every start condition. 
        \item The initial start condition is INITIAL. 
        \item Start conditions are stored as integers.
        \item The current start condition is stored in the YY$\_$START variable.
    \end{itemize}
    Start conditions can be: 
    \begin{itemize}
        \item Exclusive: declared with $\%x \: S$; disables unmarked rules when the scanner is in the $S$ start condition. 
        \item Inclusive: declared with $\%s \: S$; unmarked rules active when scanner is in the S start condition. 
    \end{itemize}
    The INITIAL condition is inclusive. Other special actions are: 
    \begin{itemize}
        \item BEGIN($S$): place scanner in start condition $S$. 
        \item ECHO: copies yytext to output.
    \end{itemize}

    \section{Bison}


    \section{ACSE}


\end{document}