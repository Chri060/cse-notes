\section{Parallel architectures}

Increasing the performance and clock frequency of a single core has become increasingly difficult due to several factors. 
Deep pipeline designs, which have been a common strategy, face significant issues:
\begin{itemize}
    \item \textit{Heat dissipation}: managing the heat generated by high-frequency operations is problematic.
    \item \textit{Speed of light limitations}: The physical limits of light speed affect transmission speed in wires.
    \item \textit{Design complexity}: designing and verifying deep pipelines is challenging and requires large design teams.
    \item \textit{Multithreaded applications}: many new applications are inherently multithreaded, demanding better support for parallel execution.
\end{itemize}

\paragraph*{Beyond Instruction-Level Parallelism}
ILP architectures, such as superscalar and VLIW, are designed to exploit fine-grained, instruction-level parallelism. 
However, they are not well-suited for supporting large-scale parallel systems. 
Multiple-issue CPUs, which attempt to issue multiple instructions per cycle, have become extremely complex, and the benefits of extracting additional parallelism are diminishing. 
Consequently, extracting parallelism at higher levels has become more attractive.

\subsection{Process and Thread-Level Parallel Architectures}
To achieve higher performance, the next step involves process- and thread-level parallel architectures. 
This approach focuses on connecting multiple microprocessors in a complex system to handle large-scale parallel tasks efficiently.
\begin{definition}[\textit{Parallel architecture}]
    A parallel computer is a collection of processing elements that cooperate and communicate to solve large problems quickly.
\end{definition}
The goal is to replicate processors to enhance performance rather than merely designing a faster single processor. 
Parallel architecture extends traditional computer architecture by incorporating a communication architecture that includes:
\begin{itemize}
    \item \textit{Abstractions}: defining the hardware/software interface.
    \item \textit{Efficient structures}: developing various structures to realize these abstractions efficiently.
\end{itemize}
By leveraging parallel architectures, systems can better support multithreaded applications and large-scale computational tasks, addressing the limitations of ILP and deep pipeline designs.

Many of the early multiprocessors were based on the SIMD (Single Instruction, Multiple Data) model, which garnered significant attention in the 1980s. 
Today, SIMD is applied only in specific instances, such as vector processors and multimedia instructions. 
Conversely, MIMD (Multiple Instruction, Multiple Data) has become the architecture of choice for general-purpose multiprocessors.

\subsection{Vector processing}
Vector processors are designed to perform high-level operations on linear arrays of numbers, known as vectors.
A typical vector processor consists of a pipelined scalar unit (which may be out-of-order or VLIW) and a vector unit. 
There are two main types of vector processors: memory-memory vector processors, where all vector operations are memory to memory, and vector-register processors, where all vector operations occur between vector registers, except for load and store operations. 
This latter type is akin to load-store architectures.

Vector processors are particularly useful in multimedia processing, including tasks such as compression, graphics, audio synthesis, and image processing. 
They are also employed in standard benchmark kernels like Matrix Multiply, FFT, Convolution, and Sort, as well as in both lossy (e.g., JPEG, MPEG video and audio) and lossless (e.g., Zero removal, RLE, Differencing, LZW) compression techniques. Other applications include cryptography (e.g., RSA, DES/IDEA, SHA/MD5), speech and handwriting recognition, operating systems and networking (e.g., memcpy, memset, parity, checksum), databases (e.g., hash/join, data mining, image/video serving), and language runtime support (e.g., stdlib, garbage collection), even extending to benchmarks like SPECint95.

MIMD architectures have gained popularity due to their flexibility. 
They can function as single-user machines for high performance on specific applications, as multiprogrammed multiprocessors running many tasks simultaneously, or as a combination of these functions. 
These systems can be constructed using standard CPUs, which is the case for nearly all modern multiprocessors. 
Each processor in an MIMD system fetches its own instructions and operates on its own data, often using off-the-shelf microprocessors. 
This scalability allows for a variable number of processor nodes and provides cost/performance advantages. 
MIMD systems can focus on single-user high-performance tasks, support multi-programmed environments, or combine these functionalities, although fault tolerance remains a concern.

To effectively utilize an MIMD system with $n$ processors, there must be at least n threads or processes to execute. 
These independent threads are typically identified by the programmer or created by the compiler, encapsulating parallelism within the threads, a concept known as thread-level parallelism. 
A thread can range from a large, independent process to parallel iterations of a loop, with parallelism identified by the software, unlike in superscalar CPUs where it is hardware-determined.

\paragraph*{MIMD machines}
Existing MIMD machines fall into two classes based on the number of processors involved, which dictates their memory organization and interconnection strategy.

Centralized shared-memory architectures can support at most a few dozen processor chips (less than 100 cores). 
These systems feature large caches and multiple memory banks, and are often referred to as symmetric multiprocessors (SMP) with a Uniform Memory Access (UMA) architecture.

Distributed memory architectures are designed to support a larger number of processors. 
These systems require a high-bandwidth interconnect but face the disadvantage of complex data communication among processors.