\section{Principle of locality}

The principle of locality asserts that programs tend to access only a fraction of the total address space at any given moment. 
This principle is further elaborated through two key properties:
\begin{enumerate}
    \item \textit{Temporal locality}: this property suggests that if a memory location is accessed, it is probable that it will be accessed again in the near future.
    \item \textit{Spatial locality}: this property indicates that if a memory location is accessed, it is likely that nearby locations will also be accessed in the near future.
\end{enumerate}
Caches leverage both forms of predictability. 
They exploit temporal locality by retaining the contents of recently accessed memory locations, anticipating their future use. 
Additionally, they exploit spatial locality by pre-fetching blocks of data surrounding recently accessed locations, capitalizing on the likelihood of adjacent memory access.

When examining a processor address, the cache tags are searched to locate a match. 
Subsequently, one of the following actions occurs:
\begin{itemize}
    \item If a match is found in the cache (HIT), the data copy is retrieved from the cache and returned.
    \item If the address is not found in the cache (MISS), a block of data is read from the main memory.
        There is a wait period, after which the data is returned to the processor, and the cache is updated accordingly.
\end{itemize}
Based on these operations, several metrics can be defined. 
\begin{definition}[\textit{Hit rate}]
    The hit rate is the fraction of accesses that are found in the cache.
\end{definition}
\begin{definition}[\textit{Miss rate}]
    The miss rate is the complement of the hit rate, indicating the fraction of accesses that result in cache misses.
\end{definition}
\begin{definition}[\textit{Hit time}]
    The hit time encompasses the time required for RAM access along with the time needed to determine whether the access resulted in a HIT or MISS.
\end{definition}
\begin{definition}[\textit{Miss time}]
    The miss time comprises the time necessary to replace a block in the cache and the time taken to deliver the block to the processor.
\end{definition}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{images/cache.png}
    \caption{Cache and processor interaction}
\end{figure}

\subsection{Block placement}
Depending on the chosen memory type, the block numbered 12 can be positioned as follows:
\begin{itemize}
    \item \textit{Fully associative}: it can be placed anywhere within the memory.
    \item \textit{Two-way set associative}: it can be placed anywhere within set zero, which corresponds to $12 \mod 4$.
    \item \textit{Direct mapped}: it can only be placed into block four, determined by $12 \mod 8$. 
\end{itemize}

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=0.45\linewidth]{images/fa.png} 
        \caption{Fully associative}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=0.6\linewidth]{images/twsa.png}
        \caption{Two-way set associative}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=0.45\linewidth]{images/dm.png}
        \caption{Direct mapped}
    \end{subfigure}
    \caption{Possible placement of blocks}
\end{figure}
In this diagram, the placement of block 12 is illustrated based on the different memory configurations mentioned above.

\subsection{Block identification}
A cache miss can occur for several reasons:
\begin{enumerate}
    \item \textit{Compulsory miss} (cold start or process migration): this happens during the first access to a block, such as during a cold start or when a process migrates. 
        It's essentially an unavoidable aspect of system operation, and there's little that can be done to mitigate it.
    \item \textit{Capacity miss}: This occurs when the cache is unable to accommodate all the blocks accessed by the program. 
        Increasing the cache size is a potential solution to reduce the frequency of these misses.
    \item \textit{Conflict miss} (collision): multiple memory locations are mapped to the same cache location, resulting in conflicts.
        This can be addressed by either increasing the cache size or increasing associativity, which allows more flexibility in mapping memory locations to cache locations.
    \item \textit{Coherence Miss} (invalidation): This type of miss occurs when another process, such as I/O operations, updates memory, leading to inconsistencies in cached data. 
        Ensuring cache coherence mechanisms are in place can help mitigate this issue.
\end{enumerate}

To locate a block, the cache index is used to select the set to search within, while the tag identifies the actual copy. 
If no matching candidates are found, a cache miss is declared.

The structure of a memory address typically includes a field for selecting data within a block. 
However, some caching applications may not utilize this field.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{images/address.png}
    \caption{Memory address general structure}
\end{figure}
Increasing associativity reduces the index size and expands the tag. 
Fully associative caches, for example, do not have an index field and can directly access any block in the cache. 

The fully associative cache, characterized by requiring only a tag and block offset, is depicted as follows.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{images/fas.png} 
    \caption{Fully associative cache}
\end{figure}
The two-way set associative cache, which necessitates a tag, index, and block offset, is represented as follows.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{images/twsas.png}
    \caption{Two-way set associative cache}
\end{figure}
The direct mapped cache, which also requires a tag, index, and block offset, is illustrated as follows.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{images/dms.png}
    \caption{Direct mapped cache}
\end{figure}
Each type of cache has its own specific structure, with varying degrees of associativity and corresponding requirements for indexing and tagging.

\subsection{Block replacement}
In the context of cache misses, block replacement is straightforward for direct-mapped caches. 
However, for set-associative or fully associative caches, the choice of replacement policy has a significant impact because replacements only occur upon misses.
Here are some commonly used replacement policies:
\begin{itemize}
    \item \textit{Random}: blocks are replaced randomly, without any specific order or pattern.
    \item \textit{Least recently used} (LRU): this policy replaces the block that has been accessed least recently. 
        Although effective, implementing LRU requires tracking the access history of each block, making it feasible only for caches with a few sets due to the computational overhead.
    \item \textit{First in first out} (FIFO): blocks are replaced based on the order they were brought into the cache. 
        FIFO is commonly used in highly associative caches where keeping track of access history for LRU may not be practical.
\end{itemize}
Each of these replacement policies has its advantages and trade-offs, and the choice depends on factors such as cache size, associativity, and the desired balance between complexity and performance.

\subsection{Write strategy}
In the event of a cache hit, we have two options for handling writes:
\begin{itemize}
    \item \textit{Write through}: this strategy involves writing the data both to the cache and to main memory simultaneously. 
        While this approach typically results in higher traffic, it simplifies cache coherence management.
    \item \textit{Write back}: with this approach, the data is written only to the cache. 
        The corresponding entry in main memory is updated only when the cache block is evicted. 
        A dirty bit per block helps reduce traffic by indicating whether the block in the cache has been modified.
\end{itemize}
In the case of a cache miss, we also have two alternatives:
\begin{itemize}
    \item \textit{No write allocate}: with this method, data is written directly to main memory without being fetched into the cache.
    \item \textit{Write allocate} (also known as fetch on write): In this scenario, the data is fetched into the cache upon a write miss.
\end{itemize}
The most common combinations of these strategies are:
\begin{itemize}
    \item \textit{Write through with no write allocate}: data is written to both the cache and main memory simultaneously, and in the event of a write miss, no data is brought into the cache.
    \item \textit{Write back with write allocate}: data is written only to the cache, and in the case of a write miss, the data is fetched into the cache before being modified.
\end{itemize}
We can also use a write buffer for write-through caches to avoid CPU stalls. 