\section{Cache performance}

\begin{definition}[\textit{Memory stall cycles}]
    Memory stall cycles is the number of cycles in which the CPU is not working (stalled) waiting for a memory access. 
\end{definition}
We assume that the cycle time includes the time necessary to manage a cache hit and that during a cache miss the CPU is stalled. 
We can now compute: 
\[\text{CPU execution time}=\left(\text{CPU clock cycles}+\text{Memory stall cycles} \right) \cdot \text{Clock cycle time}\]
\[\text{Memory stall cycles}=\text{Number of misses} \cdot \text{Miss penalty}\]
We can simplify by averaging reads and writes: 
\[\text{Memory stall cycles} = \text{IC} \cdot (\dfrac{\text{Memory accesses}}{\text{Instruction}})\cdot \text{Miss rate} \cdot \text{Miss penalty}\]
This value is independent of the hardware implementation, but dependent on the architecture. 

On the other hand the average access time is computed as: 
\begin{align*}
    T_A &= H_{rate} \cdot H_{time}+ M_{rate} \cdot M_{time} \\
        &= H_{rate} \cdot H_{time}+ M_{rate} \cdot \left( H_{time}+M_{penalty} \right) \\
        &= H_{time} \cdot \left(H_{rate} + M_{rate}\right) + M_{rate} \cdot M_{penalty} \\
        &= H_{time} + M_{rate} \cdot M_{penalty}
\end{align*}

\subsection{Basic cache optimizations}
The access time for a cache can be optimized by: 
\begin{itemize}
    \item Reducing $M_{rate}$: 
        \begin{itemize}
            \item Larger Block size (compulsory misses): 
            \item Larger Cache size (capacity misses): 
            \item Higher Associativity (conflict misses): 
        \end{itemize}
    \item Reducing $M_{penalty}$: 
        \begin{itemize}
            \item Multilevel Caches: 
        \end{itemize}
    \item Reducing $H_{time}$: 
        \begin{itemize}
            \item Giving Reads Priority over Writes: 
        \end{itemize}
\end{itemize}

\subsection{Cache design}
Within the realm of cache design, various factors interplay: cache size, block size, associativity, replacement policy, and the choice between write-through and write-back mechanisms. 
Determining the best option involves finding a balance influenced by access patterns (workload and usage) and technological expenses. 
Often, simplicity emerges as the preferred solution.

The performance metrics used to evaluate the performance are: 
\begin{itemize}
    \item Latency is concern of cache.
    \item Bandwidth is concern of multiprocessors and I/O.
    \item Access time: time between read request and when desired word arrives. 
    \item Cycle time: minimum time between unrelated requests to memory.
\end{itemize}
DRAM used for main memory, SRAM used for cache. 

\paragraph*{SRAM}
SRAM memory requires low power to retain bit and requires six transistors for each bit. 

\paragraph*{DRAM}
The DRAM must be re-written after being read and also periodically refreshed (each row simultaneously every eight milliseconds). 
However, this type of memory requires only one transistor per bit. 
The address lines are multiplexed:
\begin{itemize}
    \item Upper half of address: row access strobe (RAS). 
    \item Lower half of address: column access strobe (CAS). 
\end{itemize}

\paragraph*{Flash memory}
Flash memory belongs to the category of EEPROM, necessitating block erasure prior to overwrite. 
It retains data without power, constituting non-volatile storage.
With a finite number of write cycles, it falls in price between SDRAM and disk storage. 
Although slower than SRAM, it outpaces traditional disk speeds.

\paragraph*{Optimizations}
The Amdahl law states that the memory capacity should grow linearly with processor speed. 
Unfortunately, memory capacity and speed has not kept pace with processors. 
To overcome this issue some optimizations are possible: 
\begin{itemize}
    \item Multiple accesses to same row. 
    \item Synchronous DRAM: added clock to DRAM interface, and burst mode with critical word first.
    \item Wider interfaces.
    \item Double data rate (DDR).
    \item Multiple banks on each DRAM device.
\end{itemize}