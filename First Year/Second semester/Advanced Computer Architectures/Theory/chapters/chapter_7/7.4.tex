\section{Parallelism}

Two essential properties ensure program correctness and are typically preserved by maintaining both data and control dependencies:
\begin{itemize}
    \item \textit{Exception behavior}: preserving exception behavior ensures that any alterations in the execution order of instructions do not affect how exceptions are triggered within the program.
    \item \textit{Data flow}: this pertains to the actual movement of data values among instructions, ensuring that correct results are produced and consumed.
\end{itemize}
There are two strategies to support instruction-level parallelism:
\begin{itemize}
    \item \textit{Dynamic scheduling}: this approach relies on hardware to identify and exploit parallelism within the program.
    \item \textit{Static scheduling}: this method depends on software to identify potential parallelism beforehand.
\end{itemize}
In the desktop and server markets, hardware-intensive approaches tend to dominate.

\subsection{Dynamic scheduling}
Hardware reorders instruction execution to mitigate pipeline stalls while still upholding data flow and exception behavior.

\paragraph*{Advantages}
The main advantages of this approach include:
\begin{itemize}
    \item Handling cases where dependencies are unknown at compile time.
    \item Simplifying compiler complexity.
    \item Facilitating efficient execution of compiled code on different pipeline architectures.
\end{itemize}

\paragraph*{Disadvantages}
However, these advantages come with certain costs:
\begin{itemize}
    \item Significant increase in hardware complexity.
    \item Higher power consumption.
    \item Potential for imprecise exceptions.
\end{itemize}
In essence, instructions are fetched and issued in program order (in-order issue). 
Execution initiates as soon as operands become available, potentially allowing out-of-order execution. 
Note that out-of-order execution is feasible even in pipelined scalar architectures.

Out-of-order execution introduces the possibility of write-after-read and write-after-write data hazards. 
Additionally, out-of-order execution implies out-of-order completion unless a re-order buffer is present to ensure in-order completion.

\subsection{Static scheduling}
Compilers employ sophisticated algorithms for code scheduling to harness instruction-level parallelism.
However, the instruction-level parallelism available within a basic block—a straight-line code sequence with minimal branching—is often limited. 

To achieve significant performance improvements, instruction-level parallelism must be exploited across multiple basic blocks, transcending branches.

Static scheduling involves the compiler's detection and resolution of dependencies by reordering code to avoid dependencies. 
The compiler's output typically comprises dependency-free code, which is particularly suited for architectures like Very Long Instruction Word (VLIW) processors.

However, there are limits to instruction-level parallelism exploitation:
\begin{itemize}
    \item Unpredictable branches.
    \item Variable memory latency, such as unpredictable cache misses.
    \item Code size explosion.
    \item Increased compiler complexity.
\end{itemize}