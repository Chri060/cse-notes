\section{MIMD architectures}

MIMD architectures have gained popularity due to their versatility.
They can serve as single-user machines optimized for high performance on specific applications, act as multiprogrammed multiprocessors capable of executing many tasks concurrently, or combine these functionalities.
Modern MIMD systems typically utilize standard CPUs, where each processor fetches its own instructions and operates on its own data, often employing off-the-shelf microprocessors. 
This scalability allows for a variable number of processor nodes, offering advantages in terms of both cost and performance. 
While MIMD systems excel in single-user high-performance tasks and multi-programmed environments, ensuring fault tolerance remains a critical concern.

To effectively utilize an MIMD system with $n$ processors, there must be at least $n$ threads or processes available for execution. 
These threads are either identified by the programmer or generated by the compiler, encapsulating parallelism within threadsâ€”a concept known as TLP. 
Threads can range from large, independent processes to parallel iterations of loops, with the degree of parallelism defined by software, unlike in superscalar CPUs where it is hardware-determined.

\subsection{MIMD taxonomy}
Existing MIMD machines are categorized into two primary classes based on the number of processors involved, influencing their memory organization and interconnection strategies:
\begin{itemize}
    \item \textit{Centralized shared-memory architectures}: these systems support up to several dozen processor chips. 
        They feature large caches, multiple memory banks, and are commonly known as Symmetric Multiprocessors (SMP) with Uniform Memory Access (UMA) architecture.
    \item \textit{Distributed memory architectures}: designed to support a larger number of processors, these systems require high-bandwidth interconnects. 
        They facilitate complex data communication among processors but offer scalability beyond what centralized architectures can provide.
\end{itemize}
In MIMD architectures, memory models define how processors access and communicate with memory:
\begin{itemize}
    \item \textit{Shared address space}: processors communicate through shared variables in memory. 
        Communication management is implicit, handled through load and store operations. 
        This model is the oldest and most prevalent in parallel computing.
        It's important to note that shared memory does not necessarily imply a single centralized memory structure.
    \item \textit{Private address space}: processors communicate by sending and receiving messages. 
        Communication is managed explicitly through send and receive operations to access private memory.
        One advantage of this setup is the absence of cache coherency issues among processors, which can be a significant challenge in shared memory systems.
\end{itemize}
The MIMD memory model can be further categorized into: 
\begin{itemize}
    \item \textit{Shared programming model}: a program consists of threads of control, which can be dynamically created during execution in certain languages.
        Each thread possesses a set of private variables and a set of shared variables. 
        Threads communicate implicitly by reading and writing shared variables and synchronize on these shared variables for coordination.
    \item \textit{Private programming model}: a program comprises named processes, typically fixed at startup. 
        Each process operates within its own local address space without shared data. Logically shared data is partitioned across local processes. 
        Processes communicate explicitly via send/receive pairs, with coordination implicit in each communication event. 
        The Message Passing Interface (MPI) is a widely used software implementation of this model.
\end{itemize}
The choice between shared memory and message passing hinges on the specific characteristics of the program. 
Both models possess Turing-complete communication capabilities.

Shared memory provides advantages such as implicit communication via loads and stores, low overhead when cached, and simpler control over data placement within the caching system. 
However, scaling shared memory systems can be challenging, requiring synchronization operations and potentially complicating cache coherence.

On the other hand, message passing, especially in massively parallel processors, requires software to manage data layout and handle remote data access through message requests and replies.
This model incurs higher software overhead due to the involvement of the operating system in message handling. 
While sending messages can be relatively inexpensive, receiving messages can be costly due to the necessity of polling or interrupt handling.

\paragraph*{Bus-based symmetric shared memory}
Bus-based symmetric shared memory architectures are prevalent in the server market and increasingly adopted in desktop systems. 
They are particularly appealing for throughput servers and parallel programs due to their fine-grain resource sharing, uniform access facilitated by loads and stores, automatic data movement capabilities, and coherent cache replication. 
These architectures effectively extend uni-processor mechanisms for data access, offering a cost-effective and potent solution. 
A key feature of these architectures is the extension of the memory hierarchy to seamlessly support multiple processors.

\subsection{Cache coherence}
Shared memory machines are typically categorized into two main types: non-cache coherent and hardware cache coherent systems.
Hardware cache coherent machines are capable of working with any data placement, although performance may vary. 
Optimization efforts often target critical sections of code. 
These systems utilize load and store instructions for data communication, avoiding operating system involvement and resulting in low software overhead. 
Special synchronization primitives are commonly used. 
In large-scale systems, logically distributed shared memory is implemented through physically distributed memory modules.

Shared-memory architectures cache both private data and shared data. 
Caching shared data reduces access latency and required memory bandwidth, while also alleviating contention during simultaneous read operations by multiple processors. 
However, issues arise due to private processor caches potentially containing copies of the same variable, leading to the cache coherence problem.

Coherence ensures that any read operation retrieves the most recent write to a shared location.
While a strict implementation of this principle is challenging, a practical approach requires that all processors eventually see all writes in the correct order, a concept known as serialization. 
This coherence is maintained by following two rules:
\begin{enumerate}
    \item If processor $P$ writes $x$ and processor $P_1$ subsequently reads $x$, $P$'s write must be visible to $P_1$ if the read and write are sufficiently spaced apart without intervening writes to $x$.
    \item Writes to the same location must be serialized, ensuring that all processors observe these writes in the same order.
\end{enumerate}
Memory consistency further complicates this scenario, as it specifies the order of reads and writes across different memory locations. 
In practice:
\begin{itemize}
    \item A write operation is not considered complete until all processors have observed its effect.
    \item The order of writes relative to other memory accesses remains unchanged.
\end{itemize}
In coherent multiprocessors, copies of the same data are typically present in several caches.
Caches support both migration and replication of shared data items:
\begin{itemize}
    \item \textit{Migration}: data items can be moved to a local cache for transparent use, reducing access latency and bandwidth demand on shared memory.
    \item \textit{Replication}: copies of shared data items are made in local caches to reduce latency and contention during simultaneous read operations.
\end{itemize}
To enforce cache coherence, hardware based solutions like cache-coherence protocols are essential. 
These protocols manage the status of shared data blocks across processors. 
There are two primary classes of coherence protocols: snooping protocols and directory-based protocols.