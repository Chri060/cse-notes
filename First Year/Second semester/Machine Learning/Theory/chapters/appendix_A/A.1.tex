\section{Least squares}

Let's reconsider a dataset consisting of $N$ inputs $\textbf{x}_i=(x_{i1}, \dots, x_{iD})$, where each $x_{ij} \in\mathbb{R}$ represents a feature with dimension $D$, along with a target $t_i \in \mathbb{R}$ associated with each input $\textbf{x}_i$. 

Our aim is to predict the target $t_i$ by computing a linear combination of the input $\textbf{x}_i$, which involves generating a parameter vector $\textbf{w}= {(w_1, \dots, w_D)}^T$ that minimizes a certain loss function.
Specifically, if we consider the loss function to be the summation of squared prediction errors, it corresponds to Least Square minimization.

Now, let's define the following loss function:
\[L(\textbf{w})=\dfrac{1}{2}\sum_{i=1}^{N}{\left(t_i+\sum_{j=1}^{D}x_{ij}w_j\right)}^2\]
By defining the matrix $X=\begin{bmatrix} \textbf{x}_1 & \cdots & \textbf{x}_N \end{bmatrix}^T$, we can rewrite the loss function as:
\[L(\textbf{w})=\dfrac{1}{2}\left\lVert \textbf{t}-X\textbf{w}\right\rVert_2^2=\dfrac{1}{2}{\left(\textbf{t}-X\textbf{w}\right)}^T\left(\textbf{t}-X\textbf{w}\right)\]

\paragraph*{Minimizing the Loss}
To minimize the loss, we need to compute its derivatives with respect to each component of $\textbf{w}$:
\[\dfrac{\partial L(\textbf{w})}{\partial\textbf{w}}=\left(\dfrac{\partial L(\textbf{w})}{w_1},\ldots,\dfrac{\partial L(\textbf{w})}{w_D}\right)\]
In this case, we have two different formulations for the derivative:
\begin{enumerate}
    \item Traditional (element-wise) derivative:
        \[{\left(\dfrac{\partial L(\textbf{w})}{\partial\textbf{w}}\right)}_h=\dfrac{\partial L(\textbf{w})}{w_h}=\dfrac{\partial}{\partial w_h}\left[ \dfrac{1}{2}\sum_{i=1}^{N}{\left(t_i-\sum_{j=1}^{D}x_{ij}w_j\right)}^2 \right]\]
    \item Matrix derivative: 
        \[\dfrac{\partial L(\textbf{w})}{\partial\textbf{w}}=\dfrac{\partial}{\partial\textbf{w}}\left[\dfrac{1}{2}{\left(\textbf{t}-X\textbf{w}\right)}^T\left(\textbf{t}-X\textbf{w}\right)\right]\]
\end{enumerate}