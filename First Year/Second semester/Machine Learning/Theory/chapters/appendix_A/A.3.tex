\section{Random variables}

A discrete random variable $X$ is a variable with values in a discrete set, whose value is determined by a stochastic phenomenon.
We define a probability function $\text{P}:E\rightarrow[0,1]$ which indicates the likelihood of events in $E$: 
\[\text{P}(X=i)=\dfrac{\left\lvert i \right\rvert}{\left\lvert E \right\rvert}\]
A properly defined probability function should satisfy the following properties:
\begin{enumerate}
    \item $\forall i \in E$, $0 \leq \text{P}(X=1) \leq 1$. 
    \item $\sum_{i \in E}\text{P}(X=i)=1$. 
\end{enumerate}

\paragraph*{Cumulative function}
Assuming events are ordered, a cumulative function $F:E\rightarrow[0,1]$ defines the probability of multiple events:
\[F(i)=\text{P}(X\leq 1)=\sum_{h=1}^{i}\text{P}(X=h)=\sum_{h \in E, h \leq i} \dfrac{\left\lvert h \right\rvert}{\left\lvert E \right\rvert}\]
A properly defined cumulative function should satisfy the following properties:
\begin{itemize}
    \item $0 \leq F(i) \leq 1$ 
    \item $F(i)=0$ for all $i < \min_{h \in E}h$
    \item $F(i)=1$ for all $i geq \max_{h \in E}h$
\end{itemize}

\paragraph*{Random variables characteristics}
Quantities characterizing a random variable include the expected value (moment of order one):
\[\mathbb{E}\left[X\right]=\sum_{i \in E}i\text{P}(X=i)\]
And the variance (moment of order two):
\[\text{Var}(X)=\sum_{i \in E}{\left(\mathbb{E}\left[X\right]-i\right)}^2\text{P}(X=i)\]
The standard deviation of the variance is defined as:
\[\text{std}(X)=\sqrt{\text{Var}(X)}\]

\subsection{Continuous random variable}
Similarly, if the set $E$ is not discrete, we could define the probability density function as:
\[f(x)=\lim_{\delta x \rightarrow 0}\dfrac{\text{P}(x\leq X\leq x+\delta x)}{\delta x}\]
The properties of continuous random variables are:
\begin{itemize}
    \item $f(x) \geq 0$ for all $ x \in \Omega$
    \item $\int_{x \in \Omega}f(x)\,dx=1$
\end{itemize}

\paragraph*{Cumulative density function}
The cumulative density function is defined as:
\[F(x)=\int_{s \in \Omega, s \leq x}f(s)\,ds\]
The cumulative density function has the following properties:
\begin{itemize}
    \item $0 \leq F(x)\leq 1$ for all $x \in \Omega$
    \item $F\left(\min_{x \in \Omega}x-\varepsilon\right)=0$ for all $\varepsilon>0$
    \item $F\left(\max{x \in \Omega}x\right)=1$
\end{itemize}
A quantile of order $\alpha$ is defined as a point $z_\alpha \in \Omega$ such that: 
\[F(z_\alpha)=1-\alpha\]
or a point of the domain leaving to its left a cumulated probability of $1-\alpha$. 

\paragraph*{Random variables characteristics}
Quantities which characterize a random variable are the expected value (moment of order one):
\[\mu=\mathbb{E}\left[X\right]=\int_{x \in \Omega}xf(x)\,dx\]
And the variance (moment of order two):
\[\sigma^2=\text{Var}(X)=\int_{x \in \Omega}{\left(\mathbb{E}\left[X\right]-x\right)}^2f(x)\,dx\]

\paragraph*{Gaussian distribution}
The Gaussian distribution is expressed as:
\[f(x,\mu,\sigma)=\dfrac{1}{\sigma\sqrt{2\pi}}e^{-\frac{{(x-\mu)}^2}{2\sigma^2}}\]
And the relative cumulative density function:
\[F(x,\mu,\sigma)=\int_{-\infty}^{x}f(t,\mu,\sigma)\,dt\]

\section{Distributions}
Usually, we do not have any information about the distribution of random variables. 
Therefore, we need to estimate their mean and variance.
A consistent estimator for the expected value is:
\[\bar{X}=\dfrac{\sum_{i=1}^{n}x_i}{n}\]
A consistent estimator for the variance is:
\[\bar{s}=\dfrac{\sum_{i=1}^{n}{\left(\bar{X}-x_i\right)}^2}{n-1}\]

\begin{theorem}[Central limit]
    Assuming $\left\{X_1,\dots,X_n\right\}$ as a sequence of independent and identically distributed random variables with $\mathbb{E}\left[X_i\right]=\mu$ and $\text{Var}\left[X_i\right]=\sigma^2<\infty$, then: 
    \[\sqrt{n}\left( \dfrac{\sum_{i=1}^{n}X_i}{n}-\mu\right)\rightarrow\mathcal{N}(0,\sigma^2)\]
    where the convergence holds in distribution.
\end{theorem}