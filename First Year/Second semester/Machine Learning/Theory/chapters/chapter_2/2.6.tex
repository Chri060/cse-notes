\section{Computational learning theory}

Computational learning theory is a field of study that aims to understand the general principles of inductive learning. 
It models the complexity of the hypothesis space, the bound on the training samples, the bound on accuracy, and the probability of successful learning.

A learner ($L$) aims to grasp a concept ($C$) that effectively relates data in the input space ($X$) to a target ($t$). 
Let's suppose $L$ has identified a hypothesis $h^\ast$ that perfectly fits the training data.
We need to find how many training samples from $X$ are required to ensure that $L$ has genuinely acquired the true concept, meaning $h^\ast$ accurately represents $C$. 

Let $Acc(L)$ represent the generalization accuracy of learner $L$, indicating $L$'s performance on samples not included in the training set.
Let $\mathcal{F}$ be the collection of all potential concepts where $y = f(\mathbf{x})$.
For any learner $L$ and any possible training set:
\[\dfrac{1}{\left\lvert \mathcal{F} \right\rvert}\sum_{\mathcal{F}}Acc_G(L)=\dfrac{1}{2}\]
\begin{corollary}
    For any two learners, $L_1$ and $L_2$, if exists $f(\cdot)$ where $Acc_G(L_1)>Acc_G(L_2)$ then exists $f^\prime(\cdot)$ where $Acc_G(L_2)>Acc_G(L_1)$. 
\end{corollary}
This means that in Machine Learning we always operate under some assumptions. 

\subsection{Approximately correct hypothesis}
Let $X$ be the instance space.
Let $H =\{ h:X \rightarrow \{0,1\}\}$ represent the hypothesis space of learner $L$. 
Let $C =\{ c:X \rightarrow \{0,1\}\}$ denote the set of all possible target functions (concepts) we aim to learn.
Let be $\mathcal{D}$ be the training data drawn from a stationary distribution $P(X)$ and labeled (without noise) according to a concept $c$ we intend to learn.
A learner $L$ produces a hypothesis $h \in H$ such that: 
\[h^\ast=\argmin_{h\in H}error_{train}(h)\]

\paragraph*{Error}
We determine the error of a hypothesis as the probability of misclassifying a sample:
\[error_{\mathcal{D}}(h)=\Pr_{x\in\mathcal{D}}\left[h(x)\neq c(x)\right]=\dfrac{1}{\left\lvert \mathcal{D}\right\rvert}\sum_{x\in\mathcal{D}}I(h(x)\neq c(x))\]
This represents the training error. 
However, our interest lies in the true error of $h$:
\[error_{true}(h)=\Pr_{x \sim P(X)}\left[h(x)\neq c(x)\right]\]

Assuming $error_{true}$ as the probability of making a mistake on a sample, we can compute $error_{\mathcal{D}}$, which is the average error probability on $\mathcal{D}$.
Assuming a Bernoulli distribution for the error probability, the 95\% confidence interval is given by:
\[error_{true}(h)=error_{\mathcal{D}}(h)\pm 1.96\sqrt{\dfrac{error_{\mathcal{D}}(h)(1-error_{\mathcal{D}}(h))}{n}}\]
This calculation is inaccurate because $\mathcal{D}$ represents the training data and is not independent of $h$. 
Therefore, we require a stricter bounding of the error under additional assumptions.

\subsection{Version space and bound}
A hypothesis $h$ is deemed consistent with a training dataset $\mathcal{D}$ of the concept $c$ if and only if $h(x) = c(x)$ for each training sample in $\mathcal{D}$: 
\[\text{Consistent}(h,\mathcal{D})\overset{\text{def}}{=}\forall\left\langle x,c(x) \right\rangle\in\mathcal{D},h(x)=c(x)\]
The version space, $VS_{H,\mathcal{D}}$, with respect to the hypothesis space $H$ and the labeled dataset $\mathcal{D}$, is the subset of hypotheses in $H$ consistent with $\mathcal{D}$: 
\[VS_{H,\mathcal{D}}\overset{\text{def}}{=}\left\{h\in H|\text{Consistent}(h,\mathcal{D})\right\}\]
From now on, we consider only consistent learners, which always output a consistent hypothesis, i.e., a hypothesis in $VS_{H,\mathcal{D}}$, assuming it is not empty.

If we aim to bound the $error_{true}$ of a consistent learner, we need to find a bound for all the hypotheses in $VS_{H,\mathcal{D}}$. 
\begin{theorem}
    If the hypothesis space $H$ is finite and $\mathcal{D}$ is a sequence of $N\geq 1$ independent random examples of some target concept $c$, then for any $0\leq\varepsilon\leq 1$, the probability that $VS_{H,\mathcal{D}}$ contains a hypothesis error greater than $\varepsilon$ is less than $\left\lvert H\right\rvert e^{\varepsilon N}$: 
    \[\Pr(\exists h \in H:error_{\mathcal{D}}(h)=0\land error_{true}(h)\geq\varepsilon)\leq \left\lvert H\right\rvert e^{\varepsilon N}\]
\end{theorem}
\begin{proof}
    We have that: 
    \begin{align*}
        \Pr&\left(\left(error_{\mathcal{D}}(h_1)=0\land error_{true}(h_1)\geq\varepsilon\right)\lor\ldots\lor\left(error_{\mathcal{D}}(h_{\left\lvert VS_{ H,\mathcal{D}}\right\rvert})=0\land error_{true}(h_{\left\lvert VS_{ H,\mathcal{D}}\right\rvert})\geq\varepsilon\right)\right) \\
        &\leq\sum_{h\in VS_{H,\mathcal{D}}}\Pr(error_{\mathcal{D}}(h)=0\land error_{true}(h)\geq\varepsilon) \\
        &\leq\sum_{h\in VS_{H,\mathcal{D}}}\Pr(error_{\mathcal{D}}(h)=0| error_{true}(h)\geq\varepsilon) \\
        &\leq\sum_{h\in VS_{H,\mathcal{D}}}(1-\varepsilon)^N \\
        &\leq\left\lvert H\right\rvert (1-\varepsilon)^N \\
        &\leq\left\lvert H\right\rvert e^{-\varepsilon N}
    \end{align*}
\end{proof}

\paragraph*{Bound in practice}
Let's denote $\delta$ as the probability of having $error_{true}>\varepsilon$ for a consistent hypothesis:
\[\left\lvert H\right\rvert e^{-\varepsilon N} \leq \delta\]
We can then bound $N$ after setting $\varepsilon$ and $\delta$:
\[N\geq\dfrac{1}{\varepsilon}\left(\ln\left\lvert H\right\rvert+\ln\left(\dfrac{1}{\delta}\right) \right)\]
Similarly, we can bound $\varepsilon$ after setting $N$ and $\delta$:
\[\varepsilon \geq \dfrac{1}{N}\left(\ln\left\lvert H\right\rvert+\ln\left(\dfrac{1}{\delta}\right) \right)\]

\paragraph*{PAC-learning}
Considering a class $C$ of possible target concepts defined over an instance space $X$ with an encoding length $M$, and a learner $L$ using an hypothesis space $H$ we define: $C$ is PAC-learnable by $L$ using $H$ if for all $c\in C$, for any distribution $\Pr(X)$, $\varepsilon$ (such that $0 < \varepsilon < 1/2$), and $\delta$ (such that $0 < \delta < 1/2$), learner $L$ will with a probability at least ($1 - \delta$) output a hypothesis $h \in H$ such that $error_{true}(h) \leq \varepsilon$, in time that is polynomial in $1/\varepsilon$, $1/\delta$, $M$, and $sixe(c)$.
A sufficient condition to prove PAC-learnability is proving that a learner $L$ requires only a polynomial number of training examples, and processing per example is polynomial.

\subsection{Agnostic learning}
Up to this point, we've operated under the assumption that $c \in H$, or at the very least, that $VS_{H,\mathcal{D}}$ is not empty, and that the learner $L$ will consistently output a hypothesis $h$ such that $error_{\mathcal{D}}(h) = 0$. 
However, in a more general scenario, an agnostic learner might output a hypothesis $h$ with $error_{\mathcal{D}}(h) > 0$.
\begin{theorem}
    If the hypothesis space $H$ is finite and $\mathcal{D}$ is a sequence of $N\geq 1$ independent and identically distributed random variables examples of some target concept $c$, then for any $0 \leq \varepsilon \leq 1$, and for any learned hypothesis $h$, the probability that $error_{true}(h) - error_{\mathcal{D}}(h) > \varepsilon$ is less than $\left\lvert H\right\rvert e^{-2N\varepsilon^2}$:
    \[\Pr(\exists h\in H|error_{true}(k)>error_{\mathcal{D}}(h)+\varepsilon)\leq \left\lvert H\right\rvert e^{-2N\varepsilon^2}\]
\end{theorem}
\begin{proof}
    Utilizing the additive Hoeffding bound: let $\hat{\theta}$ be the empirical mean of $N$ independent and identically distributed Bernoulli random variables with mean $\theta$:
    \[\Pr(\theta>\hat{\theta}+\varepsilon)\leq e^{-2N\varepsilon^2}\]
    Consequently, for any single hypothesis $h$:
    \[\Pr(error_{true}(h)>error_{\mathcal{D}}(h)+\varepsilon)\leq e^{-2N\varepsilon^2}\]
    As we require this to hold true for all hypotheses in $H$:
    \[\Pr(\exists h\in H|error_{true}(h)>error_{\mathcal{D}}(h)+\varepsilon)\leq\left\lvert H\right\rvert e^{-2N\varepsilon^2}\]
\end{proof}

\paragraph*{Agnostic learning bounds}
Similar to previous derivations, we can establish a bound on the sample complexity:
\[N\geq\dfrac{1}{2\varepsilon^2}\left(\ln\left\lvert H\right\rvert+\ln\left(\dfrac{1}{\delta}\right)\right)\]
Furthermore, we can also constrain the true error of the hypothesis as follows:
\[error_{true}(h)\leq error_{\mathcal{D}}(h)+\sqrt{\dfrac{\ln\left\lvert H \right\rvert+\ln\frac{1}{\delta}}{2N}}\]

\paragraph*{VC dimension}
The VC dimension represents the size of the subset of $X$ for which $\left\lvert H\right\rvert$ can ensure a zero training error, regardless of the target function $c$.
\begin{definition}[\textit{Dichotomy}]
    A dichotomy of a set $S$ of instances is defined as a partition of $S$ into two disjoint subsets, i.e., labeling each instance in $S$ as positive or negative.
\end{definition}
\begin{definition}[\textit{Shattered}]
    A set of instances $S$ is said to be shattered by hypothesis space $H$ if and only if for every dichotomy of $S$, there exists some hypothesis in $H$ consistent with this dichotomy.
\end{definition}
The Vapnik-Chervonenkis dimension, $VC(H)$, of hypothesis space $H$ over instance space $X$, is the largest finite subset of $X$ shattered by $H$.
If an arbitrarily large set of $X$ can be shattered by $H$, then $VC(H)=\infty$.

If $\left\lvert H\right\rvert < \infty$, then $VC(H) \leq \log_2(\left\lvert H\right\rvert)$.
When $VC(H)=d$, it implies that there are at least $2^d$ hypotheses in $H$ to label $d$ instances. 
Consequently, $\left\lvert H\right\rvert \geq 2^d$.
With a probability of at least $(1-\delta)$, every $h\in H$ satisfies the following inequality:
\[error_{true}(h)\leq error_{\mathcal{D}}(h)+\sqrt{\dfrac{VC(H)\left(\ln\frac{2N}{VC(H)}+1\right)+\ln\frac{4}{\delta}}{N}}\]