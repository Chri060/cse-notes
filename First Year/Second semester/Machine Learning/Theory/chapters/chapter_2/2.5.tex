\section{Support Vector Machines}

Kernel methods face a notable limitation: the need to compute the kernel function for every sample in the training set. 
Unfortunately, this computation can be computationally infeasible in practice. 
To address this challenge, sparse kernel methods seek solutions that rely only on a subset of the training samples.
Two well-known sparse kernel methods are:
\begin{enumerate}
    \item Support Vector Machines (SVMs).
    \item Relevance Vector Machines.
\end{enumerate}

\subsection{Separable problems}
The separation between data points can also be achieved using the perceptron algorithm. 
However, in this case, the final result is highly dependent on the initialization.

When choosing the best solution, consider the line that separates the points. 
Opt for the solution with fewer points close to that separating line.
To address this, we can utilize the maximum margin classifier, which computes the margin as follows:
\[\text{margin}=\min_n{\dfrac{t_n\left(\mathbf{w}^T\boldsymbol{\phi}(\mathbf{x}_n)+b\right)}{\left\lVert \mathbf{w}\right\rVert }}\]
The goal is to find the optimal hyperplane by maximizing the expression:
\[\argmax_{\mathbf{w},b}\left\{\min_n\left[\dfrac{t_n\left(\mathbf{w}^T\boldsymbol{\phi}(\mathbf{x}_n)+b\right)}{\left\lVert \mathbf{w}\right\rVert }\right]\right\}\]
However, solving this optimization problem can be very complex due to its computational demands and potential non-convexity.

To simplify the optimization problem, we first establish a canonical hyperplane across the separating variables. 
It's essential to acknowledge the existence of an infinite set of equivalent solutions represented by:
\[\kappa \textbf{w}^T\boldsymbol{\phi}(\textbf{x})+\kappa b \quad \forall\kappa>0\]
However, we will focus solely on solutions that adhere to the condition:
\[t_n\left(\mathbf{w}^T\boldsymbol{\phi}(\mathbf{x}_n)+b\right)=1\quad \forall\textbf{x}_n\in\mathcal{S}\]
Consequently, we transform the problem into an equivalent quadratic programming task aimed at minimizing:
\[\dfrac{1}{2}\left\lVert \textbf{w}\right\rVert_2^2 \]
subject to the constraint $t_n\left(\mathbf{w}^T\boldsymbol{\phi}(\mathbf{x}_n)+b\right)\geq 1$, for all $n$. 

\paragraph*{Dual problem}
We can obtain the dual problem by utilizing Lagrange multipliers, resulting in the following Lagrangian:
\[\mathcal{L}(\textbf{w},b,\boldsymbol{\alpha})=\dfrac{1}{2}\left\lVert \textbf{w}\right\rVert^2-\sum_{i=1}^n{\alpha_i(t_i(\textbf{w}^T\boldsymbol{\phi}(\textbf{x}_i))-1)} \]
To maximize $\mathcal{L}$ with respect to $\boldsymbol{\alpha}$ and minimize it with respect to $\mathbf{w}$ and $b$, we compute the gradients with respect to $\mathbf{w}$ and $b$ and derive the dual representation:
\[\begin{cases}
    \frac{\partial}{\partial\textbf{w}}\mathcal{L}=0 \\
    \frac{\partial}{\partial b}\mathcal{L}=0
\end{cases} \rightarrow \begin{cases}
    \textbf{w}=\sum_{i=1}^{n}{\alpha_i t_i\boldsymbol{\phi(\textbf{x}_i)}}\\
    \sum_{i=1}^{n}{\alpha_i t_i}=0
\end{cases}\]
This allows us to reformulate the optimization problem as the maximization of:
\[\tilde{\mathcal{L}}(\boldsymbol{\alpha})=\sum_{n=1}^N \alpha_n-\dfrac{1}{2}\sum_{n=1}^N\sum_{m=1}^N \alpha_n\alpha_m t_n t_m k(\textbf{x}_n,\textbf{x}_m)\]
subject to the constraints: 
\[\begin{cases}
    \alpha_n\geq 0 \\
    \sum_{n=1}^{N}{\alpha_n t_n = 0} \qquad \forall n=1,\dots,N
\end{cases}\]
where the explicit feature mapping no longer appears explicitly.

\paragraph*{Discriminant function}
The resulting discriminant function can be expressed as:
\[y(\textbf{x})=\sum_{n=1}^{N}\alpha_n t_n k(\textbf{x},\textbf{x}_n)+b\]
Here, only samples on the margin contribute, indicated by $\alpha_i > 0$. 
These crucial samples are known as the Support Vectors.
The bias term, denoted as $b$, is computed as:
\[b=\dfrac{1}{\left\lvert \mathcal{S}\right\rvert }\sum_{\textbf{x}_n\in\mathcal{S}} \left(t_n-\sum_{\textbf{x}_m\in\mathcal{S}}\alpha_m t_m k(\textbf{x}_n,\textbf{x}_m)\right)\]
This formulation ensures that the decision boundary is determined by the support vectors, reflecting the critical points in the data that define the separation between classes.

\subsection{Non-separable problems}
In our prior discussions, we've proceeded on the premise that samples are linearly separable within the feature space. 
Yet, this isn't universally applicable, especially in scenarios with noisy data or other complexities. 
To address these challenges, we introduce the concept of error (represented by $\xi_i$) into our classification methodology.

With this definition, we can introduce the soft-margin optimization problem, which aims to minimize:
\[\dfrac{1}{2}\left\lVert \textbf{w} \right\rVert_2^2+C\sum_{n=1}^{N}\xi_n \]
subject to the constraints:
\[t_n\left(\mathbf{w}^T\boldsymbol{\phi}(\mathbf{x}_n)+b\right) \geq 1-\xi_n \quad \forall n\]
where $\xi_n \geq 0$ are slack variables representing penalties for margin violations.
The parameter $C$ serves as a tradeoff between error and margin: it allows adjustment of the bias-variance tradeoff, and tuning may be necessary to find the optimal value for $C$.

\paragraph*{Dual problem}
By obtaining the dual problem, we aim to maximize:
\[\tilde{\mathcal{L}}(\boldsymbol{\alpha})=\sum_{n=1}^{N}\alpha_n-\dfrac{1}{2}\sum_{n=1}^{N}\sum_{m=1}^N{\alpha_n \alpha_m t_n t_m k(\mathbf{x}_n, \mathbf{x}_m)}\]
subject to the constraints:
\[\begin{cases}
    0 \leq \alpha_n \leq C \\
    \sum_{n=1}^{N}\alpha_n t_n = 0 \qquad n=1,\dots,N
\end{cases}\]
As usual, support vectors are the samples for which $\alpha_n > 0$.
If $\alpha_n < C$, then $\xi_n = 0$, indicating that the sample is on the margin.
If $\alpha_n = C$, the sample can be within the margin and either correctly classified ($\xi_n \leq 1$) or misclassified ($\xi_n > 1$).

\paragraph*{Alternative formulation}
The same problem can be also formulated as the maximization of: 
\[\tilde{\mathcal{L}}(\boldsymbol{\alpha})=-\dfrac{1}{2}\sum_{n=1}^{N}\sum_{m=1}^N{\alpha_n \alpha_m t_n t_m k(\mathbf{x}_n, \mathbf{x}_m)}\]
\[\begin{cases}
    0 \leq \alpha_n \leq \frac{1}{N} \\
    \sum_{n=1}^{N}\alpha_n t_n = 0  \\
    \sum_{n=1}^{N}\alpha_n\geq \nu \qquad n=1,\dots,N
\end{cases}\]
Where $0\leq\nu<1$ is a user-defined parameter that enables control over both the margin errors and the number of support vectors, ensuring that the fraction of margin errors is less than or equal to $\nu$ and the fraction of support vectors is also less than or equal to $\nu$.

\subsection{Support vector machines training}
To solve the optimization problem and find $\alpha_i$ and $b$, several methods exist. 
However, the direct solution is computationally expensive, typically $O(n^3)$ where $n$ is the size of the training set.

To mitigate this computational burden, faster approaches have been developed, including:
\begin{enumerate}
    \item Chunking: Breaking the problem into smaller chunks to solve separately.
    \item Osuna's methods: Variants of chunking methods specifically tailored for SVM optimization.
    \item Sequential Minimal Optimization (SMO): A method that optimizes the dual problem iteratively by selecting pairs of variables to update.
\end{enumerate}
Additionally, for scenarios where online learning is preferred, methods such as chunking-based approaches and incremental methods can be employed. 
These methods update the model gradually as new data becomes available, thus avoiding the need to retrain the entire model from scratch.

\paragraph*{Chunking}
Chunking solves iteratively by addressing a sub-problem known as the working set. 
The working set is constructed using the current support vectors and the $M$ samples with the largest errors (known as the worst set). 
It's important to note that the size of the working set may dynamically increase during the iterations. 
Despite this, chunking converges to the optimal solution.

\paragraph*{Osuna's Method}
Osuna's method also solves iteratively by focusing on a sub-problem, the working set. 
However, unlike chunking, it maintains a fixed size for the working set. 
This method replaces some samples in the working set with misclassified samples from the dataset. 
Despite its fixed-size working set, Osuna's method still converges to the optimal solution.

\paragraph*{Sequential Minimal Optimization}
SMO operates iteratively, but uniquely, it only works on two samples at a time. 
By doing so, it keeps the size of the working set minimal. 
Moreover, the multipliers are found analytically during each iteration. Like the other methods, SMO converges to the optimal solution.

\subsection{Multi-class Support vector machines}
\paragraph*{One against all}
In the one against all approach, a $k$-class problem is decomposed into $k$ binary (2-class) problems. 
Training involves $k$ SVM classifiers on the entire dataset. 
During testing, the class selected with the highest margin among the $k$ SVM classifiers is chosen.

\paragraph*{One against one}
In one against one, a $k$-class problem is decomposed into $\frac{k(k-1)}{2}$ binary problems. 
Here, $\frac{k(k-1)}{2}$ SVM classifiers are trained on subsets of the dataset. 
During testing, all $\frac{k(k-1)}{2}$ classifiers are applied to the new sample, and the most voted label is chosen.

\paragraph*{DAGSVM}
DAGSVM also decomposes the $k$-class problem into $\frac{k(k-1)}{2}$ binary problems like one-against-one. 
However, it employs a Direct Acyclic Graph during testing to reduce the number of SVM classifiers to apply.
This leads to only $k$-1 binary SVM classifiers being involved in the test process instead of $\frac{k(k-1)}{2}$ as in one-against-one.

\paragraph*{Summary}
The methods are: 
\begin{itemize}
    \item One-against-all: requires less memory but has expensive training and cheap testing.
    \item One-against-one: requires more memory but has slightly cheaper training and expensive testing.
    \item DAGSVM:\@ moderately expensive in terms of memory requirements, with slightly cheaper training and testing.
\end{itemize}
One-against-one is considered the best performing approach due to its effective decomposition. 
DAGSVM provides a faster approximation of one-against-one.