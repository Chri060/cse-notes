\section{Ensemble}

We've explored methods to decrease variance while balancing increased bias.
However, we want to reduce variance without amplifying bias or mitigate bias altogether. 

These objectives can indeed be achieved through the utilization of two ensemble methods involving the learning of multiple models and their combination:
\begin{itemize}
    \item \textit{Bagging}: involves training multiple models independently on different subsets of the data and then combining their predictions.
    \item \textit{Boosting}: utilizes an iterative approach where models are sequentially trained, each aiming to correct the errors of its predecessors, leading to the creation of a strong ensemble model.
\end{itemize}

\subsection{Bagging}
Let assume to have $N$ datasets and to learn from them $N$ models, $y_1,y_2,\dots,y_N$. 
Now let us compute an aggregate model as: 
\[y_{AGG}=\dfrac{1}{N}\sum_{i=1}^{N}y_i\]
If the datasets are independent, the model variance of $y_{AGG}$ will be $\frac{1}{N}$ of the model variance of the single model $y_i$.
However, we generally do not have $N$ datasets. 

Bagging, short for Bootstrap Aggregation, involves the following steps:
\begin{enumerate}
    \item Generate $N$ datasets by applying random sampling with replacement.
    \item Train a model (classification or regression) using each dataset generated.
    \item To predict new samples, apply all the trained models and combine their outputs using majority voting (for classification) or averaging (for regression).
\end{enumerate}
Bagging is generally beneficial as it reduces variance, although the sampled datasets are not independent. 
It proves particularly useful with unstable learners, characterized by significant changes with small dataset variations (low bias and high variance), and in scenarios with a high degree of overfitting (low bias and high variance). 
However, it does not offer much help with robust learners, which are insensitive to data changes (typically higher bias but lower variance).

\subsection{Boosting}
Boosting aims to minimize bias by employing a series of simple (weak) learners.

The core concept of boosting involves iteratively training a sequence of weak learners, with each iteration concentrating on the samples misclassified in the preceding iteration.

Ultimately, an ensemble model is constructed by combining the outputs of all the weak learners trained.

\subsection{Summary}
The characteristics of bagging are: 
\begin{itemize}
    \item Decreases variance.
    \item Less effective for stable learners.
    \item Applicable with noisy data.
    \item Generally beneficial, though the improvement might be modest.
    \item Naturally suited for parallelization.
\end{itemize}
The characteristics of boosting are: 
\begin{itemize}
    \item Reduces bias (typically without overfitting).
    \item Compatible with stable learners.
    \item May encounter challenges with noisy data.
    \item Not always effective, but can yield significant improvements.
    \item Sequential in nature.
\end{itemize}