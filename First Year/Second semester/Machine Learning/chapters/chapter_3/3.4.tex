\section{Ensemble}

Ensemble methods aim to reduce variance or bias (or both) by combining multiple models to improve overall predictive performance. 
These objectives are typically achieved through two main techniques:
\begin{itemize}
    \item \textit{Bagging}: reduces variance without increasing bias by training multiple models on different subsets of the data.
    \item \textit{Boosting}: reduces bias by sequentially combining weak learners to create a strong model.
\end{itemize}

\subsection{Bagging}
Bagging is an ensemble technique designed to reduce model variance, making it particularly useful for high-variance, low-bias models. 
The steps for bagging are as follows:
\begin{enumerate}
    \item Generate $N$ different datasets by applying random sampling with replacement (bootstrapping) from the original training data.
    \item Train a separate model (learner) on each of these datasets in parallel.
\end{enumerate}
For prediction, each model is applied to a new sample, and the outputs are combined: 
\begin{itemize} 
    \item In classification, a majority vote across the models is used. 
    \item In regression, predictions are averaged. 
\end{itemize}
Bagging reduces variance by averaging out the noise associated with individual models. 
It is particularly effective for unstable learners (models highly sensitive to small changes in the data).
For such learners, bagging can improve stability and performance.
However, bagging is less effective for robust learners with inherently low variance, like linear models.
\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|}
    \hline
    & \textbf{Bagging} \\ \hline
    \textbf{Primary Goal} & Reduces variance \\ \hline
    \textbf{Works Best With} & Unstable learners (sensitive to data changes) \\ \hline
    \textbf{Noise Handling} & Can be applied with noisy data \\ \hline
    \textbf{Effectiveness} & Usually helps, but the difference might be small \\ \hline
    \textbf{Execution} & Parallel \\ \hline
    \end{tabular}
\end{table}

\subsection{Boosting}
Boosting is an iterative ensemble technique that combines weak learners sequentially to build a strong model, focusing on reducing bias. 
The steps in boosting are as follows:
\begin{enumerate} 
    \item Assign equal weights to all samples in the training set initially. 
    \item Train a weak learner (often a simple model with high bias) on the weighted dataset. 
    \item Calculate the error of this model on the training set. 
    \item Increase the weights of the misclassified samples so that the next model focuses more on these difficult cases. 
    \item Repeat from step 2 until a stopping criterion is met (e.g., a maximum number of learners or a desired level of accuracy). 
\end{enumerate}
Once all learners are trained, they are combined by weighting each model's predictions according to its accuracy. 
For new samples, the ensemble prediction is a weighted sum (or weighted vote) of the predictions from all the weak learners, with more accurate learners contributing more heavily.
\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|}
    \hline
    & \textbf{Boosting} \\ \hline
    \textbf{Primary Goal} & Reduces bias (generally without overfitting) \\ \hline
    \textbf{Works Best With} & Stable learners (less sensitive to data changes) \\ \hline
    \textbf{Noise Handling} & Might have problems with noisy data \\ \hline
    \textbf{Effectiveness} & Not always helpful, but can make a significant difference \\ \hline
    \textbf{Execution} & Sequential \\ \hline
    \end{tabular}
\end{table}