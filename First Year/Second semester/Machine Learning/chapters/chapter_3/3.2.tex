\section{Model validation}

To select the optimal model and tune hyperparameters effectively, we first divide the data into three subsets:
\begin{enumerate}
    \item \textit{Training set} $\mathcal{D}_{\text{train}}$: used to learn the model parameters.
    \item \textit{Validation set} $\mathcal{D}_{\text{validation}}$: used to select the best model.
    \item \textit{Test set} $\mathcal{D}_{\text{test}}$: used to evaluate the final model's performance.
\end{enumerate}
A typical split is 50\%-25\%-25\% for training, validation, and test sets, respectively. 
For reliable validation, the validation set must be large enough to prevent overfitting to its specific samples, which could lead to suboptimal model selection.

\subsection{Leave-One-Out Cross Validation}
In leave-one-out cross-validation, the model is trained on all samples except one $\{\textbf{x}_i, t_i\}$, and the performance is assessed on that omitted sample. 
The overall prediction error estimate is the average error over all samples:
\[\mathcal{L}_{LOO}=\dfrac{1}{N}\sum_{i=1}^{N}{\left( t_i-y_{\mathcal{D}_i}(\mathbf{x}_i) \right)}^2\]
Here, $y_{\mathcal{D}_i}$ is the model trained on $\mathcal{D}$ excluding $\{\textbf{x}_i, t_i\}$. 

The $\mathcal{L}_{\text{LOO}}$ estimate is nearly unbiased, though slightly pessimistic. 
However, LOO-CV is computationally intensive, as it requires training $N$ models.

\subsection{K-Fold Cross Validation}
In K-fold cross-validation, the training data $\mathcal{D}$ is split into $k$ equally sized folds: $\mathcal{D}_1, \mathcal{D}_2, \dots, \mathcal{D}_k$. 
For each fold $\mathcal{D}_i$, the model is trained on $\mathcal{D}$ excluding $\mathcal{D}_i$, and the error is calculated on $\mathcal{D}_i$:
\[\mathcal{L}_{\mathcal{D}_i}=\dfrac{k}{N}\sum_{(\mathbf{x}_n,t_n) \in \mathcal{D}_i} {\left( t_n-y_{\mathcal{D}\setminus\{\mathcal{D}_i\}}(\mathbf{x}_n) \right)}^2\]
The prediction error is then estimated by averaging across all folds:
\[\mathcal{L}_{\text{k-fold}}=\dfrac{1}{k}\sum_{i=1}^{k}\mathcal{L}_{\mathcal{D}_i}\]

The $\mathcal{L}_{\text{k-fold}}$ estimate of prediction error is slightly biased (pessimistic) but more computationally feasible than LOO-CV. 
Typically, $k$ is set to 10.

\subsection{Adjustment tecniques}
Several metrics adjust the training error based on model complexity to aid in model evaluation for complex models:
\renewcommand*{\arraystretch}{2}
\begin{table}[H]
    \centering
    \begin{tabular}{|l|c|}
        \hline
        \textbf{Criteria} & \textbf{Formula} \\ \hline
        Mallows's $C_p$ & $C_p=\dfrac{1}{N}\left( \text{RSS}+2M\sigma^2 \right)$ \\ \hline
        Akaike Information Criteria (AIC) & $\text{AIC}=-2\ln(L)+2M$ \\ \hline
        Bayesian Information Criteria (BIC) & $\text{BIC}=-2\ln(L)+M\ln(N)$ \\ \hline
        Adjusted $R^2$ & $A_{R^2}=1-\dfrac{\text{RSS}/(n-m-1)}{\text{TSS}/(N-1)}$ \\ \hline
    \end{tabular}
\end{table}
\renewcommand*{\arraystretch}{1}
In these formulas:
\begin{itemize}
    \item $M$: number of model parameters
    \item $N$: number of samples
    \item $L$: likelihood function
    \item $\sigma^2$: estimate of noise variance
    \item RSS: residual sum of squares
    \item TSS: total sum of squares
\end{itemize}
AIC and BIC are often used when maximizing the log-likelihood. 
Compared to AIC, BIC imposes a stronger penalty for model complexity, favoring simpler models.