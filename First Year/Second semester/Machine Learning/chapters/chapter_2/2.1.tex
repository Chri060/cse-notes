\section{Introduction}

Supervised learning stands as the predominant and well-established learning approach. 
Its core objective is to enable a computer, given a training set $\mathcal{D}=\left\{\left\langle x,t \right\rangle\right\}$, to approximate a function $f$ that maps an input $x$ to an output $t$.
The input variables $x$, often referred to as features or attributes, are paired with output variables $t$, also known as targets or labels. 
The tasks undertaken in supervised learning are as follows:
\begin{itemize}
    \item \textit{Classification}: when $t$ is discrete. 
    \item \textit{Regression}: when $t$ is continuous. 
    \item \textit{Probability estimation}: when$t$ represents a probability.
\end{itemize}
Supervised learning finds application in scenarios where:
\begin{itemize}
    \item Humans lack the capability to perform the task directly (e.g., DNA analysis).
    \item Humans can perform the task but lack the ability to articulate the process (e.g., medical image analysis).
    \item The task is subject to temporal variations (e.g., stock price prediction).
    \item The task demands personalization (e.g., movie recommendation).
\end{itemize}

\subsection{Function approximation}
The process of approximating a function $f$ from a dataset $\mathcal{D}$ involves several steps:
\begin{enumerate}
    \item \textit{Define a loss function} $\mathcal{L}$: this function calculates the discrepancy between $f$ and $h$, a chosen approximation.
    \item \textit{Select a hypothesis space} $\mathcal{H}$: this space consists of a set of candidate functions from which to choose an approximation $h$. 
    \item \textit{Minimize} $\mathcal{L}$ \textit{within} $\mathcal{H}$: the goal is to find an approximation $h$ within the hypothesis space $\mathcal{H}$ that minimizes the loss function $\mathcal{L}$.
\end{enumerate}
The hypothesis space $\mathcal{H}$ can be expanded to theoretically achieve a perfect approximation of the function $f$. 
However, a significant challenge arises because the loss function $\mathcal{L}$ cannot be easily determined, primarily due to the absence of the actual function $f$.

\subsection{Taxonomy}
The taxonomy is as follows: 
\begin{itemize}
    \item \textit{Parametric} or \textit{nonparametric}: parametric methods are characterized by having a fixed and finite number of parameters, while nonparametric methods have a number of parameters that depend on the training set.
    \item \textit{Frequentist} or \textit{Bayesian}: frequentist approaches utilize probabilities to model the sampling process, whereas Bayesian methods use probability to represent uncertainty about the estimate.
    \item \textit{Empirical risk minimization} or \textit{structural risk minimization}: empirical risk refers to the error over the training set, while structural risk involves balancing the training error with model complexity.
\end{itemize}
The type of Machine Learning can be: 
\begin{itemize}
    \item \textit{Direct}: this method involves learning an approximation of $f$ directly from the dataset $\mathcal{D}$.
    \item \textit{Generative}: in this approach, the model focuses on modeling the conditional density $\Pr(t | x)$ and then marginalizing to find the conditional mean:
        \[\mathbb{E} \left[ t | x \right] = \int t \Pr(t | x)\, dt\]
    \item \textit{Discriminative}: this method models the joint density $\Pr(x,t)$, infers the conditional density $\Pr(t | x)$, and then marginalizes to find the conditional mean:
        \[\mathbb{E} \left[ t | x \right] = \int t \Pr(t | x)\, dt\]
\end{itemize}