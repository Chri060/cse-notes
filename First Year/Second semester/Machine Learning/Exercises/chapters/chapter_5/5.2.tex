\section{Exercise two}

Comment on the following statements about Gaussian Processes (GP). 
Assume to have a dataset generated from a GP $\mathcal{D} = (x_i, y_i)^N_{i=1}$.
Motivate your answers.
\begin{enumerate}
    \item GPs are parametric methods.
    \item The computation of the estimates of the variance of the GP $\hat{\sigma}^2(x)$ corresponding to the input $x$ provided by $\mathcal{D}$ does not require the knowledge of the samples output $(y_1,\ldots, y_N)$.
    \item In the neighborhood of the input points $(x_1,\ldots,x_N)$, we observed the variance of the GP gets smaller and smaller as we collect more samples.
    \item The complexity of the computation of the estimates of the mean $\hat{\mu}(x)$ and variance $\hat{\sigma}^2(x)$ scales as $N^3$, i.e., cubically with the number of samples $N$.
\end{enumerate}

\subsection*{Solution}
\begin{enumerate}
    \item False: they require to store the gram matrix whose dimension depends on the number of samples.
    \item True: it requires only the gram matrix and the computation of the kernel on the new point.
    \item True: the uncertainty we have around the sampled points decreases as we get more and more samples.
    \item True: indeed, it requires the inversion of the gram matrix which has $N^3$ computational cost.
\end{enumerate}