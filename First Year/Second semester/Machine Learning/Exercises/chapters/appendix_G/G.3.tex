\section{Control}

\paragraph*{Policy selection}
Choosing a policy can be achieved through:
\begin{itemize}
    \item \textit{Brute force}: exhaustively enumerate all possible policies, evaluate their values, and select the one maximizing value. 
        This method guarantees a deterministic optimal policy but requires evaluating  $\left\lvert\mathcal{A}\right\rvert^{\left\lvert\mathcal{S}\right\rvert}$ policies.
    \item \textit{Dynamic Programming}:
        \begin{itemize}
            \item \textit{Policy iteration}: iteratively evaluate the current policy and update it in the greedy direction.
            \item \textit{Value iteration}: repeatedly apply the Bellman optimality equation in its recursive form. 
                However, solving the Bellman optimality equation in closed form is not possible due to the non-linearity introduced by the max operator:
            \[V^\ast(s)=\max_{a\in\mathcal{A}}\left\{R(s,a)+\gamma\sum_{s^\prime\in\mathcal{S}}P(s^\prime|a,s)V^\ast(s^\prime)\right\}\]
        \end{itemize}
\end{itemize}

\paragraph*{Policy iteration}
Iterate until convergence:
\begin{enumerate}
    \item \textit{Policy evaluation}: compute the value $V^{\pi_k}$ of the current policy $\pi_k$.
    \item \textit{Policy improvement}: update the policy from $\pi_k$ to $\pi_{k+1}$ based on the newly estimated values (greedy improvement):
        \begin{align*}
            \pi_{k+1}(s)    &= \argmax_{a\in\mathcal{A}}Q^{\pi_k}(s,a) \\
                            &= \argmax_{a\in\mathcal{A}}\left\{R(s,a)+\gamma\sum_{s^\prime\in\mathcal{S}}P(s^\prime|a, s)V^{\pi_k}(s^\prime)\right\}\quad\forall s \in \mathcal{S}
        \end{align*}
        Policy iteration guarantees convergence to $\pi^\ast$ in a finite number of steps.
\end{enumerate}

\paragraph*{Value iteration}
Directly evaluate the optimal policy by computing $V^\ast(s)$.
Repeatedly apply the Bellman optimality equation:
\[V_{k+1}(s) = \max_{a\in\mathcal{A}}\left\{R(s, a) + \gamma\sum_{s^\prime\in\mathcal{S}}P(s^\prime|s, a)V_k(s^\prime)\right\}\]
Once $V^\ast(s)$ is obtained, recover the optimal policy, i.e., the greedy one with respect to $V^\ast(s)$.
Value iteration asymptotically converges to $V^\ast(s)$.