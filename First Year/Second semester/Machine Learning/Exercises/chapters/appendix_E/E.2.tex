\section{Gaussian processes}

A Gaussian process is defined as a probability distribution over functions $y(\mathbf{x}_i)$ where the values of $y(\mathbf{x}_i)$ evaluated at any set of points $\mathbf{x}_1,\ldots, \mathbf{x}_N$ jointly have a Gaussian distribution.
This distribution is completely specified by the mean and the covariance:
\begin{itemize}
    \item Usually, we do not have any prior information about the mean of $y(\mathbf{x})$, so we take it to be zero.
    \item The covariance is given by the kernel function:
        \[\text{Cov}[y(\mathbf{x}_i),y(\mathbf{x}_j)|\mathbf{x}_i,\mathbf{x}_j]=\mathbb{E}[y(\mathbf{x}_i)y(\mathbf{x}_j)|\mathbf{x}_i,\mathbf{x}_j]=K(\mathbf{x}_i,\mathbf{x}_j)\]
\end{itemize}
With this formulation, Gaussian Processes (GPs) are kernel methods that can be applied to solve regression problems.

The target is represented as $\mathbf{t} = y(\mathbf{x}) + \varepsilon$, where $\varepsilon \sim \mathcal{N} (0, \sigma^2)$ is a noise term independent of the point $\mathbf{x}$.
The conditional distribution of the targets $\mathbf{t}_N = (t_1,\ldots, t_N )^T$ of size $N$ is given by:
\[p(\mathbf{t}_N|\mathbf{y}_N)=\mathcal{N}(\mathbf{t}_N|\mathbf{y}_N,\sigma^2\mathbf{I}_N)\]
Here, $\mathbf{y}_N=(y(\mathbf{x}_1),\ldots,y(\mathbf{x}_N))^T$.
The prior distribution of $p(\mathbf{y}_N)=\mathcal{N}(0,\mathbf{K}_N)$, where:
\[\mathbf{K}_N=\begin{bmatrix}
    K(\mathbf{x}_1,\mathbf{x}_1) & \cdots & K(\mathbf{x}_1,\mathbf{x}_N) \\
    \vdots & \ddots & \vdots \\
    K(\mathbf{x}_N,\mathbf{x}_1) & \cdots & K(\mathbf{x}_N,\mathbf{x}_N)
\end{bmatrix}\]
Hence, the marginal distribution of the target is:
\[p(\mathbf{t}_N)=\int p(\mathbf{t}_N|\mathbf{y}_N)p(\mathbf{y})\,d\mathbf{y}_N=\mathcal{N}(\mathbf{t}_N|\mathbf{0},\mathbf{C}_N)\]
Here, $\mathbf{C}_N=\mathbf{K}_N+\sigma^2\mathbf{I}_N$.
Our objective is to predict the target $\mathbf{t}_{N+1}$ corresponding to a specific unseen input $\mathbf{x}_{N+1}$.
From the definition, we have:
$p(\mathbf{t}_{N+1}) = \mathcal{N} (\mathbf{t}_{N+1}|\mathbf{0}, \mathbf{C}_{N+1})$,
Where:
\[\mathbf{C}_{N+1}=\begin{bmatrix}
    \mathbf{C}_{N} & \mathbf{k} \\
    \mathbf{k}^T & c
\end{bmatrix} \quad \mathbf{k}=(K(\mathbf{x}_1,\mathbf{x}_{N+1}),\ldots,K(\mathbf{x}_N,\mathbf{x}_{N+1}))^T \quad c=K(\mathbf{x}_{N+1},\mathbf{x}_{N+1})+\sigma^2\]
We need to compute $p(\mathbf{t}_{N+1}|\mathbf{t}_N , \mathbf{x}_1,\ldots,\mathbf{x}_N) = \mathcal{N} (m(\mathbf{x}_{N+1}), \sigma^2(\mathbf{x}_{N+1}))$, where: 
\begin{itemize}
    \item Mean: $m(\mathbf{x}_{N+1}) = \mathbf{k}^T\mathbf{C}_N^{-1}\mathbf{t}$
    \item Variance: $\sigma^2(\mathbf{x}_{N+1}) = c -\mathbf{k}^T\mathbf{C}_N^{-1}\mathbf{t}$
\end{itemize}

\paragraph*{Python}
To model the relationship between petal length and width as a Gaussian Process (GP), follow these steps:
\begin{enumerate}
    \item Load the data and normalize them.
    \item Select the values of:
        \begin{itemize}
            \item Noise variance $\sigma^2 = \text{Var}[\varepsilon] = 0.2$
            \item Constant $k = 1$
            \item Length-scale $l = 0.8$
        \end{itemize}
        \[K(\mathbf{x}_i,\mathbf{x}_j)=ke^{-\frac{\left\lVert \mathbf{x}_i-\mathbf{x}_j \right\rVert _2^2 }{2l^2}}\]
    \item Initialize a GP regression model (\texttt{GaussianProcessRegressor}).
    \item Predict new values.
\end{enumerate}

\paragraph*{Hyperparameters}
Although Gaussian Processes (GPs) are non-parametric methods, the noise variance $\sigma^2$ and the parameters of the kernel have to be estimated or set. 
This can be done by:
\begin{itemize}
    \item Utilizing a priori information on the problem being analyzed.
    \item Maximizing their log-likelihood on an independent dataset.
    \item Potentially refining them as new data are collected.
\end{itemize}
However, it's important to note a caveat: often, these hyperparameters are estimated using the same data used for prediction. 
This practice is not advisable in machine learning, as it is equivalent to overfitting.