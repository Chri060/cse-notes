\section{Introduction}

In the case that the model you are considering is not performing well even after tuning its parameters properly through cross-validation, two opposite options emerge: simplify the model or increase its complexity. 
In the second scenario, one might approach the problem in a more complex space by introducing handcrafted features or examining the problem within the kernel space.

\subsection{Kernel construction}
To leverage kernel substitution, it's essential to have valid kernel functions:
\begin{enumerate}
    \item Choose a feature space mapping $\phi(\mathbf{x})$ and use it to find the corresponding kernel:
        \[k(\mathbf{x},\mathbf{x}^\prime)=\phi(\mathbf{x})^T\phi(\mathbf{x}^\prime)=\sum_{i=1}^M\phi_i(\mathbf{x})\phi_i(\mathbf{x}^\prime)\]
        Here, $\phi(\mathbf{x})$ represents basis functions such as polynomials.
        For each $i$, we select $\phi_i(\mathbf{x})=\mathbf{x}^i$ in the one-dimensional case. 
    \item Without explicitly constructing the function $\phi(\mathbf{x})$, a necessary and sufficient condition for a function $k(\mathbf{x},\mathbf{x}^\prime)$ to be a kernel is the Gram matrix $\mathbf{K}$, where its elements are given by $k(\mathbf{x}_n,\mathbf{x}_m)$, to be positive semi-definite for all possible choices of the set $\{\mathbf{x}_n\}$
        Positive semi-definite does not imply that the matrix's elements are non-negative; rather, it means $\mathbf{y}^T\mathbf{Ky} \geq 0$ for non-zero vectors $\mathbf{y}$ with real entries.
        In other words, for any real numbers $\{\mathbf{x}_n\}$ such that $\sum_n\sum_mK_{n,my_ny_m\geq 0}$.
        New kernels can be constructed from simpler kernels as building blocks, thanks to Mercer's theorem.
\end{enumerate}