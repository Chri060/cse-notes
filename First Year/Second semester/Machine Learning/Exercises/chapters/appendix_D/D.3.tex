\section{Ensemble methods}

Ensembling methods offer another avenue for enhancing predictive performance by combining multiple models.
Two widely adopted ensembling techniques are bagging and boosting.

\subsection{Bagging}
The goal of bagging is to achieve a reduction in variance without significantly increasing bias. 
This is accomplished by training multiple learners, possibly in parallel:
\begin{enumerate}
    \item Generate multiple datasets by applying random sampling with replacement (bootstrapping).
    \item Train a model on each dataset.
\end{enumerate}
To make predictions for new samples, apply all the trained models and combine their outputs using techniques such as majority voting (for classification) or averaging.

Bagging is generally effective in reducing variance, although the sampled datasets are not independent. 
It is particularly beneficial for unstable learners, which are models that exhibit significant changes with even small variations in the dataset, typically characterized by low bias and high variance in regression tasks.

\subsection{Boosting}
The goal of boosting is to achieve low bias by utilizing simple (weak) learners while simultaneously minimizing variance. 
This is accomplished by sequentially training weak learners:
\begin{enumerate}
    \item Initially, assign equal weights to all samples in the training set.
    \item Train a weak learner on the weighted training set.
    \item Compute the error of the trained model on the weighted training set.
    \item Increase the weights of samples misclassified by the model.
    \item Repeat steps 2-4 until some predefined stopping criterion is met.
\end{enumerate}
The ensemble of models learned through this process can be applied to new samples by computing the weighted prediction of each model, where more accurate models are given higher weights.

\subsection{Comparison}
Bagging:
\begin{itemize}
    \item Reduces variance.
    \item Not suitable for stable learners.
    \item Applicable with noisy data.
    \item Generally provides assistance, though the impact may be modest.
    \item Executes in parallel.
\end{itemize}
Boosting:
\begin{itemize}
    \item Reduces bias (typically without overfitting).
    \item Compatible with stable learners.
    \item May encounter issues with noisy data.
    \item Not always beneficial, but it can significantly improve performance.
    \item Operates sequentially.
\end{itemize}