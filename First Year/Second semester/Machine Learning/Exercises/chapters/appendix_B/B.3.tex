\section{Logistic regression}

Logistic regression functions as a probabilistic discriminative approach by directly assigning probabilities to elements belonging to certain sets.

\subsection{Functions definition}
To proceed with our analysis, we must define the following elements:
\begin{itemize}
    \item Hypothesis space: 
        \[y(\textbf{x})=\sigma(\textbf{w}^T\textbf{x})=\sigma(w_0+x_1w_1+x_2w_2)\]
        Here, $\sigma=\frac{1}{1+e^{z}}$. 
    \item Loss function: distance of misclassified points in ${\{(\textbf{x}_n, t_n)\}}^{N}_{n=1}$ with $t_n\in\{-1,1\}$: 
        \[L_P(\textbf{w})=\text{P}(\textbf{y}|\textbf{X},\textbf{w})=\sum_{n=1}^N t_n\ln y(\textbf{x}_n)+\left(1-t_n\right)\ln(1-y(\textbf{x}_n))\]
    \item Optimization method: online gradient descent.
\end{itemize}

\paragraph*{Result evaluation}
To visualize the separating hyperplane or decision boundary (line) we need to plot:
\[\sigma(\textbf{x}^T\textbf{x}) = \dfrac{1}{2} \rightarrow \sigma(w_0 + x_1w_1 + x_2w_2) = \dfrac{1}{2} \rightarrow x_2=-\dfrac{w_1x_1+w_0}{w_2}\]


\subsection{Python implementation}
Implementation in Python can be accomplished through the \texttt{sklearn} library utilizing the \texttt{LogisticRegression} module.

\subsection{Logit}
Given the function:
\[\textit{logit}(y)=\log\left(\dfrac{y}{1-y}\right)\]
We can utilize it with the output of logistic regression:
\[\text{logit}(y(\textbf{x}))=\textbf{w}^T\textbf{x}=w_0+x_1w_1+x_2w_2\]
This exhibits a similar characterization to linear regression. 
Consequently, we can conduct hypothesis testing to ascertain the significance of the parameters.