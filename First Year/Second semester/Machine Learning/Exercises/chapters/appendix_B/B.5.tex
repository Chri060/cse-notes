\section{K-nearest neighbor}

The $k$-nearest neighbor is a discriminative function approach. 

\paragraph*{1-nearest neighbor}
The concept revolves around leveraging nearby points to predict the target of a new point.
Given a dataset ${\left\{ \textbf{x}_n,t_n \right\}}_{i=1}^{N}$ and a new data point $\textbf{x}_q$, we predict the target as:
\[i_q\in \argmin_{n \in \left\{ 1,\dots,N \right\}}\left\lVert \textbf{x}_q-\textbf{x}_n\right\rVert_2 \]
Resulting in $\hat{t}_q=t_{i_q}$. 
This approach seamlessly caters to both regression and classification tasks without explicit training. The training process is essentially querying the dataset.

Key design choices include the selection of the distance function and the number of neighbors. 
If $k>1$, the targets can be combined as follows:
\begin{itemize}
    \item For classification, predict the mode class (with tie-breaking rules):
        \[\hat{t}_q\in\argmax_{C_k}\left\lvert \{ i \in \mathcal{N}_k(\textbf{x}_q):t_i=C_k \}\right\rvert \]
    \item For regression, predict the average target:
        \[\hat{t}_q=\dfrac{1}{k}\sum_{i \in \mathcal{N}_k(\textbf{x}_q)}t_i\]
    \item Other approaches include providing a probability distribution instead of a class, using weights proportional to the inverse of the distance. 
\end{itemize}
The choice of $k$ introduces varying degrees of regularization, ranging from strong to mild regularization.