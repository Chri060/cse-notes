\section{Exercise 1}

Consider the following snippet of code:
\begin{verbatim}
V1 = np.linalg.inv(np.eye(nS) - gamma * pi @ P_sas) @ (pi @ R_sa)

V_old = np.zeros(nS)
tol = 0.0001
V2 = pi @ R_sa
while np.any(np.abs(V_old - V2) > tol):
    V_old = V2
    V2 = pi @ (R_sa - gamma * P_sas @ V)
\end{verbatim}
\begin{enumerate}
    \item Describe the purpose of the procedure of line 1 and the purpose of the procedure of lines 3-8. 
        Are they correct? 
        If not, propose a correction.
    \item What is the main disadvantage of the procedure of line 1 compared to the one of lines 3-8?
    \item What happens to the two procedures when gamma= 1?
\end{enumerate}

\subsection*{Solution}
\begin{enumerate}
    \item The procedure of line 1 computes the closed-form solution of the state value function $V^\pi$ of policy pi in the MDP with transition model \texttt{P\_sas}, reward \texttt{R_sa} and discount factor gamma. 
        The procedure of lines 3-8 performs the iterative application of the Bellman expectation operator to compute the same value function $V^\pi$. 
        The iterative procedure is stopped when a given threshold tol between consecutive approximation is reached. 
        However, line 8 does contain a mistake and should be corrected as follows:
        \begin{verbatim}
V2 = pi @ (R_sa + gamma * P_sas @ V)
        \end{verbatim}
    \item The main disadvantage of the procedure of line 1 compared to the one of lines 3-8 is a computational one, i.e., the computation of the closed-form solution might be infeasible when the number of states/actions is large.
    \item When gamma= 1 the procedure of line 1 might lead to a singular matrix (attempting to invert it), whereas the procedure of lines 3-8 might never reach the requested tolerance tol.
\end{enumerate}