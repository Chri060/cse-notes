\section{Exercise three}

Consider a linear regression with input $x$, target $y$ and optimal parameter $\theta^\ast$.
\begin{enumerate}
    \item What happens if we consider as input variables $x$ and $2x$?
    \item What we expect on the uncertainty about the parameters we get by considering as input variables $x$ and $2x$?
    \item Provide a technique to solve the problem.
    \item What happens if we consider as input variables $x$ and $x^2$?
\end{enumerate}
Motivate your answers.

\subsection{Solution}
The original formulation is: 
\[y=\theta^\ast x\]
\begin{enumerate}
    \item In this scenario, the formulation simplifies to:
        \[y=\theta_1x+\theta_22x\]
        As the two variables are dependent, it can alternatively be expressed as:
        \[y=\underbrace{\left(\theta_1+2\theta_2\right)}_{\theta^\ast} x\]
        Thus, yielding the original formulation:
        \[y=\theta^\ast x\]

        Moreover, for computing the closed-form optimization, the formula employed is:
        \[\omega=\left(x^Tx\right)^{-1}xt\]
        However, in this particular case, the matrix $x$ takes the form:
        \[x=\begin{bmatrix}
            x_1 & 2x_1 \\
            x_2 & 2x_2 \\
            \vdots & \vdots \\
            x_n & 2x_n \\
        \end{bmatrix}\]
        Hence, $x^Tx$ becomes singular, rendering the previous formula inapplicable.

        In general, if $x$ lacks full rank, then $x^Tx$ becomes singular, and its inverse cannot be computed.
    \item The parameter we get have a high variance, since we have an infinite number of couples of parameters minimizing the loss of the samples in the considered problem. 
        Indeed, if the parameters of the two inputs are $w_1$ and $w_2$ we would have that the true relationship would be:
        \[y=\theta_1x+\theta_22x=\left(\theta_1+2\theta_2\right)x\]
        Which can be satisfied by an infinite number of solutions.
    \item In this case, the use of ridge regression is able to partially cope with the influence of using highly linearly correlated features. 
        Another viable option is to remove the variables which are linearly dependent, for instance by checking if they have correlation equal to $1$ or $-1$. 
    \item In this case we do not have a badly conditioned matrix since $x$ and $x^2$ are not linearly dependent and the corresponding design matrix would not be ill-conditioned.
        As a result, we can find a closed form optimization. 
\end{enumerate}