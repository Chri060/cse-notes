\section{Exercise three}

Answer to the following questions about the bias-variance decomposition, model selection, and related topics. 
Motivate your answers.
\begin{enumerate}
    \item If your linear regression model underfits the training data (i.e., the model is not complex enough to explain the data), would you apply PCA to compute a more suitable feature space for your model?
    \item If solving a regression problem, the design matrix $X^TX$, is singular, would you apply PCA to solve this issue?
    \item Assuming a classifier fits very well the training data but underperforms on the validation set, would you apply Bagging or Boosting to improve it?
    \item Assuming that you trained a classifier with a $K$-fold cross-validation and it consistently has poor performances both on training and on validation folds, would you apply Bagging or Boosting to improve it?
    \item You applied ridge regression to train a linear model using a rather large regularization coefficient, would you think that bagging would improve your model?
    \item You have been asked to implement a feature selection process on a system with very limited computational resources. Would you opt for a filter approach or for a wrapper approach?
    \item You have been asked to implement a feature selection process to improve as much as possible the performance of your model. Would you opt for a filter approach or for a wrapper approach?
    \item You need to train a linear regression model using as input the readings of several sensors. 
        Assuming that you know that some of these sensors might be faulty (i.e., resulting in meaningless readings), which linear regression approach would you use to train your model?
    \item A linear regression model, computed using ordinary least squares, has a validation error that is much larger than training error. 
        Assuming that you do not want to change neither the input features nor the kind of model, what would you do to improve it?
    \item If you have to choose among a few models knowing only the training error (assuming you cannot retrain them or evaluate them on a different dataset), what would you do?
\end{enumerate}

\subsection*{Solution}
\begin{enumerate}
    \item No, because if linear regression does not fit training data (underfitting), the features computed with PCA will not solve the problem as they are linear combinations of input variables.
    \item Yes, applying PCA and selecting the top K components we will allow to avoid co-linearity in the resulting feature space.
    \item Bagging, because it might reduce the variance of the model. 
        In contrast, boosting allows to reduce bias without increasing (significantly) the variance (however, in this example we have a low bias and high variance).
    \item Boosting, because it can successfully reduce bias of a stable learner without increasing the variance which seems to be the problem of the learner in this case.
    \item No, because ridge regression with large regularization coefficient will be very stable (limited variance) and this would not allow to exploit significantly bagging.
    \item Filter, because a wrapper approach involves solving an optimization problem that requires training several models. 
        In contrast, filter approaches only require to compute statistics on the features.
    \item Wrapper, because filter approaches assume features are independent and might not find the best subset.
    \item Lasso, because it implicitly performs a feature selection that will get rid of the faulty sensors.
    \item Regularization can help me improve the performance by reducing the variance. 
        In particular ridge regression would be the most obvious choice. 
        Another solution, if viable, is to increase the number of samples used for training.
    \item Adjusted complexity matrix can be used to correct the training error taking into account also the model complexity.
\end{enumerate}