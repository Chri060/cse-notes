\section{Exercise one}

Consider the following statement regarding PCA and tell if they are true or false. Provide motivation for your answers.
\begin{enumerate}
    \item Even if all the input features are on very similar scales, we should still perform mean normalization (so that each feature has zero mean) before running PCA.
    \item Given only scores $t_i$ and the loadings $W$, there is no way to reconstruct any reasonable approximation to $x_i$. 
    \item Given input data $x_i\in\mathbb{R}^d$, it makes sense to run PCA only with values of $k$ that satisfy $k \leq d$.
    \item PCA is susceptible to local optima, thus trying multiple random initializations may help.
\end{enumerate}

\subsection*{Solution}
\begin{enumerate}
    \item True: since the principal components are identifying the directions where the most of the variance of the data is present, where the directions is defined as a vector with tail in the origin, we should remove the mean values for each component in order to identify correctly these directions.
    \item False: by applying again the loadings matrix $W$ to the scores $t_i$, thanks to the orthogonality property of $W$, we are able to reconstruct perfectly the original mean normalized vectors. 
        If we want to reconstruct the original vectors we should also store the mean values for each dimension.
    \item True: running it with $k = d$ is possible but usually not helpful and $k > d$ does not make sense.
    \item False: there is no source of randomization and no initialization point in the algorithm to perform PCA.
\end{enumerate}