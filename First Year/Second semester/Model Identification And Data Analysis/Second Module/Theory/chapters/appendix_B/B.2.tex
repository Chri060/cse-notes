\section{Singular Value Decomposition}

Singular Value Decomposition is a fundamental matrix factorization technique in linear algebra. 
It decomposes a matrix into three separate matrices, revealing the inherent structure and properties of the original matrix. 
Given a matrix $A$ of size $m \times n$, the Singular Value Decomposition of $A$ is represented as:
\[A=U\Sigma V^T\]
Here: 
\begin{itemize}
    \item $U$ is an $m \times m$ orthogonal matrix, where the columns are the left singular vectors of $A$. 
    \item $\Sigma$ is an $m \times n$ diagonal matrix with non-negative real numbers on the diagonal, known as the singular values of $A$. 
        The diagonal entries are typically arranged in descending order.
    \item $V$ is an $n \times n$ orthogonal matrix, where the columns are the right singular vectors of $A$.
\end{itemize} 

\paragraph*{Usage}
Singular Value Decomposition has been a significant milestone in machine learning due to its ability to efficiently address several key problems:
\begin{enumerate}
    \item \textit{Compression of information}: Singular Value Decomposition allows for the compression of information by representing data in a lower-dimensional space while retaining essential features. 
        This is achieved by truncating the singular values and corresponding singular vectors, effectively reducing the size of the data representation.
    \item \textit{Separation of signal and noise}: Singular Value Decomposition can be used to separate signal from noise in data. 
        By decomposing the data matrix into its singular values and vectors, it becomes possible to isolate dominant patterns (signal) from random fluctuations (noise).
    \item \textit{Solution to order-reduction problems}: Singular Value Decomposition provides a solution to order-reduction problems by capturing the dominant modes or patterns in the data while discarding less significant components. 
        This is particularly useful for reducing the complexity of models or data representations while preserving essential information.
\end{enumerate}

\paragraph*{Rank reduction}
Singular Value Decomposition enables an optimal rank reduction, which is crucial for minimizing the loss of information during dimensionality reduction. 
This optimal rank reduction aims to minimize the residual matrix, ensuring that the discarded components contain the least amount of information possible.
Formally, an optimal rank reduction minimizes the Frobenius norm of the residual matrix, denoted as $\left\lvert H_{\text{residual}_{qb}} \right\rvert _F$, which is defined as the square root of the sum of squared elements of the residual matrix:
\[\min\left\lvert H_{\text{residual}_{qb}} \right\rvert _F=\sqrt{\sum_{i,j}\left(H_{\text{residual}_{qb}}^{i,j}\right)^2} \]