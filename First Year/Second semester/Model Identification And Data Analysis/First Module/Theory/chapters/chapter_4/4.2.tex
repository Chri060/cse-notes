\section{Prediction problem}

Consider a zero-mean ARMA process represented as:
\[y(t)=W(z)e(t) \qquad e(t)\sim WN(0,\lambda^2)\]
We make the following assumptions:
\begin{enumerate}
    \item The pair $\left( W(z),e(t) \right)$ constitutes a canonical representation of $y(t)$.
    \item $W(z)$ has no zeros on the unit circle boundary.
\end{enumerate}

\paragraph*{K-step prediction}
Given observations up to time $t$:
\[\dots,y(t-100),y(t-99),\dots,y(t-1),y(t)\]
we aim to predict the future value of the process at time $t+k$:
\[\hat{y}(t+k|t)\]

Considering the sample set $y(t), y(t-1),y(t-2)$, potential one-step predictors include:
\begin{itemize}
    \item Simple average of past values: $\hat{y}(t+1|t)=\dfrac{y(t)+y(t-1)+y(t-2)}{3}$.
    \item Weighted average of past values: $\hat{y}(t+1|t)=\dfrac{2y(t)+\frac{1}{2}y(t-1)+\frac{1}{2}y(t-2)}{3}$.
    \item Geometric mean of past values: $\hat{y}(t+1|t)=\sqrt[3]{y(t)y(t-1)y(t-2)}$.
    \item Complex rules based on prior experimental knowledge.
\end{itemize}
However, in these examples, the underlying model generating the samples isn't fully considered, neglecting valuable information.
For an optimal predictor, we seek to leverage both the information from the model (data generation mechanism) and the information from past observations (specific realization).