\section{AR and ARX models}

Let's consider the AR and ARX class of models:
\[\mathcal{M}(\vartheta): y(t)=\dfrac{B(z)}{A(z)}u(t-d)+\dfrac{1}{A(z)}e(t)\qquad e(t \sim WN(0,\lambda^2))\]
Here, the polynomials are defined as:
\[\begin{cases}
    A(z)=1-a_1z^{-1}-a_zz^{-2}-\dots-a_mz^{-m} \\
    B(z)=b_1+b_2z^{-1}+b_3z^{-2}+\dots+b_pz^{-p+1} \\
    C(z)=1
\end{cases}\]
Where: 
\[\vartheta=\begin{bmatrix}
    a_1 & a_2 & a_n & \dots & a_m & b_1 & b_2 & \dots & b_p
\end{bmatrix}^T\]
We can rewrite the model as:
\begin{align*}
    \mathcal{M}(\vartheta): &A(z)y(t)=B(z)u(t-d)+e(t) \\
                            &y(t)-y(t)+A(z)y(t)=B(z)u(t-d)+e(t) \\
                            &y(t)=(1-A(z))y(t)+B(z)u(t-d)+e(t) \\
                            &\underbrace{\left(a_1z^{-1}+a_zz^{-2}+\dots\right)y(t) + \left(b_1+b_2z^{-1}+b_3z^{-2}+\dots\right)u(t-d)}_{\text{predictable at time } t} +e(t)
\end{align*}
The prediction for the predictable component can be expressed as:
\[\hat{\mathcal{M}}(\vartheta):\hat{y}(t|t-1)=a_1y(t-1)+a_2y(t-2)+\dots+b_1u(t-d)+b_2u(t-d-1)+\dots\]
From this, we can derive the regression vector:
\[\varphi(t)=\begin{bmatrix}
    y(t-1) & y(t-2) & \dots & y(t-m) & u(t-d) & u(t-d-1) & \dots
\end{bmatrix}^T\]
These values are obtained from past instances of the process. 
Consequently, we can rewrite the predictor as:
\[\hat{y}(t|t-1)=\vartheta^T\varphi(t)=\varphi(t)^T\vartheta\]
This formulation reveals the linear relationship with $\vartheta$.

Since $\hat{y}$ exhibits linearity with respect to $\vartheta$, $J_N(\vartheta)$ becomes quadratic in $\vartheta$. 
For optimizing the function $J_N(\vartheta)$, the following conditions are necessary:
\begin{itemize}
    \item $\hat{\vartheta}_N$ is a stationary point: $\frac{\partial J_N(\vartheta)}{\partial\vartheta}$ in $\vartheta=\hat{\vartheta}_N$ should equal zero.
    \item $\hat{\vartheta}_N$ is a minimum: $\frac{\partial^2 J_N(\vartheta)}{\partial\vartheta^2}$ in $\vartheta=\hat{\vartheta}_N$ must be greater than or equal to zero.
\end{itemize}

\paragraph*{First derivative}
The general expression for the derivative is:
\begin{align*}
    \frac{\partial J_N(\vartheta)}{\partial\vartheta}   &=\dfrac{d}{d\vartheta}\left[\dfrac{1}{N}\sum_{t=1}^N\left( y(t)-\varphi(t)^T\vartheta \right)^2\right] \\
                                                        &=\dfrac{1}{N}\sum_{t=1}^N\dfrac{d}{d\vartheta}\left[\left( y(t)-\varphi(t)^T\vartheta \right)^2\right] \\
                                                        &=\dfrac{1}{N}\sum_{t=1}^N 2\left( y(t)-\varphi(t)^T\vartheta \right)\dfrac{d}{d\vartheta}\left( y(t)-\varphi(t)^T\vartheta \right) \\
                                                        &=\dfrac{2}{N}\sum_{t=1}^N \left( y(t)-\varphi(t)^T\vartheta \right)\left(-\varphi(t)\right) \\
                                                        &=-\dfrac{2}{N}\sum_{t=1}^N \varphi(t) \left( y(t)-\varphi(t)^T\vartheta \right)
\end{align*}
Now, we impose:
\begin{align*}
    &-\dfrac{2}{N}\sum_{t=1}^N \varphi(t) \left( y(t)-\varphi(t)^T\hat{\vartheta}_N \right) = 0 \rightarrow \\
    &-\dfrac{2}{N}\sum_{t=1}^N \varphi(t)y(t) + \dfrac{2}{N}\sum_{t=1}^N \varphi(t)\varphi(t)^T\hat{\vartheta}_N = 0 \rightarrow \\ 
    &\hat{\vartheta}_N \sum_{t=1}^N \varphi(t)\varphi(t)^T = \sum_{t=1}^N \varphi(t)y(t)
\end{align*}
These are known as the least-squares normal equations, forming a system of $n_\vartheta$ linear equations in $n_\vartheta$ unknowns.

If $\sum_{t=1}^N \varphi(t)\varphi(t)^T$ is non-singular, we can express $\hat{\vartheta}_N$ as:
\[\hat{\vartheta}_N=\left( \sum_{t=1}^N \varphi(t)\varphi(t)^T \right)^{-1}\left( \sum_{t=1}^N \varphi(t)y(t) \right)\]
This represents the Ordinary Least Squares formula.

We encounter two scenarios:
\begin{itemize}
    \item If the Hessian is positive definite, $\sum_{t=1}^{N} \varphi(t)\varphi(t)^T$ is invertible and represents a well-shaped paraboloid.
    \item If the Hessian is singular, $\sum_{t=1} ^{N} \varphi(t)\varphi(t)^T$ is not invertible, and the ordinary least-squares formula may not hold.
        In this case, there are multiple solutions that are all equivalent due to a degenerate paraboloid. 
        Multiple global minima pose a problem because the data were generated by a single system. 
        Causes of multiple minima include:
        \begin{itemize}
            \item The data record may not adequately represent the underlying physical phenomenon.
            \item The selected model might be overly complex, leading to equivalent models describing the same phenomenon.
        \end{itemize}
\end{itemize}

\paragraph*{Second derivative}
The general expression for the second derivative is:
\[\frac{\partial^2 J_N(\vartheta)}{\partial\vartheta^2}=\dfrac{d}{d\vartheta}\left[\frac{\partial^2 J_N(\vartheta)}{\partial\vartheta^2}\right]=\dfrac{2}{N}\sum_{t=1} ^{N} \varphi(t)\varphi(t)^T\]
This matrix must be positive semidefinite.

\begin{definition}[\textit{Positive semi-definite matric}]
    A square matrix $M$ is positive semi-definite if for any nonzero vector $x$, we have $x^TMx \geq 0$.
\end{definition}
In our context, we require:
\[x^T\frac{\partial^2 J_N(\vartheta)}{\partial\vartheta^2}x=x^T\dfrac{2}{N}\sum_{t=1} ^{N} \varphi(t)\varphi(t)^T x=\dfrac{2}{N}\sum_{t=1} ^{N} \underbrace{\left(x^T\varphi(t)\right)}_{q(t)}  \underbrace{\left(\varphi(t)^Tx\right)}_{q(t)}=\dfrac{2}{N}\sum_{t=1}^{N} q(t)^2 \geq 0\]
Thus, the Hessian is always positive semidefinite.