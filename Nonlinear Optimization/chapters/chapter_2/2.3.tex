\section{Convex functions}

\begin{definition}[\textit{Convex function}]
    A function $f : C \rightarrow\mathbb{R}$ defined on a convex set $C \subseteq \mathbb{R}^n$ is convex if:
    \[f (\alpha \mathbf{x}_1 + (1-\alpha )\mathbf{x}_2) \leq \alpha f (\mathbf{x}_1) + (1-\alpha )f (\mathbf{x}_2) \qquad\forall \mathbf{x}_1,\mathbf{x}_2 \in C\quad\forall\alpha \in[0,1]\]
\end{definition}
\begin{definition}[\textit{Strictly proper function}]
    A function $f$ is strictly convex if the above inequality is strict for all distinct points $\mathbf{x}_1,\mathbf{x}_2 \in C$ and for any $\alpha\in(0,1)$:
    \[f (\alpha \mathbf{x}_1 + (1-\alpha )\mathbf{x}_2) < \alpha f (\mathbf{x}_1) + (1-\alpha )f (\mathbf{x}_2)\]
\end{definition}
\begin{definition}[\textit{Concave function}]
    A function $f$ is concave if $-f$ is convex, meaning:
    \[f (\alpha \mathbf{x}_1 + (1-\alpha )\mathbf{x}_2) \geq \alpha f (\mathbf{x}_1) + (1-\alpha )f (\mathbf{x}_2) \qquad\forall \mathbf{x}_1,\mathbf{x}_2 \in C\quad\forall\alpha \in[0,1]\]
\end{definition}
\begin{definition}[\textit{Linear function}]
    A function $f$ is linear if it is both convex and concave, which means:
        \[f (\alpha \mathbf{x}_1 + (1-\alpha )\mathbf{x}_2) = \alpha f (\mathbf{x}_1) + (1-\alpha )f (\mathbf{x}_2)\]
\end{definition}
\begin{definition}[\textit{Epigraph}]
    The epigraph of $f : S \subseteq \mathbb{R}^n \rightarrow \mathbb{R}$, denoted as $\text{epi}(f)$, is the set:
    \[\text{epi}(f) = \{(\mathbf{x},y) \in S \times \mathbb{R} \mid f(\mathbf{x}) \leq y\}\]
\end{definition}
\noindent It consists of all points lying on or above the graph of $f$.
\begin{definition}[\textit{Domain}]
    The domain of a convex function $f : C \rightarrow\mathbb{R}$ is given by:
    \[\text{dom}(f) = \{\mathbf{x} \in C \mid f (\mathbf{x}) <+\infty\}\]
\end{definition}

\subsection{Properties}
Let $C \subseteq \mathbb{R}^n$ be a nonempty convex set, and let $f : C \rightarrow\mathbb{R}$ be a convex function. 
\begin{property}
    For each $\beta \in\mathbb{R}$ (also $\beta\in + \infty$), the level sets
    \[L_{\beta} = \{\mathbf{x} \in C \mid f (\mathbf{x}) \leq \beta\} \qquad L_{\beta}^{+} = \{\mathbf{x} \in C \mid f (\mathbf{x}) <\beta\}\]
    Are convex subsets of $\mathbb{R}^n$.
\end{property}
\begin{property}
    A convex function $f$ is always continuous in the relative interior of its domain (relative to the affine hull of $C$). 
\end{property}
\begin{property}
    A function $f$ s convex if and only if its epigraph $\text{epi}(f)$ is a convex subset of $\mathbb{R}^{n+1}$. 
\end{property}

\subsection{Convex optimization}
Consider the optimization problem:
\[\min_{\mathbf{x}\in C} f(\mathbf{x})\] 
Here, $C \subseteq\mathbb{R}^n$ is convex and $f : C \rightarrow\mathbb{R}$ is a convex function.
\begin{proposition}
    If $C$ and $f$ are convex, then any local minimum of $f$ on $C$ is also a global minimum.
\end{proposition}
\begin{proof}
    Suppose $\mathbf{x}^\prime$ is a local minimum. 
    Assume for contradiction that there exists another point $\mathbf{x}^\ast\in C$ such that $f (\mathbf{x}^\ast) < f (\mathbf{x}^\prime)$.
    Since $f$ convex, for any $\alpha  \in (0, 1)$: 
    \[f (\alpha \mathbf{x}^\prime + (1 - \alpha )\mathbf{x}^\ast) \leq \alpha f (\mathbf{x}^\prime) + (1 - \alpha )f (\mathbf{x}^\ast) < f (\mathbf{x}^\prime)\]
    This contradicts the assumption that $\mathbf{x}^\prime$ is a local minimum.
    Hence, any local minimum must also be a global minimum.
\end{proof}
\begin{proposition}
    If $f$ is strictly convex on $C$, then there is at most one global minimum (provided $f$ is bounded below).
\end{proposition}
\begin{proof}
    Suppose $\mathbf{x}_1^\ast$ and $\mathbf{x}_2^\ast$ are two distinct global minima.
    Since $C$ is convex, their midpoint belongs to $C$: 
    \[\dfrac{1}{2}\left(\mathbf{x}_1^\ast+\mathbf{x}_2^\ast\right)\in C\]
    By strict convexity of $f$: 
    \[f\left(\dfrac{1}{2}\left(\mathbf{x}_1^\ast+\mathbf{x}_2^\ast\right)\right)<\dfrac{1}{2}f\left(\mathbf{x}_1^\ast\right)+\dfrac{1}{2}f\left(\mathbf{x}_2^\ast\right)\]
    But both $\mathbf{x}_1^\ast$ and $\mathbf{x}_2^\ast$ cannot be global minima.
\end{proof}
\begin{proposition}
    For a linear programming problem of the form:
    \[P = \{\mathbf{x}\in\mathbb{R}^n\mid\mathbf{Ax} \geq\mathbf{b}, \mathbf{x}\geq \mathbf{0} \} \neq\varnothing\] 
    Either there exists at least one optimal extreme point, or the objective function value is unbounded below over $P$.
\end{proposition}

\subsection{Gradient-based characterization}
\begin{proposition}
    Let $f : C \rightarrow\mathbb{R}$ be a continuously differentiable function ($C^1$) on a nonempty, convex, and open set $C \subseteq\mathbb{R}^n$. 
    Then, $f$ is convex if and only if:
    \[f(\mathbf{x}) \geq f (\mathbf{x}) +  \nabla ^T f (\mathbf{x})(\mathbf{x}-\overline{\mathbf{x}}) \qquad\forall \mathbf{x},\overline{\mathbf{x}} \in C\quad \mathbf{x}\neq\overline{\mathbf{x}}\]
\end{proposition}
\noindent This condition states that the function always lies above its first-order Taylor approximation, implying convexity.
\begin{proposition}
    Let $f : C \rightarrow\mathbb{R}$ be a continuously differentiable function ($C^1$) on a nonempty, convex, and open set $C \subseteq\mathbb{R}^n$. 
    Then, $f$ is strictly convex if and only if
    \[f(\mathbf{x}) > f (\mathbf{x}) +  \nabla ^T f (\mathbf{x})(\mathbf{x}-\overline{\mathbf{x}}) \qquad\forall \mathbf{x},\overline{\mathbf{x}} \in C\quad \mathbf{x}\neq\overline{\mathbf{x}}\]
\end{proposition}

\subsection{Hessian-based characterization}
\begin{proposition}
    Let $f : C \rightarrow\mathbb{R}$ be a twice continuously differentiable function ($C^2$) on a nonempty, convex, and open set $C \subseteq\mathbb{R}^n$. 
    Then, $f$  is convex if and only if its Hessian matrix is positive semidefinite for all $\mathbf{x}\in C$: 
    \[\nabla^2 f(\mathbf{x}) = \left( \frac{\partial^2f}{\partial x_i \partial x_j }\right)\succeq \mathbf{0}\qquad\forall\mathbf{x}\in C\]
\end{proposition}
\noindent This means that for all $\mathbf{y}\in\mathbb{R}^n$: 
\[\mathbf{y}^T\nabla^2f(\mathbf{x})\mathbf{y}\geq\mathbf{0}\]
\begin{proposition}
    Let $f : C \rightarrow\mathbb{R}$ be a twice continuously differentiable function ($C^2$) on a nonempty, convex, and open set $C \subseteq\mathbb{R}^n$. 
    Then, $f$  is convex if and only if its Hessian matrix is positive definite for all $\mathbf{x}\in C$: 
    \[\nabla^2 f(\mathbf{x}) = \left( \frac{\partial^2f}{\partial x_i \partial x_j }\right)\succ \mathbf{0}\qquad\forall\mathbf{x}\in C\]
\end{proposition}
\noindent This means that for all $\mathbf{y}\in\mathbb{R}^n$: 
\[\mathbf{y}^T\nabla^2f(\mathbf{x})\mathbf{y}>\mathbf{0}\qquad\forall\mathbf{y}\neq\mathbf{0}\]
This is a sufficient condition for strict convexity but not necessary.

\subsection{Subgradient of convex functions}
n general, convex (or concave) functions are not necessarily differentiable everywhere.
To handle nondifferentiable points, we extend the concept of the gradient for $C^1$ functions to piecewise $C^1$ functions through the notion of subgradients.

\begin{definition}[\textit{Subgradient}]
    Let $C \subseteq\mathbb{R}^n$ and $f : C \rightarrow\mathbb{R}$ be a convex function.
    A vector $\boldsymbol{\gamma}\in \mathbb{R}^n$  is called a subgradient of $f$ at $\mathbf{x}\in C$ if:
    \[f (\mathbf{x}) \geq f (\mathbf{x}) + \boldsymbol{\gamma}^T (\mathbf{x}-\overline{\mathbf{x}}) \qquad \forall \mathbf{x} \in C \]
\end{definition}
\begin{definition}[\textit{Subdifferential}]
    The subdifferential 0f $f$ at $\underline{\mathbf{x}}$, denoted as $\partial f (\mathbf{x})$, is the set of all subgradients of $f$ at $\mathbf{x}$: 
    \[\partial f(\mathbf{x})=\{\boldsymbol{\gamma}\in\mathbb{R}^n\mid f(\mathbf{x})\geq \boldsymbol{\gamma}^T(\mathbf{x}-\overline{\mathbf{x}}) \quad\forall\mathbf{x}\in C\}\]
\end{definition}
\noindent If $f$ is differentiable at $\overline{\mathbf{x}}$, then $\partial f(\overline{\mathbf{x}})$ contains only one element: the usual gradient $\nabla f(\overline{\mathbf{x}})$. 
If $f$ is not differentiable, then $\partial f(\overline{\mathbf{x}})$ is a set of possible slopes that support $f$ at $\overline{\mathbf{x}}$. 

Let $C \subseteq\mathbb{R}^n$ be convex, and let $f : C \rightarrow\mathbb{R}$ be a convex function.
\begin{property}
    For every interior point $\overline{\mathbf{x}}\in\text{int}(C)$, there exists at least one subgradient.
    In particular, there exists a vector $\boldsymbol{\gamma}\in\mathbb{R}^n$ such that the hyperplane:
    \[H= \{(\mathbf{x},y) \in\mathbb{R}^{n+1} \mid y = f (\overline{\mathbf{x}}) + \boldsymbol{\gamma}^T (\mathbf{x}-\overline{\mathbf{x}})\}\]
    is a supporting hyperplane of the epigraph of $f$ at $(\overline{\mathbf{x}},f (\overline{\mathbf{x}}))$.
\end{property}
\noindent The existence of at least one subgradient at every interior point of $C$  is a necessary and sufficient condition for $f$ to be convex on $\text{int}(C)$.
\begin{property}
    For every $\mathbf{x} \in C$, the subdifferential $\partial f (\mathbf{x})$ is a nonempty, convex, closed and bounded set.
\end{property}
\begin{property}
    A point $\mathbf{x}^\ast$ is a global minimum of $f$ on $C$ if and only if: 
    \[\mathbf{0} \in\partial f (\mathbf{x}^\ast)\]
\end{property}
\noindent That is, the zero vector must be included in the subdifferential at $\mathbf{x}^\ast$.