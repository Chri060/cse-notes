\section{Vectorial database}

Vector databases are gaining significant traction, driven by the rise of artificial intelligence and Machine Learning applications, particularly those leveraging large language models in combination with retrieval-augmented generation. 
These technologies enable more sophisticated data retrieval methods, providing powerful enhancements to search, recommendation, and discovery systems. 

By storing and processing data as high-dimensional vector representations, vector databases facilitate more efficient and context-aware data retrieval. 
Unlike traditional databases that rely on keyword-based search, vector databases allow AI systems to interpret the semantic similarity and context between data points. 
This capability is especially valuable for applications that require understanding the meaning and relationships between large datasets, such as personalized search, content recommendations, and natural language understanding.

\paragraph*{Vectors}
A vector database organizes data as high-dimensional vector representations, which allows for contextual search and discovery that goes beyond simple keyword matching. 
Vectors are grouped according to their semantic similarity, meaning the database can retrieve data based on the inherent meaning or context, rather than exact matches. 
This makes data retrieval more efficient and meaningful, particularly for tasks that involve understanding the relationship between data points.

In the fields of mathematics and physics, a vector is a quantity characterized by both magnitude and direction. 
Similarly, in vector databases, vectors represent various attributes or features of an object, capturing key characteristics in a structured, multidimensional space. 
These vector representations form the foundation of information processing in many AI and Machine Learning applications, enabling machines to understand and interpret complex data patterns.

One common technique used to create these vectors is embedding, which transforms data into vector format. 
For textual data, pre-trained models map words and phrases into a multidimensional space, effectively capturing their meanings and relationships in context. 
This process allows vector databases to find and retrieve relevant information based on vector similarity, rather than relying on exact text matches, offering more nuanced and context-aware search results.

\paragraph*{Similarity}
To measure the similarity between vectors, metrics like Cosine similarity and Euclidean distance are commonly employed.
Cosine similarity is particularly popular due to its computational efficiency and ability to capture the orientation of vectors in space, making it ideal for comparing the semantic similarity between high-dimensional vectors. 
These similarity measures enable the database to determine how closely related two data points are, helping AI systems retrieve the most contextually relevant information based on the vector representations.

\paragraph*{Vectors matching}
Vector databases excel not only in efficiently storing high-dimensional vectors but also in enabling fast and accurate matching of vectors. 
Searching for nearest neighbors in an unindexed database involves calculating the distance from the query vector to each point in the dataset, which can be highly computationally expensive for large datasets.
Therefore, using indexing techniques to speed up this process is essential for efficient vector matching.

\paragraph*{Tree-based indexing}
One common approach to creating efficient vector indexes is through tree-based techniques, which organize data into structures that optimize search operations. 
Below are two popular tree-based indexing methods used for vector matching:
\begin{itemize}
    \item \textit{K-dimensional trees}: a type of binary space partitioning tree. 
        In this structure, each node represents a $k$-dimensional point. 
        The non-leaf nodes define splitting hyperplanes that partition the space into two regions. 
        Each split is based on the median value of a specific dimension. 
        The primary advantage of $K$-dimensional trees is that they reduce the search space by narrowing down potential matches at each level of the tree.
        The time complexity for search operations in K-dimensional trees is $\mathcal{O}(\log n)$, where $n$ is the number of objects in the dataset. 
        However, this method becomes less efficient as the dimensionality of the data increases. 
        With high-dimensional vectors, the tree becomes less effective, and the performance may degrade to a level similar to exhaustive search, as most of the points in the tree will need to be evaluated. 
    \item \textit{R-trees}: this indexing structure groups nearby objects into bounding rectangles, which are then represented at higher levels of the tree. 
        The key idea is that if a query does not intersect a bounding rectangle at a certain level, it will not intersect any of the objects contained within it. 
        This enables the search process to eliminate large portions of the dataset without checking each individual vector. 
        At the leaf level, each rectangle corresponds to a single object, while higher levels aggregate multiple objects into larger rectangles, forming a coarse approximation of the dataset.
        R-trees are balanced binary trees, meaning that the leaf nodes are always at the same depth. 
        The tree is structured in pages, and each page has a minimum and maximum fill limit, which optimizes storage and retrieval.
        A key challenge in R-tree construction is ensuring that the tree is well-balanced, that the bounding rectangles do not cover too much empty space, and that there is minimal overlap between them. 
        This reduces the number of subtrees that need to be examined during a search, improving search efficiency.
\end{itemize}

\paragraph*{Approximate indexing}
For high-dimensional data, exact search techniques often perform poorly due to the curse of dimensionality.
To address this challenge, vector databases use approximate indexing:
\begin{itemize}
    \item \textit{Locality Sensitive Hashing}: hashes similar items into the same buckets. 
    \item \textit{Hierarchical Navigable Small World}: organizes data points into a hierarchical graph where nodes represent vectors. 
        The search begins at the top layer, which contains a few representative nodes, and moves downward through more detailed layers, progressively refining the result. 
    \item \textit{Inverted File with Product Quantization}: organizes the dataset using an inverted index, a structure that maps data points to representative clusters. 
        Each vector is assigned to the nearest cluster center. 
        Product quantization compresses the vectors within each cluster by dividing them into smaller sub-vectors. 
        However, the reconstructed vectors differ slightly from the originals, introducing a small loss in accuracy.
\end{itemize}

\subsection{Milvus}
Milvus, developed by Zilliz, is an open-source vector database designed to handle large-scale, high-dimensional data for similarity search and unstructured data retrieval. 
Milvus is optimized for scalability, offering performance that can range from small projects to large, enterprise-level deployments. 
The architecture of Milvus is built around several key principles, including disaggregated storage and computation, microservices, and separation of streaming and historical data. 

One of Milvus' standout features is its disaggregated architecture, which uses separate components for storage and computation, making it adaptable to various deployment environments. 
Milvus also relies on a logging mechanism called Log As Data, ensuring that all data changes are tracked in real-time, facilitating efficient updates and retrieval.

Milvus uses a shard-based system. 
Each shard is overseen by a supervisor, known as the shard leader, who ensures that new data is added, stored safely in object storage, and kept up to date for search requests. 
When historical data is needed, the supervisor forwards the request to other query nodes, ensuring seamless access to both current and past information.

Milvus uses two types of data segments to manage the flow of information. 
The Growing Segment handles in-memory data, replaying information from the Log Broker, and ensures that data is fresh and appendable through a flat indexing method.
Meanwhile, the Sealed Segment is immutable and uses alternative indexing methods for optimized search and retrieval, making it more efficient for large-scale data storage.

To maintain data integrity, Milvus utilizes a Write-Ahead Log, which temporarily holds new data before it is fully integrated into the system. 
This log functions like a to-do list for the database, ensuring that all new data is processed and recorded efficiently before being committed to the permanent storage.