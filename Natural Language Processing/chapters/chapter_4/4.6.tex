\section{Multi modal models}

Aligning the embedding spaces is crucial for multi-modal models. 
Embeddings can be generated not only for text.
By employing contrastive learning, we can force these two distinct embedding spaces to align with each other. 
The process involves taking a collection of image-text pairs, and training a classifier on batches of these pairs.
Specifically, the classifier learns to identify which piece of text corresponds to which image and vice versa. 
Aligning the embedding space in this manner enables powerful applications like semantic image search using text queries.

\subsection{Multi-task learning}
Language models are highly versatile and have been adapted for multi-task learning. 
Research has shown that models trained on multiple tasks often outperform those specialized for a single task. 
Some approaches even aim to learn the optimal prompt for each specific task. 
In the context of multi-modal learning, the transformer architecture demonstrates remarkable flexibility. 
It is relatively straightforward to extend text-to-text models to handle multi-modal settings, where both text and images are processed together. 
This capability facilitates the learning of tasks across different types of media, enabling more comprehensive and integrated models.