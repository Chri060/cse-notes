\section{Neural Networks}

The human brain is made up of tens of billions of neurons, forming a massive network where each neuron is connected to thousands of others. 
Each neuron acts as a simple processing unit: it receives electrochemical signals from other neurons and sends an output signal if the sum of inputs exceeds a certain threshold. 
The brain learns complex tasks by organizing neurons in specific patterns.

Artificial Neural Networks aim to mimic this behavior using artificial neurons. 
These neurons are modeled as simple step functions, where they weight inputs and compare the sum to a threshold. 
The output is then calculated using a non-linear activation function.

\subsection{Configuration}
To create different neural network models, several factors can be adjusted:
\begin{itemize}
    \item \textit{Topology}: the structure of the network, which defines how neurons are connected.
    \item \textit{Activation function}: the mathematical function used to transform the input to the neuron.
    \item \textit{Loss function}: the function that the network tries to minimize, such as binary cross-entropy for classification tasks.
\end{itemize}
\noindent Neural networks are composed of neurons arranged in layers:
\begin{itemize}
    \item \textit{Input layer}: receives the initial data.
    \item \textit{Hidden layers}: intermediate layers that process information and allow the network to model complex relationships.
    \item \textit{Output layer}: produces the final prediction.
\end{itemize}
\noindent Each connection between neurons has a weight, and each neuron has a bias (threshold), which determines whether it activates.

\subsection{Training}
Neural networks learn through a process called backpropagation, which optimizes the network's parameters. 
The steps are as follows: 
\begin{itemize}
    \item \textit{Initialization}: weights and biases are initialized randomly.
    \item \textit{Backpropagation}: perform the following: 
        \begin{enumerate}
            \item \textit{Forward pass}: the input data is passed through the network to make a prediction.
            \item \textit{Backward pass}: the parameters (weights and biases) are adjusted to reduce the error:
                Connections that lead to correct predictions are rewarded.
                Connections that contribute to incorrect predictions are penalized.
        \end{enumerate}
\end{itemize}
\noindent This process is essentially gradient descent, where the network seeks to minimize the loss function by computing derivatives using the chain rule from calculus.

\subsection{Properties}
Neural networks are incredibly powerful, with some key properties:
\begin{itemize}
    \item A network with just one hidden layer can, in theory, learn any function if the layer is large enough.
    \item There is a wide variety of network structures to explore.
    \item Many hyperparameters need to be tuned, such as network architecture and the choice of activation function.
\end{itemize}
Training neural networks can be computationally intensive compared to other algorithms like decision trees or linear classifiers. 
However, advancements in hardware and software optimizations have made it increasingly feasible to train large, complex networks.