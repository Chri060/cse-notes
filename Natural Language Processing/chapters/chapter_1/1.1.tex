\section{Natural language}

The origins of spoken language are widely debated. 
Estimates range from as early as 2.5 million years ago to as recent as 60,000 years ago, depending on how one defines human language.

The development of written language, however, is more clearly documented. 
The first known writing systems emerged in Mesopotamia (modern-day Iraq) around 3500 BCE. 
Initially, these were simple pictograms representing objects, but over time, they evolved into abstract symbols representing sounds, paving the way for more complex communication.

The characteristics of human language are as follows: 
\begin{itemize}
    \item \textit{Compositional}: language allows us to form sentences with subjects, verbs, and objects, providing an infinite capacity for expressing new ideas.
    \item \textit{Referential}: we can describe objects, their locations, and their actions with precision.
    \item \textit{Temporal}: language enables us to convey time, distinguishing between past, present, and future events.
    \item \textit{Diverse}: thousands of languages are spoken worldwide, each with unique structures and expressions.
\end{itemize}

\subsection{Natural Language Processing}
One reason to care about Natural Language Processing is the sheer volume of human knowledge now available in machine-readable text. 
With the rise of conversational agents, human-computer interactions increasingly rely on language understanding. 
Furthermore, much of our daily communication is now mediated by digital platforms, making Natural Language Processing more relevant than ever.

However, Natural Language Processing is a challenging field. 
Human language is highly expressive, allowing people to articulate virtually anythingâ€”including ambiguous or nonsensical statements. 
Resolving this ambiguity is one of the core difficulties in computational linguistics. 
Moreover, meaning can be influenced by pronunciation, emphasis, and context, making interpretation even more complex. 
Fortunately, language is often redundant, allowing for error correction and inference even when mistakes occur.


\subsection{History}
The field of Natural Language Processing has its roots in linguistics, computer science, speech recognition, and psychology. 
Over time, it has evolved through various paradigms, driven by advancements in formal language theory, probabilistic models, and Machine Learning.

During World War II, early work in Natural Language Processing was influenced by information theory, probabilistic algorithms for speech, and the development of finite state automata.

Between 1957 and 1970, two primary approaches emerged. 
The symbolic approach, based on formal language theory and AI logic theories, focused on rule-based processing. 
Meanwhile, the stochastic approach leveraged Bayesian methods, leading to the development of early Optical Character Recognition systems.

From 1970 to 1993, the focus shifted toward empirical methods and finite-state models. 
Researchers worked on understanding semantics, discourse modeling, and structural analysis of language. 

By the mid-1990s, symbolic approaches began to decline, and the late 1990s saw a surge in data-driven methods, fueled by the rise of the internet and new application areas.

The 2000s marked a deep integration of Machine Learning into Natural Language Processing. 
The increasing availability of annotated datasets, collaboration with Machine Learning and high-performance computing communities, and the rise of unsupervised systems solidified empiricism as the dominant paradigm.

From 2010 to 2018, Machine Learning became ubiquitous in Natural Language Processing, with Neural Networks driving major advances in conversational agents, sentiment analysis, and language understanding.

Since 2018, the field has been revolutionized by Transformer architectures. 
Pretrained language models, such as BERT and GPT, have enabled transfer learning at an unprecedented scale, leading to the rise of massive online language models.

\begin{chronology}[10]{1939}{2026}{0.9\textwidth}
    \event[1940]{1950}{Formal language theory}
    \event[1957]{1970}{Stochastic and symbolic paradigms}
    \event[1970]{1993}{Finite state models}
    \event[1994]{1999}{Symbolic paradigm decline}
    \event[2000]{2010}{Machine Learning}
    \event[2010]{2018}{Deep Learning}
    \event[2018]{2025}{Transformers}
\end{chronology}