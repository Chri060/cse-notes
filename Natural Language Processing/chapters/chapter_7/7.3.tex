\section{Retrieval Augmented Generative models}

Building a high-performing RAG system involves a series of nuanced design decisions that significantly affect system accuracy, latency, and scalability.
Each of these decisions involves trade-offs between accuracy, retrieval speed, memory consumption, and interpretability.

Evaluating RAG performance is challenging due to the inherently generative nature of the output. 
Traditional metrics may not suffice, especially when the exact match is unreliable. 
Generated outputs may use different wording than ground-truth answers while still conveying the same meaning. 
To address this, researchers often employ LLM-based evaluation strategies:
\begin{itemize}
    \item \textit{LLM-as-a-Judge}: a single LLM is tasked with comparing the generated response against the expected answer to determine semantic equivalence.
    \item \textit{LLM-as-a-Jury}: multiple LLMs independently assess the similarity between the generated output and the reference, then vote on agreement. This ensemble approach increases robustness and mitigates individual model bias.
\end{itemize}