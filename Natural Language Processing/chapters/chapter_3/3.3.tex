\section{Text normalization}

When ranking documents, it's important to normalize for document length. 
Longer documents tend to have a larger vocabulary, which makes it more likely they will contain the query terms.
However, this doesn't necessarily mean they are more relevant to the user's search. In fact, shorter documents with the same term count should often be preferred.

\subsection{Document length normalization}
One simple way to normalize for document length is to divide the term frequency by the document length. 
However, the most common method of normalization uses the L2 norm (also called the Euclidean norm) instead of the L1 norm (which is simply dividing by the document length).

\subsection{Cosine similarity normalization}
In the Vector Space Model, each document is represented as a vector of term frequencies weighted by their inverse document frequency. 
The vector for a document might look like this:
\[\mathbf{d}=(\text{tf}_{1,d}-\text{idf}_1,\dots,\text{tf}_{1,d}-\text{idf}_n)\]
\noindent To compute the similarity between a query and a document, we measure the cosine of the angle between their vectors. 
The cosine similarity formula is:
\[\text{similarity}(\mathbf{d}_1,\mathbf{d}_2)=\dfrac{\mathbf{d}_1\cdot\mathbf{d}_2}{\left\lVert \mathbf{d}_1\right\rVert\left\lVert \mathbf{d}_2\right\rVert  }\]
Here $\mathbf{d}_1$ represents the query vector, and $\mathbf{d}_2$ represents the document vector.
The cosine of the angle is used because it produces a similarity value in the range $[0,1]$. 
To calculate the cosine similarity, the vectors are normalized by their Euclidean (L2) norm, rather than the length of the document in terms of tokens (which would be an L1 norm).

\subsection{Alternative length normalization}
There have been many studies into alternative methods of length normalization. 

\paragraph*{Pivoted Length Normalization}
One notable method is Pivoted Length Normalization, which aims to retain the beneficial information from longer documents while preventing them from being unfairly favored.
The idea behind Pivoted Length Normalization is that longer documents generally contain more information, but simple length normalization could lose valuable length information. 
Instead, PLN adjusts the normalization to account for both the document length and the average document length in the corpus:
\[\dfrac{\text{tf}_{t,d}}{L_d}\rightarrow\dfrac{\text{tf}_{t,d}}{bL_d+(1-b)L_{\text{avg}}}\]
Here, $L_d=\sum_{t}\text{tf}_{t,d}$ is the length of the document, $L_{\text{avg}}=\dfrac{1}{N}\sum_dL_d$ is the average document length across the corpus, and $b$ is a parameter that controls the balance between the document length and the average length, with $0\leq b \leq 1$. 

\paragraph*{Best March 25}
Another widely used length normalization method is BM25 (Best Match 25), a ranking function that builds upon the ideas of TF-IDF and length normalization. 
The formula for BM25 is as follows:
\[\text{RSV}_d=\sum_{t \in q}\log\dfrac{N}{\text{df}_t}\dfrac{(k_1+1)\text{tf}_{t,d}}{k_1\left((1-b)+b\cdot\frac{L_d}{L_{\text{avg}}}\right)+\text{tf}_{t,d}}\]
Here, $k_1$ and $b$ aare parameters that control how much weight is given to term frequency and document length, $N$ is the total number of documents in the corpus, $\text{df}_t$ is the document frequency of term, $\text{tf}_{t,d}$ is the term frequency of $t$ in document $d$, and $L_d$ is the length of the document.
Some of the reasons for its lasting effectiveness include:
\begin{itemize}
    \item \textit{Asymptotic term importance}: the influence of a term on the document's score decreases as its frequency increases. 
        This means documents with extremely high term frequency won't always be ranked at the top.
    \item \textit{Parameter control}: the $k_1$ and $b$ parameters allow fine-tuning based on the corpus, and default values typically work well. 
        However, these parameters can be improved through validation on a specific dataset.
\end{itemize}