\section{Supervised Learning}

Machine Learning involves techniques that help machines become more intelligent by learning from past data to predict future outcomes.
\begin{definition}[\textit{Machine Learning}]
    A computer program is considered to learn from experience $E$ when it improves its performance on a specific task $T$ based on that experience, as measured by a performance metric $P$.
\end{definition}

In supervised learning, each training example is represented as a vector in a feature space. 
These training examples are labeled with the correct class. 
The task is to divide the feature space in such a way that the model can make accurate predictions for new, unseen data points.

In practice, however, data is rarely perfectly clean or neatly separable. Often, different classes of data overlap, meaning they may not be linearly separable. 
Additionally, instances are described by many features, and not all features are equally useful for distinguishing between classes. 
Some features might provide more meaningful information, while others might be less relevant.

To address this, classifiers divide the feature space into regions, and the boundaries between these regions can either be linear or non-linear. 
Linear models use simple decision boundaries to separate classes. 
On the other hand, non-linear models are capable of creating more complex decision boundaries to better fit the data.

\paragraph*{Training}
The process of training a model involves finding a formula that can predict the correct labels for new instances. 
The learning algorithm takes in the training data and their corresponding labels, and then searches for the best parameters for the model. 
These parameters are adjusted in order to minimize prediction loss on the training data.
The learning algorithm operates based on its own settings, called hyperparameters, which control aspects of the learning process.

\paragraph*{Hyperparameters}
Hyperparameters play a crucial role in determining the model's behavior. 
These are parameters that govern the learning algorithm itself, and they can influence the complexity of the model. 

\subsection{Overfitting}
Overfitting is a common challenge in Machine Learning. 
As the model becomes more complex, the error on the training data tends to decrease. 
However, at some point, the model may begin to memorize the training data rather than learning generalizable patterns, leading to poor performance on unseen data. 
This is known as overfitting. 
The goal is to find the model that strikes the right balance, one that generalizes well to new, unseen data, rather than simply fitting the training data perfectly.

To prevent overfitting, we need to carefully select hyperparameters that control the model's complexity.
Since the training data alone doesn't tell us how well the model will generalize, and the test data should be reserved for final evaluation, we use a separate validation dataset. 
This dataset is a portion of the training data held out during the training process. 
The model is trained multiple times with different hyperparameter settings, and its performance is evaluated on the validation set. 
By comparing how well each configuration generalizes, we can choose the hyperparameters that lead to the best performance.