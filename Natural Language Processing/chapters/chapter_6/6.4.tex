\section{Dialog systems evaluation}

Evaluating dialogue systems remains a complex and evolving challenge. 
Unlike task-specific models, open-domain conversational agents must be assessed across multiple dimensions of quality—many of which are inherently subjective and difficult to measure automatically.
Eight commonly considered dimensions of quality in dialogue include: repetition, interestingness, coherence, fluency, listening, inquisitiveness, humanness, and engagingness. 

\subsection{Human evaluation}
Human assessments are essential for gauging subjective aspects of quality. 
These evaluations are typically divided into two formats:
\begin{itemize}
    \item \textit{Participant evaluation}: users interact with the system and then answer targeted questions.
    \item \textit{Observers evaluation}: annotators compare pairs of conversations and assess them along qualitative axes. 
\end{itemize}
\noindent These comparative judgments often provide more reliable insight than absolute scoring.

\subsection{Automatic evaluation}
Automatic evaluation of dialogue systems remains an open research problem.
Traditional metrics from fields like machine translation are rarely used in conversational AI because they show low correlation with human judgments, and fail to capture contextual appropriateness, tone, or relevance in conversation.
The main alternative approaches are: 
\begin{itemize}
    \item \textit{Adversarial evaluation}: inspired by the Turing Test, this method trains a classifier to distinguish between human and machine responses.
         The more the model can fool the classifier, the better its perceived quality.
    \item \textit{LLM-as-a-judge}: these models are prompted to score or rank conversations based on quality dimensions—offering scalable, semi-automated evaluation with surprisingly strong alignment to human preferences.
\end{itemize}
\noindent For systems built to accomplish specific tasks evaluation focuses more on functional success such as: end-to-end task success, slot error rate, user studies, efficiency, and system robustness.