\section{Validation}

Underfitting occurs when the model is too simplistic to capture the underlying patterns in the data, leading to poor performance. 
To address this, we can increase the complexity of the model. 
However, increasing complexity too much can result in overfitting. 
In this case, the model starts to fit the noise in the training data rather than the true underlying patterns, which reduces its ability to generalize to new, unseen data.

The goal during training is to find a model that strikes a balance between complexity and generalization. 
The optimal model should perform well not just on the training data but also on future data that it has not seen before.

The loss on the training data alone is not a reliable indicator of future performance, as it tends to be overly optimistic. 
The training data can differ from new, unseen data, so it's crucial to evaluate the model on a separate test set to get a more accurate estimate of its generalization capability.
\noindent To assess the model's ability to generalize, the dataset is typically divided into three distinct subsets:
\begin{itemize}
    \item \textit{Training set}: used to train the model and adjust its parameters.
    \item \textit{Validation set}: used to fine-tune the model's hyperparameters and select the best model configuration.
    \item \textit{Test set}: used for the final evaluation of the model's performance on unseen data.
\end{itemize}

\subsection{Cross validation}
Cross validation s a technique that helps estimate a model's performance by using the available training data for both training and error estimation on unseen data. 
This method allows us to evaluate the model's ability to generalize without the need for a separate test set in the initial stages of model development.
Various cross validation techniques include:
\begin{itemize}
    \item \textit{Hold-out validation}: when a large dataset is available, it is split into distinct subsets for training and validation. 
    \item \textit{Leave-one-out cross validation}: the model is trained on all data points except one, which is then used for validation. 
        This process is repeated for each data point in the dataset, ensuring that each instance is used for both training and validation. 
        While this method provides an unbiased estimate of performance, it can be computationally expensive, especially with large datasets.
    \item \textit{K-fold cross-validation}: the dataset is randomly split into $K$ equally sized subsets (folds).
        The model is then trained on $K-1$ folds and validated on the remaining fold.
        This process is repeated for all folds, with each fold serving as the validation set once.
        The final model performance is averaged across all $K$ iterations. 
\end{itemize}

\subsection{Early stopping}
Overfitting in Neural Networks often manifests as a consistently decreasing training error as the number of gradient descent iterations increases. 
While the model continues to improve on the training data, its ability to generalize to unseen data may start to degrade after a certain point. 
Early stopping is a regularization technique aimed at preventing overfitting by stopping the training process once the model's performance on a validation set begins to decline.

\noindent In early stopping we start by holding out a validation set, nad then while we train on the training set we cross validate and if the validation error starts to increase while the training error continues to decrease, halt the training process. 

Early stopping can be applied to model selection and evaluation, especially when fine-tuning hyperparameters. 
These hyperparameters, such as learning rate or network architecture, significantly influence the model's capacity and ability to generalize.

\subsection{Weight decay}
Regularization techniques aim to reduce overfitting by constraining the model's complexity based on prior assumptions about the data. 
In the context of Neural Networks, weight decay is one of the most widely used regularization methods. It helps control model complexity by penalizing large weights, which are often associated with overfitting.

In traditional training, we typically maximize the likelihood of the data given the model parameters. 
However, to impose additional structure and reduce complexity, we can take a Bayesian approach by incorporating a prior distribution over the weights $\mathbf{w}$, leading to a Maximum A Posteriori estimation:
\[ \mathbf{w}_{\text{MAP}}=\argmax_{\mathbf{w}}\Pr(\mathcal{D}\mid\mathbf{w})\Pr(\mathbf{w})\]
\noindent In practice, small weights tend to improve the generalization ability of Neural Networks. 
This can be modeled by assuming a Gaussian prior on the weights, which encourages the model to prefer smaller weights. 
Incorporating this prior into the optimization problem results in the following objective function:
\[\hat{\mathbf{w}}=\argmin_{\mathbf{w}}\sum_{n=1}^N(t_n-g(\mathbf{x}_n\mid\mathbf{w})^2)+\gamma{\left\lVert \mathbf{w}\right\rVert}_2^2 \]
\noindent Here, $\gamma$ is the regularization parameter that controls the strength of the weight decay.
Larger values of $\gamma$ penalize large weights more heavily, promoting smaller weights and thereby reducing the model's complexity.
By controlling $\gamma$, we can find a balance between fitting the data well and maintaining a model that generalizes effectively. 

\subsection{Dropout}
Dropout is a stochastic regularization technique designed to mitigate overfitting in Neural Networks by randomly deactivating certain neurons during training. 
This forces the model to learn more robust and independent feature representations, preventing neurons from becoming too dependent on one another (coadaptation). 
By randomly dropping out neurons, the model is encouraged to generalize better by relying on multiple paths for information flow.
\noindent The process of applying dropout during training involves applying a mask to deactivate neurons in each layer with a certain probability, iteratively. 
Once training is complete, all neurons are reactivated for the full network during testing. 
The behavior of the full network can be thought of as the averaged result of all the sub-networks trained during the dropout process. 
To compensate for the dropouts during training, weight scaling is applied at test time, effectively averaging the outputs of all possible sub-networks.