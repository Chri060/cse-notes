\section{Activation functions}

Activation functions are a fundamental component of Neural Networks, introducing the non-linearity necessary for the model to capture complex patterns in data.
They determine the output of each neuron and influence the network's ability to learn.
\renewcommand*{\arraystretch}{2}
\begin{table}[H]
    \centering
    \begin{tabular}{|l|l|l|}
    \hline
    \textbf{Name}      & \textbf{Function}  \\ \hline
    Linear             &  $g(a)=a$                 \\ \hline
    Sigmoid            &  $g(a)=\dfrac{1}{1+e^{-a}}$            \\ \hline
    Hyperbolic tangent & $g(a)=\frac{e^a-e^{-a}}{e^a+e^{-a}}$                  \\ \hline
    Rectified Linear Unit & $g(a)=\begin{cases} 0 \qquad \text{if }a<0 \\ a \qquad \text{otherwise}\end{cases}$  \\ \hline
    Leaky Rectified Linear Unit & $g(a)=\begin{cases} 0.01a \qquad \text{if }a<0 \\ a \qquad\qquad \text{otherwise}\end{cases}$  \\ \hline
    Exponential Linear Unit & $g(a)=\begin{cases} \alpha(e^a-1) \qquad \text{if }a<0 \\ a \qquad\qquad\quad\:\: \text{otherwise}\end{cases}$  \\ \hline
    \end{tabular}
    \caption{Most used activation functions}
\end{table}
\renewcommand*{\arraystretch}{1}
\noindent The features of each activation function are the following:
\begin{itemize}
    \item \textit{Sigmoid}: often used in output layers to model probabilities due to its range $(0, 1)$ and simplicity. 
        However, it suffers from saturation, leading to small gradients.
    \item \textit{Hyperbolic tangent}: preferred in hidden layers as it outputs values in $(-1, 1)$, centering data and often resulting in faster convergence than sigmoid.
    \item \textit{ReLU}: popular for its simplicity and efficiency, it mitigates the vanishing gradient problem but is susceptible to dying neurons, where certain neurons become inactive.
    \item \textit{Leaky ReLU} and \textit{ELU}: variants of ReLU designed to address the dying neuron issue by allowing small gradients for negative inputs.
\end{itemize}

\paragraph*{Vanishing gradient}
Functions like sigmoid and hyperbolic tangent to saturate for extreme input values, producing gradients close to zero. 
Since backpropagation relies on gradient updates, this can cause gradients to vanish as they propagate through the layers.
This issue significantly hinders the training of deep networks.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{images/ReLU.png}
    \caption{Rectified Linear Unit}
\end{figure}
\noindent ReLU has revolutionized Deep Learning by addressing the vanishing gradient problem and enabling faster and sparser activations.
Its advantages include also the scale invariance with respect to output scaling.
\noindent However, it has limitations:
\begin{itemize}
    \item Non-zero-centered outputs, which can slow optimization.
    \item Non-differentiability at zero, although this is often addressed by practical implementations.
    \item Dying neurons, which occur when gradients for negative inputs become zero, effectively disabling these neurons during training.
\end{itemize}

\paragraph*{Output layers}
The choice of activation function in the output layer depends on the problem type:
\begin{itemize}
    \item \textit{Regression}: linear activation.
    \item \textit{Binary classification}: hyperbolic tangent or sigmoid. 
    \item \textit{Multi-class classification}: softmax, defined as:
        \[y_k=\dfrac{e^{z_k}}{\sum_ke^{z_k}}\]
        Here, $z_k$ is the activation value of the $k$-th output neuron, and the softmax function produces a probability distribution across classes.
\end{itemize}

\paragraph*{Hidden layers}
Hidden layers benefit from activation functions that introduce non-linearity, enabling the network to learn complex relationships. Popular choices include:
\begin{itemize}
    \item \textit{Sigmoid} and \textit{hyperbolic tangent}: useful for smaller networks but prone to vanishing gradients in deeper architectures.
    \item \textit{ReLU} and its variants: often preferred for Deep Learning due to their computational efficiency and ability to handle vanishing gradients.
\end{itemize}

\subsection{Function approximation}
\begin{theorem}
    A single hidden layer Feed Forward Neural Network with $S$-shaped activation functions can approximate any measurable function to any desired degree of accuracy on a compact set.
\end{theorem}
\noindent This universal approximation theorem highlights the expressive power of Neural Network. 
However, practical challenges such as optimization, generalization, and the need for sufficient hidden units often require careful design choices.
For classification tasks, a single additional hidden layer is typically sufficient to achieve good performance.