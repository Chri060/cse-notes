\section{Training}

Training a neural network involves learning a set of parameters, primarily weights $\mathbf{w}$ to approximate the target values $t_n$ as accurately as possible.
Given a training dataset $\mathcal{D}=\left\{\left\langle x_1,t_1 \right\rangle,\dots,\left\langle x_N,t_N\right\rangle\right\}$, the goal is to determine parameters such that, for new data, the model's predictions $y_n(x_n|\theta)$ are close to the true target values $t_n$.
In essence, the training process seeks parameters that generalize well, ensuring $g(x_n\mid\mathbf{w})\approx t_n$ even for unseen inputs.

In Neural Networks, the discrepancy between predictions and targets is often quantified using the Sum of Squared Errors:
\[\mathcal{L}(\mathbf{w})=\sum_n^N\left(t_n-g(x_n\mid\mathbf{w})\right)^2\]
This loss function is nonlinear with respect to the weights, making the optimization process more challenging than for simpler linear models.

\paragraph*{Nonlinear optimization}
To minimize the error $\mathcal{L}(\mathbf{w})$, we use optimization techniques to adjust the weights. 
The aim is to find $\mathbf{w}$ that satisfies:
\[\dfrac{\partial \mathcal{L}(\mathbf{w})}{\partial \mathbf{w}}=0\]
For Neural Networks, solving this equation directly is infeasible due to the complexity and nonlinearity of the error surface. 
Instead, we employ iterative methods such as gradient descent, which progressively updates the weights in the direction that reduces the loss.

\subsection{Gradient descent}
Gradient descent updates the weights using the formula:
\[\mathbf{w}^{k+1}=\mathbf{w}^k-\eta\dfrac{\partial \mathcal{L}(\mathbf{w})}{\partial \mathbf{w}}\Bigg|_{\mathbf{w}^k}\]
Here, $\eta$ is the learning rate, controlling the step size in each iteration. 

Gradient descent may struggle with local minima or oscillations in the error surface. 
To mitigate this, a momentum term is often added:
\[\mathbf{w}^{k+1}=\mathbf{w}^k-\eta\dfrac{\partial \mathcal{L}(\mathbf{w})}{\partial \mathbf{w}}\Bigg|_{\mathbf{w}^k}-\alpha\dfrac{\partial \mathcal{L}(\mathbf{w})}{\partial \mathbf{w}}\Bigg|_{\mathbf{w}^k}\]
Here, $\alpha$ is the momentum coefficient, which determines how much past gradients influence the current update. 

\paragraph*{Nestorov accelerated gradient}
The Nesterov accelerated gradient method builds upon the momentum technique to improve optimization performance. 
It achieves this by first making a momentum-based prediction and then refining the update using the gradient evaluated at the predicted position. 
This two-step process helps anticipate the trajectory of the optimization, resulting in faster convergence.

\noindent The update process is as follows:
\begin{enumerate}
    \item \textit{Momentum-based prediction}: the algorithm predicts the next position using the momentum term:    
        \[\mathbf{w}^{k+\frac{1}{2}}=\mathbf{w}^k-\alpha\dfrac{\partial E(\mathbf{w})}{\partial \mathbf{w}}\Bigg|_{\mathbf{w}^{k-1}}\]
    \item \textit{Gradient-based correction}: the weights are then updated by evaluating the gradient at the predicted position:
        \[\mathbf{w}^{k+1}=\mathbf{w}^k-\eta\dfrac{\partial E(\mathbf{w})}{\partial \mathbf{w}}\Bigg|_{\mathbf{w}^{k+\frac{1}{2}}}\]
\end{enumerate}

\paragraph*{Multiple restarts}
For non-convex error surfaces, the optimization process can converge to local minima depending on the initial weights. 
To improve the chances of finding the global minimum, a common strategy is to perform multiple restarts:
\begin{enumerate}
    \item Initialize the weights $\mathbf{w}$ randomly several times.
    \item Train the model independently for each initialization.
    \item Select the solution with the lowest training error.
\end{enumerate}
This approach explores diverse regions of the parameter space, increasing the likelihood of finding a better overall solution.

\paragraph*{Computation}
Using all data points to compute weight updates, as in batch gradient descent, can be computationally expensive, especially for large datasets. 
The gradient of the error function in this approach is given by:
\[\dfrac{\partial \mathcal{L}(\mathbf{w})}{\partial \mathbf{w}}=\dfrac{1}{N}\sum_n^N\dfrac{\partial \mathcal{L}(x_n,\mathbf{w})}{\partial\mathbf{w}}\]
While precise, this method becomes inefficient as $N$ grows. 
Alternative approaches balance computation time and optimization efficiency:
\begin{itemize}
    \item \textit{Stochastic gradient descent}: the gradient is computed using a single data point at each iteration. 
        This method is computationally cheaper and enables faster iterations. 
        However, the updates can have high variance, leading to oscillations in the optimization process.
    \item \textit{Minibatch gradient descent}: a compromise between batch gradient descent and stochastic gradient descent is mini-batch gradient descent, where gradients are calculated using a small, randomly selected subset (mini-batch) of the dataset. 
        Mini-batch gradient descent strikes a balance, reducing computational costs while maintaining stable updates. 
        This approach allows for faster convergence and scalability, especially with large datasets.
\end{itemize}

\paragraph*{Backpropagation}
In Neural Networks, gradients are efficiently computed through backpropagation, a method that leverages the network's structure to update weights in parallel. 
Backpropagation utilizes the chain rule for derivatives, enabling efficient computation of gradients for complex, layered functions.
\noindent Backpropagation is executed in two steps:
\begin{enumerate}
    \item \textit{Forward pass}: the input propagates through the network to compute the output for each neuron.
        Local derivatives (dependent only on a neuron's immediate inputs) are calculated and stored for later use.
    \item \textit{Backward pass}: gradients are propagated backward using the stored values from the forward pass.
        The error's partial derivatives with respect to each weight are computed and used to update the weights.
\end{enumerate}

\paragraph*{Layers learning rate}
In Deep Neural Networks, layers exhibit varying learning dynamics due to differences in the magnitude of gradients. 
Conversely, deeper layers might experience exploding gradients, which can destabilize training. 
These disparities highlight the need for layer-specific learning rates to optimize learning across all layers effectively.

\subsection{Batch normalization}
To achieve faster convergence in Neural Networks, it is beneficial for the inputs to be whitened. 
This helps address covariate shift, where the input distribution changes, making learning more challenging.
However, even within a network, the intermediate layers may experience internal covariate shift, where the distribution of inputs to each layer changes during training.
\noindent Batch normalization is a powerful technique to combat internal covariate shift by normalizing the activations of each layer, ensuring they approximate a unit Gaussian distribution during training.

Batch normalization is typically applied after Fully Connected or convolutional layers, and before the activation functions.

\paragraph*{Algorithm}
Given a mini-batch $\mathcal{B} = \{ x_1, \dots, x_m \}$, the algorithm normalizes the inputs and applies learnable parameters $\gamma, \beta$. 
\begin{algorithm}
    \caption{Batch normalization}
    \begin{algorithmic}[1]
        \State $\mu_{\mathcal{B}} \gets \frac{1}{m} \sum_{i=1}^m x_i$ \Comment{Compute mini-batch mean}
        \State $\sigma_{\mathcal{B}}^2 \gets \frac{1}{m} \sum_{i=1}^m (x_i - \mu_{\mathcal{B}})^2$ \Comment{Compute mini-batch variance}
        \State $\hat{x}_i \gets \frac{x_i - \mu_{\mathcal{B}}}{\sqrt{\sigma_{\mathcal{B}}^2 + \epsilon}}$ \Comment{Normalize}
        \State $y_i \gets \gamma \hat{x}_i + \beta$ \Comment{Scale and shift}
    \end{algorithmic}
\end{algorithm}
\noindent Batch normalization has several notable benefits:
\begin{itemize}
    \item \textit{Improved gradient flow}: it enhances the flow of gradients through the network, making it easier to train deep architectures.
    \item \textit{Higher learning rates}: allows for the use of higher learning rates, leading to faster convergence.
    \item \textit{Reduced dependency on weight initialization}: the strong dependence on initial weights is mitigated, as the normalization process stabilizes the learning.
    \item \textit{Regularization effect}: acts as a form of regularization, slightly decreasing the need for techniques like dropout.
\end{itemize}

\subsection{Weight initialization}
The effectiveness of gradient descent in training Neural Networks heavily depends on the initialization of weights at the start. 
Proper weight initialization ensures stable and efficient convergence by mitigating problems like vanishing or exploding gradients. 
Different strategies for initializing weights are summarized below:
\begin{itemize}
    \item \textit{Zero initialization}: setting all weights to zero leads to symmetric gradients, causing every neuron in the same layer to update identically, rendering the network ineffective.
    \item \textit{Large weights}: initializing with large random weights can cause gradients to explode as they propagate through the layers, destabilizing training.
    \item \textit{Small weights}: using small random values can work for shallow networks. 
        However, in deeper architectures, this can cause gradients to diminish, leading to the vanishing gradient problem.
\end{itemize}
To address these limitations, more sophisticated initialization methods have been developed to balance the gradient flow through the network.

\paragraph*{Xavier initialization}
Proposed by Glorot and Bengio, Xavier initialization ensures that the variance of activations remains consistent across layers. 
This helps prevent signals from vanishing or exploding as they propagate forward or backward. 
The weights are drawn from a distribution:
\[\mathbf{w}\sim \mathcal{N}\left(0,\frac{1}{n_{\text{in}}}\right)\]
Here, $n_{\text{in}}$ represents the number of input units to the neuron.

For more stability, the approach can be extended to account for both input and output units. 
In this case, weights are initialized as:
\[\mathbf{w}\sim \mathcal{N}\left(0,\frac{2}{n_{\text{in}}+n_{\text{out}}}\right)\]

\paragraph*{He initialization}
Designed specifically for networks using ReLU activation functions, He initialization sets weights with a larger variance to accommodate the ReLU's behavior, where only positive values are passed. 
The weights are initialized as:
\[\mathbf{w}\sim \mathcal{N}\left(0,\frac{2}{n_{\text{in}}}\right)\]
This ensures that gradients maintain their magnitude during backpropagation, which is especially crucial in deep architectures. 
By maintaining better gradient flow, He initialization prevents dying neurons. 