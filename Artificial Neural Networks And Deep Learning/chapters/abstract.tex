\begin{abstract}
    Neural Networks have matured into flexible and powerful non-linear data-driven models, effectively tackling complex tasks in both science and engineering. 
    The emergence of Deep Learning, which utilizes Neural Networks to learn optimal data representations alongside their corresponding models, has significantly advanced this paradigm.

    We will begin with the evolution from the Perceptron to modern Neural Networks, focusing on the Feed Forward architecture. 
    The training of Neural Networks through backpropagation, along with best practices to prevent overfitting, including cross-validation, stopping criteria, weight decay and dropout.
    
    The course will also delve into specific applications such as image classification using Neural Networks, and we will examine Recurrent Neural Networks and related architectures. 
    Key theoretical concepts will be discussed, including the role of Neural Networks as universal approximation tools, and challenges like vanishing and exploding gradients.
    
    We will introduce the Deep Learning paradigm, highlighting its distinctions from traditional machine learning methods.
    The architecture and breakthroughs of Convolutional Neural Networks will be a focal point, including their training processes and data augmentation strategies.
    
    Furthermore, we will cover structural learning and Long-Short Term Memory networks, exploring their applications in text and speech processing. 
    Topics such as Autoencoders, data embedding techniques like Word2vec.
    
    Finally, we will discuss transfer learning with pre-trained deep models, examine extended models such as Fully Convolutional Neural Networks for image segmentation and object detection methods, and explore generative models like Generative Adversarial Networks.
\end{abstract}