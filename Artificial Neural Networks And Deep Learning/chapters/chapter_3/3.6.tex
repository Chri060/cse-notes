\section{Autoencoder}

Autoencoders are Neural Networks used primarily for Unsupervised Learning tasks, particularly data reconstruction. 
Their structure consists of an input layer, an encoder $\mathcal{E}$, a decoder $\mathcal{D}$, and an output layer. 
The objective is to minimize the reconstruction error, typically by optimizing the Mean Squared Error between the input and the output. 

Autoencoders are trained to reconstruct all the data in a given training set. 
The reconstruction loss over a batch $S$ is defined as:
\[\mathcal{L}=\sum_{s\in S}{\left\lVert s-\mathcal{D}(\mathcal{E}(s))\right\rVert}_2 \]
The training process uses standard backpropagation algorithms to optimize the parameters of both the encoder and decoder.

In essence, the Autoencoder learns an identity mapping between the input and output, with the encoder producing a latent representation of the input data.
These features $z=\mathcal{E}(s)$ are referred to as the latent representation of the data.

\paragraph*{Regularization}
Autoencoders do not always provide exact reconstruction because the dimensionality of the latent space is typically much smaller than that of the input. 
However, this dimensionality reduction is beneficial, as it forces the latent representation to be a compact and meaningful description of the data.
To guide the latent representation to exhibit certain desired properties, a regularization term can be added to the loss function:
\[\mathcal{L}=\sum_{s\in S}{\left\lVert s-\mathcal{D}(\mathcal{E}(s))\right\rVert}_2+\lambda\mathcal{R}(s) \]
Here, $\mathcal{R}$ could enforce properties like sparsity, or smoothness and sharp edges in the case of image data.
The use of deep Autoencoders enables the network to learn more powerful, nonlinear representations. 
Moreover, convolutional layers and transpose convolution can be employed to create deep convolutional Autoencoders.

The quality of the reconstruction improves as the latent space dimension increases. 
In the extreme case, if the latent space dimension is as large as the input, the network can perfectly reconstruct the input data, effectively learning an identity mapping.
However, reducing the latent space dimension forces the Autoencoder to learn a more compact, meaningful representation of the input.

\subsection{Initialization}
Autoencoders can serve as an effective way to initialize a model, especially when dealing with limited labeled data and a large pool of unlabeled data. 
The process involves the following steps:
\begin{enumerate}
    \item Train the Autoencoder in a fully unsupervised manner using the unlabeled data $S$. 
    \item Remove the decoder and retain only the encoder's weights.
    \item Add a Fully Connected layer for classification, using the latent representation produced by the encoder.
    \item Fine-tune the model using the labeled data $L$.  
        If $L$ is sufficiently large, the encoder weights can also be fine-tuned.
\end{enumerate}
This approach helps prevent overfitting because the latent vector of the Autoencoder already provides a good representation of the inputs, which can be leveraged for downstream tasks.

\subsection{Generative Autoencoder}
Autoencoders can also be used as generative models, but with some modifications.
Here's how it works:
\begin{enumerate}
    \item Train the Autoencoder on a dataset $S$.
    \item Discard the encoder.
    \item Sample random vectors $z\sim\phi_z$ from a latent distribution to generate a new latent representation, which is then fed into the decoder.
\end{enumerate}
However, this approach faces challenges, particularly in estimating the correct distribution of the latent space. 
As the dimensionality of the latent space increases, the likelihood of sampling from under-populated regions of the latent space rises. 
This makes it harder to generate meaningful samples from the decoder. In low-dimensional latent spaces, this can work reasonably well, but as the dimension grows, it becomes difficult to estimate the latent distribution effectively.

Variational Autoencoders address this issue by forcing the latent space to follow a Gaussian distribution.
This constraint not only improves reconstruction accuracy but also enables the Autoencoder to generate new data samples in a controlled manner, making Variational Autoencoders true generative models.