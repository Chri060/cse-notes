\section{Training}

Each Convolutional Neural Network can be conceptualized as a multi-layer perceptron characterized by sparse and shared connectivity. 
In principle, CNNs can be trained using gradient descent to minimize a loss function over a batch of data.

The gradient can be computed using backpropagation, which leverages the chain rule, provided that we can derive the operations for each layer of the CNN. 
It's important to consider weight sharing during the derivative computation, as this leads to a reduction in the number of parameters that need to be accounted for.

Deep learning models are often data-hungry, requiring large datasets for effective training. 
To cope with data scarcity we can employ two main tecniques. 
\begin{itemize} 
    \item \textit{Data augmentation}: each annotated image represents a broader set of images that are likely to belong to the same class. 
        Data augmentation expands this set by applying transformations to the images, which can be: 
        \begin{itemize} 
            \item \textit{Geometric transformations}: such as shifts, rotations, affine/perspective distortions, shearing, scaling, and flipping.
            \item \textit{Photometric transformations}: including adding noise, modifying average intensity, altering contrast, or superimposing other images.
        \end{itemize}
    \item \textit{Transfer Learning}: using pre-trained models can leverage existing knowledge from larger datasets, effectively reducing the need for extensive new training data.
\end{itemize} 

\subsection{Image augmentation}
To make the model more robust, the augmented images should retain the original label and promote invariance to transformations. 
However, augmentation should be carefully chosen to avoid transformations that obscure class-distinguishing features.

\paragraph*{Mixup augmentation}
Mixup is a domain-agnostic data augmentation method that doesn't require knowledge of specific transformations. 
It creates virtual samples by interpolating pairs of examples and their labels.
Given two training samples $(I_i,y_i)$ and $(I_j,y_j)$ (possibly from different classes), virtual samples can be created as follows:
\[\begin{cases}
    \tilde{I}=\lambda I_i+(1-\lambda)I_j \\
    \tilde{y}=\lambda y_i+(1-\lambda)y_j \\
\end{cases}\]
Here, $\lambda \in [0,1]$ and $y_i$, and $y_j$ are one-hot encoded labels.
Mixup effectively extends the training distribution by encouraging the model to generalize through linear interpolations of features.

Augmented training pairs $\{(A_l(I),y)\}_l$ guide the model to learn invariances to selected transformations. 
However, synthetic augmentation might not fully capture real-world variations, such as background variations, exposure issues, and other factors.

\paragraph*{Overfitting}
Data augmentation helps counteract overfitting by expanding the effective size of the training set. 
This technique can even be implemented as a layer within the network to ensure varied augmentation at each epoch. 
Additionally, data augmentation can mitigate class imbalance by generating more samples of minority classes, and in some cases, class-specific transformations can preserve relevant label information.
If augmentation introduces hidden class-discriminative features, the model may inadvertently rely on these for classification.

\paragraph*{Test time augmentation}
TTA enhances prediction accuracy by applying augmentations during testing. 
The process involves: 
\begin{itemize} 
    \item Applying random augmentations to each test image. 
    \item Classifying all augmented versions and storing the prediction probabilities. 
    \item Aggregating the predictions to obtain a final decision. 
\end{itemize}
While effective, TTA is computationally demanding and requires careful configuration of transformations to avoid excessive processing.

\subsection{Transfer learning}
In cases with limited data, transfer learning provides a powerful approach to leverage pre-trained models, such as ResNet, EfficientNet, or MobileNet, which have already learned general visual features. Transfer learning typically involves:
\begin{enumerate} 
    \item Removing the final fully connected (FC) layers of the pre-trained model. 
    \item Adding new FC layers tailored to the new problem, initialized randomly. 
    \item Freezing the pre-trained layers and training only the new layers.
\end{enumerate}
There are two primary strategies in transfer learning:
\begin{itemize}
    \item \textit{Transfer learning}: only the newly added FC layers are trained, ideal when the pre-trained model closely matches the new problem but has limited new data.
    \item \textit{Fine-tuning}: the entire model is retrained, but starting from the pre-trained weights. 
        Fine-tuning works best when a substantial amount of new data is available, or when the pre-trained model doesn't directly match the new task. 
        Lower learning rates are generally used for fine-tuning to preserve learned weights while gradually adapting them.
\end{itemize}
To maximize the effectiveness of a pre-trained model we can add a new output layer with minimal parameters.
In transfer learning, train only this output layer.
In fine-tuning, unfreeze some of the last layers and train the entire network at a reduced learning rate.