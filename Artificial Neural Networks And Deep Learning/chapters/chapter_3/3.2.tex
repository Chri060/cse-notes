\section{Image classification}

Image classification is the task of assigning an input image $\mathbf{I}\in\mathbb{R}^{R\times C \times 3}$ to a label $y$ from a predefined set of categories $\Lambda$. 
The goal of the classifier is to map the image to a corresponding class label. 
This mapping can be represented as:
\[f_{\boldsymbol{\theta}}:\mathbf{I} \rightarrow f_{\boldsymbol{\theta}}(\mathbf{I})\in\Lambda\]

\subsection{Linear classifier}
In a linear classifier, the image $\mathbf{I}$ is first flattened into a vector.
The image has three color channels (RGB), and each channel is linearized column by column.

Each element of this flattened vector corresponds to a neuron in the input layer of the neural network, and the network's output layer consists of $L$ neurons, where $L$ is the number of classes.
The network architecture connects each input neuron to every output neuron, resulting in a matrix of weights $\mathbf{w}\in\mathbb{R}^{L\times d}$. 

For each input image, the classifier computes a score for each class.
The score for the $i$-th class is the inner product between the image vector $\mathbf{x}$ and the corresponding row in the weight matrix $\mathbf{w}$. 
Since this is a linear classifier, nonlinearity is not required, and the softmax activation function can be omitted.

The classifier output $\mathcal{K}(\mathbf{I})$, is a vector of class scores, where each component $s_i$ represents the score for class $i$:
\[\mathcal{K}(\mathbf{x})=\mathbf{w}^T\mathbf{x}\]
The classifier's goal is to assign the image to the class with the highest score.

\subsubsection{Training}
The training process involves optimizing the classifier's parameters $\mathbf{w}$ to minimize the loss function over the training data.
The loss function $\mathcal{L}(\mathbf{w})$ quantifies the error between the predicted class scores and the true labels:
\[\mathbf{w}=\argmin_{\mathbf{w}}\sum_{(\mathbf{x}_i,y_i)}\mathcal{L}(\mathbf{w})\]

\paragraph*{Geometric interpretation}
From a geometric perspective, the classifier can be viewed as a linear function in the feature space $\mathbb{R}^d$, where each image is represented as a point in this space. 
The classifier is a hyperplane that separates different class points.

In the case of a two-dimensional feature space, he classifier's decision boundary is represented by a line. 
This decision boundary is defined by the equation $f(\mathbf{x})=w_0x_0+w_1x_1+w_2x_2$.

\subsection{K-Nearest-Neighbours}
In the $k$-nearest neighbors algorithm, the class of a test image is predicted by the most frequent label among its $k$-closest training images in the feature space.
Given a test image $\mathbf{I}_j$, the predicted class $\hat{y}_j$ is determined as follows:
\[\hat{y}_j=\argmax_{i=1,\dots,L}y_{j^\ast}\]
Here, $j^\ast$ represents the mode (most frequent class label) of the $k$-nearest images to the test image $\mathbf{I}_j$. 
The distance function $d(\cdot)$ typically used in $k$-NN could be the Euclidean distance or Manhattan distance, which quantifies the similarity between images in the feature space.
Although $k$-NN is easy to implement and requires no training, it is computationally expensive at test time and struggles with high-dimensional data.

\subsection{Challenges}
Image classification faces a range of challenges due to the complexity and variability inherent in image data. 
These challenges include:
\begin{enumerate}
    \item \textit{Dimensionality}: images and videos are high-dimensional, with each image consisting of thousands or millions of pixels. 
        This leads to issues with memory and computational resources.
    \item \textit{Label ambiguity}: a single label may not adequately represent the full content of an image, as images can have multiple elements, and classification might not capture all relevant details. 
    \item \textit{Invariance to transformations}: images can change significantly due to various transformations such as changes in illumination, deformations, or viewpoints. 
        However, these transformations often do not alter the core content or label of the image.
    \item \textit{Inter-class variability}: within the same class, images can differ significantly in terms of background, lighting, scale, or perspective. 
    \item \textit{Perceptual similarity}: similarity between images does not always correspond to pixel-level similarity.
\end{enumerate}