\section{Image classification}

Image classification is the task of assigning an input image $\mathbf{I}\in\mathbb{R}^{R\times C \times 3}$ to a label $y$ from a predefined set of categories $\Lambda$. 
The goal of the classifier is to map the image to a corresponding class label. 
This mapping can be represented as:
\[f_\theta:\mathbf{I} \rightarrow f_\theta(\mathbf{I})\in\Lambda\]

\subsection{Linear classifier}
In a linear classifier, the image $\mathbf{I}$ is first flattened into a vector.
The image has three color channels (RGB), and each channel is linearized column by column.

Each element of this flattened vector corresponds to a neuron in the input layer of the neural network, and the network's output layer consists of $L$ neurons, where $L$ is the number of classes.
The network architecture connects each input neuron to every output neuron, resulting in a matrix of weights $\mathbf{w}\in\mathbb{R}^{L\times d}$. 

For each input image, the classifier computes a score for each class.
The score for the $i$-th class is the inner product between the image vector $\mathbf{x}$ and the corresponding row in the weight matrix $\mathbf{w}$. 
Since this is a linear classifier, nonlinearity is not required, and the softmax activation function can be omitted.

The classifier output $\mathcal{K}(\mathbf{I})$, is a vector of class scores, where each component $s_i$ represents the score for class $i$:
\[\mathcal{K}(\mathbf{x})=\mathbf{w}^T\mathbf{x}\]
The classifier's goal is to assign the image to the class with the highest score.

\subsubsection{Training}
The training process involves optimizing the classifier's parameters $\mathbf{w}$ to minimize the loss function over the training data.
The loss function $\mathcal{L}(\mathbf{w})$ quantifies the error between the predicted class scores and the true labels:
\[\mathbf{w}=\argmin_{\mathbf{w}}\sum_{(\mathbf{x}_i,y_i)}\mathcal{L}(\mathbf{w})\]

\paragraph*{Geometric interpretation}
From a geometric perspective, the classifier can be viewed as a linear function in the feature space $\mathbb{R}^d$, where each image is represented as a point in this space. 
The classifier is a hyperplane that separates different class points.

In the case of a two-dimensional feature space, he classifier's decision boundary is represented by a line. 
This decision boundary is defined by the equation $f(\mathbf{x})=w_0x_0+w_1x_1+w_2x_2$, where $f(\mathbf{x})$ is the classifier's function.

\subsection{K-Nearest-Neighbours}
In the $k$-nearest neighbors ($k$-NN) algorithm, the class of a test image is predicted by the most frequent label among its $k$-closest training images in the feature space.
Given a test image $\mathbf{I}_j$, the predicted class $\hat{y}_j$ is determined as follows:
\[\hat{y}_j=\argmax_{i=1,\dots,L}y_{j^\ast}\]
Here, $j^\ast$ represents the mode (most frequent class label) of the $k$-nearest images to the test image $\mathbf{I}_j$. 
The distance function $d(\cdot)$ typically used in $k$-NN could be the Euclidean distance or Manhattan distance, which quantifies the similarity between images in the feature space.
Although $k$-NN is easy to implement and requires no training, it is computationally expensive at test time and struggles with high-dimensional data.

\subsection{Challenges}
Image classification faces a range of challenges due to the complexity and variability inherent in image data. 
These challenges include:
\begin{enumerate}
    \item \textit{Dimensionality}: images and videos are high-dimensional, with each image consisting of thousands or millions of pixels. 
        This leads to issues with memory and computational resources.
    \item \textit{Label ambiguity}: a single label may not adequately represent the full content of an image, as images can have multiple elements, and classification might not capture all relevant details. 
    \item \textit{Invariance to transformations}: images can change significantly due to various transformations such as changes in illumination, deformations, or viewpoints. 
        However, these transformations often do not alter the core content or label of the image.
    \item \textit{Inter-class variability}: within the same class, images can differ significantly in terms of background, lighting, scale, or perspective. 
    \item \textit{Perceptual similarity}: similarity between images does not always correspond to pixel-level similarity.
\end{enumerate}

\subsection{Data driven features}
To tackle the challenges of image classification, it's essential to first extract relevant features from the images before passing them to a classifier. 
Feature extraction helps reduce the data's dimensionality while retaining the critical information needed for accurate classification.

\paragraph*{Handcrafted features}
Handcrafted features are manually designed based on expert knowledge of the domain. These features have some clear advantages, including interpretability, tunability, and the ability to assign different weights to features. 
However, they are often not generalizable and require significant effort to design and craft, limiting their scalability and flexibility.

\paragraph*{Data driven features}
In contrast to handcrafted features, data-driven features are automatically learned directly from the data. 
This approach is more scalable and can lead to better performance since the features evolve from the raw data itself, without needing manual design or domain expertise.
This method adapts more effectively to a wide range of tasks and can handle larger datasets with greater efficiency.

\paragraph*{Convolutional Neural Networks}
Convolutional Neural Networks are a prime example of data-driven feature learning. 
These networks automatically learn to identify patterns through layers of convolution and pooling. 
As the network deepens, it captures increasingly abstract and high-level features of the image. 
This eliminates the need for manual feature engineering and typically results in better performance. 
By letting the model directly discover the most relevant features from the data, Convolutional Neural Networks reduce human bias and can generalize more effectively to new, unseen data.