\section{Reinforcement Learning}

Reinforcement Learning encompasses several key approaches:
\begin{itemize}
    \item \textit{Markov Decision Process}: a mathematical framework for modeling decision-making, involving states, actions, transition probabilities, and rewards. 
        The goal is to find a policy that maximizes cumulative rewards while considering uncertainty.
    \item \textit{Partially Observable Markov Decision Process}: an extension of Markov Decision Process where the current state is uncertain and must be inferred from observations. 
        The objective remains the same, but the agent maintains a belief over possible states based on observations.
    \item \textit{Stochastic games}: models for decision-making with multiple agents, where outcomes depend on actions and random factors. 
        Players aim to optimize strategies considering other players' actions and uncertainties.
\end{itemize}
In Reinforcement Learning, the computer learns the optimal policy based on a training set $\mathcal{D}$ containing tuples $\left\langle x,u,x^\prime,r \right\rangle$, where $x$ is the input, $u$ is the action, $x^\prime$ is the resulting state after the action, and $r$ is the reward.
The policy $Q^\ast$ is defined to maximize $Q^\ast(x,u)$ over actions $u$ for each state $x$ in the training set. 

Various techniques such as Q-Learning, SARSA, and fitted Q-iteration are used to find this optimal policy.