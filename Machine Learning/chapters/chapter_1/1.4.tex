\section{Reinforcement Learning}

Reinforcement Learning encompasses several key approaches:
\begin{itemize}
    \item \textit{Markov Decision Process}: a mathematical framework for modeling decision-making, involving states, actions, transition probabilities, and rewards. 
        The goal is to find a policy that maximizes cumulative rewards while considering uncertainty.
    \item \textit{Partially Observable Markov Decision Process}: an extension of Markov Decision Process where the current state is uncertain and must be inferred from observations. 
        The objective remains the same, but the agent maintains a belief over possible states based on observations.
    \item \textit{Stochastic games}: models for decision-making with multiple agents, where outcomes depend on actions and random factors. 
        Players aim to optimize strategies considering other players' actions and uncertainties.
\end{itemize}
In Reinforcement Learning, the computer learns the optimal policy based on a training set $\mathcal{D}$ containing tuples $\left\langle s,a,s^\prime,r \right\rangle$, where $s$ is the input, $a$ is the action, $s^\prime$ is the resulting state after the action, and $r$ is the reward.
The policy $\pi^\ast(a \mid s)$ is defined to maximize $Q_\pi(s,a)$ over actions $a$ for each state $s$ in the training set. 

Various techniques such as Q-Learning, SARSA, and fitted Q-iteration are used to find this optimal policy.