\section{Open Multi Processing}

OpenMP (Open Multi-Processing) is an application programming interface (API) designed for parallel programming on shared memory systems. 
It allows developers to efficiently utilize multiple processors or cores for concurrent execution, making it a powerful tool for parallelizing computationally intensive tasks. 
OpenMP provides compiler directives, library routines, and environment variables to manage multi-threading in C, C++, and Fortran programs.
\noindent The key features of OpenMP are: 
\begin{itemize}
    \item \textit{Compiler support}: OpenMP requires support from the compiler.
    \item \textit{Portability}: OpenMP is a standardized approach for parallel programming and is portable across various shared memory architectures.
    \item \textit{Ease of use}: OpenMP is known for its simplicity and ease of adoption. 
        It provides a minimal set of directives that are sufficient to implement significant parallelism. 
    \item \textit{Scalability}: OpenMP supports both coarse-grained and fine-grained parallelism, allowing it to scale from small systems to large multi-core processors.
\end{itemize}

\paragraph*{Programming model}
OpenMP is based on the fork-join parallel model:
\begin{enumerate}
    \item The master thread (the main program thread) begins execution.
    \item It forks a specified number of slave threads to execute tasks in parallel.
    \item The slave threads execute tasks concurrently, with the runtime environment managing the allocation of threads to different processors.
    \item Once all tasks are completed, the threads join back to the master thread, which continues execution.
\end{enumerate}

\paragraph*{Directives}
The core component of OpenMP is its use of pragmas, which are preprocessor directives that provide the compiler with instructions about parallel execution. 
Pragmas are used to define parallel tasks, control synchronization, and manage the execution flow. 
The basic syntax for an OpenMP directive is:
\begin{lstlisting}[style=C]
#pragma omp <name> [list of clauses]
\end{lstlisting}

\subsection{Control structures}
OpenMP programs typically execute serially until they encounter a parallel directive, which instructs the program to spawn multiple threads. 
The thread executing the code becomes the master thread (with thread ID 0), and it creates a group of slave threads that execute the same block of code concurrently. 
Each thread operates on a copy of the code within the parallel block, and once the block is completed, an implicit barrier ensures that all threads synchronize before the master thread continues.

A basic parallel directive in OpenMP looks like this:
\begin{lstlisting}[style=C]
#pragma omp parallel 
{
    /* parallel section */
}
\end{lstlisting}
Several clauses can be added to control the behavior of the parallel region:
\begin{itemize}
    \item \texttt{if(condition)}: this clause allows for conditional parallelization. 
        The parallel region will only be executed in parallel if the specified condition evaluates to true.
    \item \texttt{num\_threads(int)}: this specifies the number of threads to be used in the parallel region, overriding the default behavior.
    \item \textit{Data scope clauses}: these clauses control the visibility and lifetime of variables within the parallel region, and we'll cover them in more detail in later sections.
\end{itemize}
Under the hood, the compiler may replace the OpenMP parallel directive with a Pthreads-based implementation for thread management.

\subsection{Work sharing}
Work-sharing constructs in OpenMP allow the division of execution across multiple threads within a parallel region. 
Unlike parallel regions, work-sharing constructs do not create new threads. 
Instead, they divide the work already assigned to the threads in the team that is executing the parallel region. 
Importantly, work-sharing constructs do not imply a barrier when entering, but there is an implicit barrier when exiting.

These constructs are designed to split work across threads in different ways, depending on the nature of the task. 
They provide mechanisms for both data parallelism and functional parallelism, allowing efficient parallel execution.


\paragraph*{For loop}
The \texttt{for} construct is used to divide the iterations of a loop among the threads in the team, which is a form of data parallelism. 
The loop iterations cannot be modified internally within the loop.
\begin{lstlisting}[style=C]
#pragma omp parallel 
{
    #pragma omp for
    /* for loop */
}
\end{lstlisting}
The possible clauses are: 
\begin{itemize}
    \item \texttt{schedule}: specifies how iterations are assigned to threads.
    \item \texttt{nowait}: prevents synchronization at the end of the loop, allowing threads to proceed without waiting for others to complete.
    \item \textit{Data-scope clauses}: manage variable scope within the parallel loop.
    \item \texttt{reduction}: aggregates results across iterations. 
        A private copy of the variable is maintained by each thread, and at the end of the loop, the reduction operation is applied across all private copies.
\end{itemize}
\noindent The scheduling types are: 
\begin{itemize}
    \item \texttt{static}: divides loop iterations into equal-sized blocks (specified by chunk), which are statically assigned to threads. 
        If no chunk is specified, iterations are evenly distributed.
    \item \texttt{dynamic}: divides loop iterations into chunks of size chunk, distributed at runtime. 
        Once a thread completes a chunk, it is assigned another.
    \item \texttt{runtime}: the schedule is determined at runtime based on the \texttt{OMP\_SCHEDULE} environment variable.
    \item \texttt{guided}: starts with large chunks and gradually decreases their size.
\end{itemize}
The trade-off in scheduling lies between load balancing (dynamic scheduling with small chunks) and low overhead (static scheduling with large chunks).

\paragraph*{Sections}
The \texttt{sections} construct allows different sections of code to be executed in parallel, each by a different thread. 
This is an example of functional parallelism where the work is divided into discrete, independent sections.
\begin{lstlisting}[style=C]
#pragma omp parallel 
{
    #pragma omp sections 
    {
        #pragma omp section 
        {
            /* code section 1 */
        }
        #pragma omp section 
        {
            /* code section 2 */
        }
    }
}
\end{lstlisting}
Each section is executed exactly once by a thread, and the sections are executed concurrently.

The directive \texttt{single} ensures that a section of code is executed by only one thread, typically the master thread or a specific thread designated by the OpenMP runtime.
The directive \texttt{master} ensures that a section of code is executed only by the master thread, typically the one with thread ID 0. This can be useful for tasks that should only be done once or for thread-specific operations.

\subsection{Synchronization}
Synchronization constructs in OpenMP are used to coordinate the execution of multiple threads, ensuring that certain sections of code are executed in a controlled manner. 
These constructs are critical when dealing with shared resources, preventing race conditions, and maintaining consistency in parallel programs.

\paragraph*{Critical directive}
The \texttt{critical} directive ensures that a section of code is executed by only one thread at a time, preventing concurrent access to shared resources. 
This is essential for protecting critical sections where race conditions could occur.
\begin{lstlisting}[style=C]
#pragma omp critical [name] 
{
    /* code section */
}
\end{lstlisting}
The name allows you to define multiple critical sections. 
If different critical sections share the same name, they are treated as the same critical region, ensuring that only one thread can access them at a time.
If no name is provided, all unnamed critical sections are treated as a single region.

\paragraph*{Barrier directive}
The \texttt{barrier} directive synchronizes all threads within the team. 
When a thread reaches a barrier, it will wait until all other threads have reached the same point, after which all threads can resume executing the code that follows the barrier.
\begin{lstlisting}[style=C]
#pragma omp barrier
\end{lstlisting}
This construct is less commonly used because many other OpenMP constructs implicitly synchronize threads, making the explicit use of barriers less frequent.

\paragraph*{Atomic directive}
The \texttt{atomic} directive ensures that a specific memory location is accessed atomically. 
This guarantees that no other thread can modify the memory location during a read-modify-write operation, thus avoiding race conditions.
\begin{lstlisting}[style=C]
#pragma omp atomic
    /* statement */
\end{lstlisting}
The \texttt{atomic} directive can only be used for individual statements, not for entire code blocks. 
It ensures that the operation on a specific memory location is completed without interruption, supporting safe updates in multi-threaded environments.
\noindent Only one atomic operation can be applied to a specific memory location at a time. 
Multiple reads and writes are not allowed within the atomic region.

\subsection{Data environment}
OpenMP is based on the shared-memory programming model, where most variables are shared by default among threads. 
However, in parallel computing, explicit control over how variables are scoped is crucial for managing data consistency and performance. 
The OpenMP Data Scope Attribute Clauses provide the necessary control over how variables are handled in parallel regions. 
These clauses include \texttt{private}, \texttt{shared}, \texttt{default}, and \texttt{reduction}, which allow developers to define the visibility and allocation of variables across threads.

\paragraph*{Private clause}
The \texttt{private} clause ensures that each thread gets its own instance of a variable.
A new object of the same type is created for each thread, and any reference to the original variable is replaced with a reference to the private instance.
\begin{lstlisting}[style=C]
#pragma omp <name> private(list)
\end{lstlisting}
Each thread gets a unique copy of the variable, initialized independently of other threads.
Useful when a variable's value should not be shared between threads and should be initialized separately for each.

\paragraph*{Shared clause}
The \texttt{shared} clause declares variables that are shared among all threads in a parallel region.
There is only one memory location for a shared variable, and all threads can read and write to this location.
The programmer must ensure that the variable is accessed safely to avoid race conditions. 
\begin{lstlisting}[style=C]
#pragma omp <name> shared (list)
\end{lstlisting}
All threads access the same memory location for the variable.
Ideal for variables that need to be updated or read by all threads, such as counters or buffers.

\paragraph*{Default clause}
The \texttt{default} clause sets the default data scope for variables that are not explicitly scoped by \texttt{private} or \texttt{shared}.
By default, variables are \texttt{shared}, 
\begin{lstlisting}[style=C]
#pragma omp <name> default(shared | none)
\end{lstlisting}
Controls the default scoping of variables in the parallel region.
Helps enforce stricter control over variable scoping, reducing errors in large parallel applications.

\paragraph*{Reduction clause}
The \texttt{reduction} clause is used to perform a reduction operation on a variable across threads. 
Each thread gets a private copy of the variable, and at the end of the parallel region, these private copies are combined using a specified operator.
\begin{lstlisting}[style=C]
#pragma omp <name> reduction(operator: list)
\end{lstlisting}
Allows threads to operate on private copies of variables, and at the end of the region, a reduction operator is applied to aggregate the results.
Useful for operations like summing up values or finding the minimum or maximum in parallel loops.

\subsection{Memory model}
In OpenMP, threads share a common memory space, but each thread also has its own private memory. 
The interaction between private and shared memory, and how updates to shared variables are synchronized across threads, is governed by the memory consistency model.
This model ensures that threads have a coherent view of memory, especially when accessing or modifying shared variables.
The key elements of OpenMP memory model are: 
\begin{itemize}
    \item \textit{Memory access}: each thread has access to private memory, which is used for data specific to the thread.
        Threads can also access shared memory, a common address space, where variables can be read or modified by multiple threads.
    \item \textit{Relaxed memory consistency}: OpenMP uses a relaxed memory consistency model where threads have their own temporary view of memory between consistency points.
        A consistency point is a point in the program where memory views of all threads are synchronized, ensuring that all threads have the same view of shared memory at those moments.
        In between consistency points, each thread may have its own version of the shared memory, which can lead to data races if not carefully controlled.
    \item \textit{Shared data modifications}: if shared data is modified by multiple threads, the potential for data races arises. 
        A data race occurs when multiple threads modify the same shared variable without proper synchronization, leading to unpredictable behavior.
\end{itemize}
To manage memory consistency explicitly, OpenMP provides the \texttt{flush} directive, which enforces a global consistency of shared variables.
\begin{lstlisting}[style=C]
#pragma omp flush [flush-set]
\end{lstlisting}
The \texttt{flush} directive ensures that all threads have a consistent view of shared variables at a particular point in the program.
Between a \texttt{flush} and the next update of shared variables, all threads are guaranteed to have the same global view of shared memory.
If the \texttt{flush-set} is not specified, all shared variables are affected by the flush operation. 
This is generally best avoided if possible, as specifying the set of variables to flush can be more efficient.
The compiler is allowed to move \texttt{flush} operations if they involve disjoint sets of variables, which can improve performance but can also lead to incorrect behavior if used incorrectly.

Certain OpenMP constructs implicitly enforce memory consistency:
\begin{itemize}
    \item At barrier regions: All threads synchronize at the barrier, ensuring consistency.
    \item At entry and exit of parallel and critical regions: These constructs synchronize threads and provide consistency points.
    \item At exit from work-sharing constructs, unless the nowait clause is used: this ensures synchronization among threads at the end of a work-sharing region.
\end{itemize}
While certain synchronization points implicitly enforce consistency, in some cases, the programmer must explicitly use the flush directive to guarantee a consistent memory view at entry to work-sharing constructs and master regions, a flush is not implied and may need to be used manually.

\subsection{Runtime functions}
OpenMP standard provides a set of runtime functions that allow you to control and gather information about the execution of a parallel program. 
These functions are part of the OpenMP API and enable developers to dynamically manage the parallel execution environment: 
\begin{lstlisting}[style=C]
// Returns the number of threads currently executing in the parallel region from which the function is called.
int omp_get_num_threads()

// Returns the unique identifier (thread ID) of the calling thread.
int omp_get_thread_num()

// Sets the number of threads that will be used in the next parallel region.
double omp_get_wtime()

// Provides the wall clock time in seconds since some arbitrary point in the past.
double omp_get_wtick()
\end{lstlisting}

\paragraph*{Environment variables}
In addition to runtime functions, OpenMP allows you to control the execution of parallel programs using environment variables. 
These variables influence the behavior of the OpenMP runtime system and can be set at runtime.

\subsection{Nested parallelism}
OpenMP supports nested parallelism, where parallel regions can be nested within other parallel regions. 
This allows for more flexible and efficient parallel execution, particularly in complex workloads.

Nested parallelism can be enabled or disabled using the following methods:
\begin{itemize}
    \item \textit{Using the runtime function}: you can enable or disable nested parallelism through the \texttt{omp\_set\_nested} function. 
    \item \textit{Using the environment variable}: you can also control nested parallelism through the environment variable \texttt{OMP\_NESTED}. 
\end{itemize}
The default behavior for nested parallelism is implementation-dependent, and different OpenMP compilers may have varying defaults. 
In some systems, nested parallelism may be disabled by default.
To control the number of threads in nested parallel regions, you can use the following environment variables:
\begin{itemize}
    \item \texttt{OMP\_NUM\_THREADS=<list of integers>}: sets the default number of threads for different levels of nested parallelism. 
        Each entry in the list corresponds to a level of nesting. 
        If the nesting level exceeds the number of entries, the last value in the list is used for all subsequent levels.
    \item \texttt{OMP\_MAX\_ACTIVE\_LEVELS}: specifies the maximum number of active parallel regions that can be nested.
    \item \texttt{OMP\_THREAD\_LIMIT}: sets a limit on the total number of threads that can be created, which can help prevent excessive thread creation in recursive applications.
\end{itemize}
In nested parallelism, each nested parallel region has its own set of threads, and thread IDs are reset to 0 for each new nested team.
Therefore, the global thread IDs from \texttt{omp\_get\_thread\_num} are not sufficient to uniquely identify threads across nested regions.

To manage nested parallelism more effectively, OpenMP provides several runtime functions:
\begin{lstlisting}[style=C]
// Returns the maximum number of threads that can be used in the current parallel region
int omp_get_thread_limit()

// Returns the maximum number of active parallel regions in the current nested parallel execution
int omp_get_max_active_levels()

// Sets the maximum number of active parallel regions that can be nested
void omp_set_max_active_levels(int max_levels)

// Returns the current level of nesting within parallel regions. It returns the depth of the current parallel region
int omp_get_level()

// Returns the level of the current active parallel region
int omp_get_active_level()

// Returns the thread ID of the ancestor thread at a specific level of nesting. 
// The level argument specifies the ancestor level.
int omp_get_ancestor_thread_num(int level)

// Returns the size of the thread team for a specified nesting level
int omp_get_team_size(int level)
\end{lstlisting}

\subsection{Thread cancellation}
Prior to OpenMP 4.0, once a parallel region was started, it would run to completion. 
It was not possible to abort or cancel a parallel region during execution. 
However, OpenMP 4.0 introduced the ability to cancel parallel execution, providing more flexibility for certain applications like divide-and-conquer algorithms or error handling.

OpenMP's thread cancellation mechanism is designed as a best effort approach. 
This means that the system does not guarantee an immediate termination of threads, but it provides a way to attempt cancellation under certain conditions.

Cancellation is useful for scenarios where the execution of parallel tasks can be halted early, such as in a search algorithm where processing can stop once a result is found. 
Similarly, cancellation is useful for handling errors or unexpected situations where continuing execution is no longer desirable.

\paragraph*{Cancellation directives}
The \texttt{cancel} directive is used to mark a region of the code where threads can potentially be canceled.
\begin{lstlisting}[style=C]
#pragma omp cancel <construct-type>
\end{lstlisting}
The \texttt{<construct-type>} specifies which type of construct is to be canceled

A thread can check whether cancellation has been requested by encountering a cancellation point.
When a thread reaches this point, it checks for a cancellation flag set by the cancel directive. 
If cancellation is requested, the thread will attempt to terminate at this point.
\begin{lstlisting}[style=C]
#pragma omp cancellation point <construct-type>
\end{lstlisting}
At a cancellation point, the thread checks the cancellation flag. 
If the flag is set, the thread terminates its execution. 
Cancellation points can occur: at another cancel region or at a barrier (either implicit or explicit).

\subsection{Tasks}
OpenMP tasks are a powerful construct for parallelizing algorithms that have irregular or runtime-dependent execution flows, where the work cannot be easily divided into independent iterations or chunks. 
Unlike loops that execute a predictable number of iterations, tasks allow for more dynamic and flexible execution.

An OpenMP task is a block of code within a parallel region that can be executed concurrently with other tasks in the same region. 
The task is created when encountered in the code but is not necessarily executed immediately or in the order it appears in the source code. 
Instead, tasks are dynamically assigned to threads by the OpenMP runtime system.
Tasks are particularly useful in situations like while loops or any algorithm that requires dynamic decision-making about what work needs to be done next. 
OpenMP manages the queuing and scheduling of tasks, allowing for better load balancing and better use of resources in cases where the execution flow is not regular.

\paragraph*{Task synchronization}
Once a task is created, it is not executed immediately; instead, it is added to a task queue. 
Tasks will only be executed when resources are available. However, there are points at which synchronization is required to ensure that tasks complete before proceeding further.
OpenMP provides constructs to handle synchronization of tasks:
\begin{enumerate}
    \item \texttt{taskwait}: this directive forces the current thread to wait for all child tasks to complete before it resumes execution. 
        It's particularly useful when the completion of tasks needs to be ensured before continuing.
\begin{lstlisting}[style=C]
#pragma omp taskwait 
\end{lstlisting}
    \item \texttt{taskgroup}: the \texttt{taskgroup} directive synchronizes not only the child tasks but also their descendants. 
        This is useful when there are multiple layers of tasks, and synchronization is required at multiple levels.
\begin{lstlisting}[style=C]
#pragma omp taskgroup
\end{lstlisting}
\end{enumerate}
Tasks are also guaranteed to complete at synchronization points such as a barrier (explicit or implicit) or task synchronization points.

\paragraph*{Task dependencies}
OpenMP tasks support task dependencies, which help manage complex parallel workflows, particularly when certain tasks need to wait for others to finish before they can start (e.g., in pipeline parallelism). 
This allows computation to overlap with other activities, such as I/O, and can reduce idle time.


\paragraph*{Depend clause}
The \texttt{depend} clause specifies the dependencies between tasks, indicating that one task must finish before another begins. 
This helps ensure that tasks are executed in the correct order.
\begin{lstlisting}[style=C]
#pragma omp task depend(in: data) depend(out: result)
\end{lstlisting}

\paragraph*{Priority clause}
The \texttt{priority} clause gives a hint to the runtime system about the importance of a task. 
Tasks with higher priority are executed first, although this is just a hint and not a strict guarantee. 
To use the priority feature, the environment variable \texttt{OMP\_MAX\_TASK\_PRIORITY} must be set before running the program (with a default value of 0).
\begin{lstlisting}[style=C]
#pragma omp task priority(5)
\end{lstlisting}

\paragraph*{Task scheduling}
The scheduling of tasks can introduce overhead. 
OpenMP tasks provide a mechanism to reduce this overhead by allowing for the scheduling of larger units of work, but at the cost of less flexibility in handling load imbalances.
This approach is efficient for cases where the tasks are large enough to offset the scheduling overhead, but it limits flexibility in handling load imbalances.

\paragraph*{Task loops}
OpenMP also provides a convenient way to handle loops using tasks.
By using the \texttt{taskloop} construct, the runtime system can efficiently divide loop iterations into tasks, reducing the complexity of manually managing task creation for each iteration.
\noindent When using loops with tasks, OpenMP automatically handles task dependences and scheduling. 
This allows for flexible parallel execution of loops with a dynamic number of iterations or varying workloads.

\subsection{SIMD vectorization}
SIMD (Single Instruction, Multiple Data) vectorization is a powerful technique used to enable parallelism within a single thread by processing multiple data elements simultaneously with vector instructions. 
In OpenMP, SIMD directives allow loops to be vectorized, helping to improve performance by taking advantage of CPU vector units.

The simplest way to apply SIMD vectorization is with the \texttt{\#pragma omp simd} directive: 
\begin{lstlisting}[style=C]
#pragma omp simd
    /* for loop */
\end{lstlisting}
It instructs the compiler to vectorize a loop and execute all iterations with SIMD instructions, which means multiple loop iterations are processed in parallel by a single thread using vector registers.
The loop iterations are divided into chunks, and each chunk is executed by a SIMD lane. 
The compiler generates the SIMD instructions necessary to execute the loop efficiently, but it's up to the user to ensure that the vectorization maintains correctness.

Several options can be used to fine-tune SIMD performance and ensure optimal execution:
\begin{enumerate}
    \item \textit{Data scope clauses}: clauses such as \texttt{private}, \texttt{firstprivate}, and \texttt{reduction} can be used with the SIMD directive to control how variables are scoped during the vectorized loop.
    \item \texttt{collapse} \textit{clause}: used to combine two perfectly nested loops into a single larger loop, which can help improve vectorization. 
        However, this comes with additional complexity, so it should be used carefully.
    \item \texttt{simdlen(size)} \textit{clause}: the \texttt{simdlen} clause suggests a preferred vector length. 
        The compiler is free to ignore this value, but it can help optimize performance in some cases.
    \item \texttt{safelen} \textit{clause}: the \texttt{safelen} clause sets an upper limit on the vector length that the compiler cannot exceed. 
        This is useful when dealing with loop-carried dependencies, as the vector length must be smaller than the smallest dependence distance to avoid incorrect behavior.
\end{enumerate}

\paragraph*{Composite contruct}
The \texttt{\#pragma omp for simd} directive is a combination of both the SIMD vectorization and work-sharing constructs. 
It tells the compiler to distribute iterations among threads in a team while also vectorizing the loop.
\noindent This construct distributes iterations across multiple threads, and within each thread, the loop is vectorized using SIMD instructions. 
It's important to note that the number of threads and scheduling policy can greatly affect performance. 
If the number of threads increases, the work per thread becomes smaller, so each thread should ideally work with a chunk corresponding to the vector length.

\paragraph*{Scheduling policies}
To avoid performance degradation, OpenMP allows specifying SIMD scheduling policies in conjunction with the \texttt{simd} directive. 
The \texttt{schedule(simd)} clause defines how iterations are distributed among threads while using SIMD instructions for parallel execution.
\begin{lstlisting}[style=C]
#pragma omp for simd schedule(simd:static, 5) 
\end{lstlisting}

\subsection{OpenMP for heterogeneous architectures}
Recent versions of OpenMP support parallel execution not only on CPUs but also on heterogeneous architectures. 
This allows a single OpenMP codebase to be executed both on the host CPU and on target devices like GPUs or other accelerators, without requiring changes in the code.

The \texttt{target} directive is used to offload computation to a device while keeping the same code that runs on the host. 
The code inside the target region will be executed on the device if available; otherwise, it will continue to execute on the host.
\begin{lstlisting}[style=C]
#pragma omp target
{
    // Code region to be offloaded to the target device
}
\end{lstlisting}
By default, execution is synchronous, meaning that the host thread will block until the device finishes executing the offloaded region.
If you want to avoid blocking, you can use the \texttt{nowait} clause, which tells the host to not wait for the target device to finish before continuing with other work.

\paragraph*{Map clause}
The \texttt{map} clause is crucial when working with offloaded code, as it controls the transfer of data between the host and the target device memory.
\begin{lstlisting}[style=C]
#pragma omp target map(to: x[0:n]) map(from: y[0:m])
{
    // Offloaded code
}
\end{lstlisting}
Here: 
\begin{itemize}
    \item \texttt{map(to)}: data is copied from the host to the target device before the offloaded code executes.
    \item \texttt{map(from)}: data is copied from the target device back to the host after the offloaded code executes.
    \item \texttt{map(tofrom)}: data is both sent to the device before execution and received from the device after execution.
\end{itemize}
The map-type option tells the compiler how data should be transferred between the host and device, allowing optimizations to minimize unnecessary copies. 
The default mapping type is tofrom, meaning the data is both sent to and retrieved from the device unless specified otherwise.