\section{Algorithms parallelization}

\paragraph*{Automatic parallelization}
In automatic parallelization, developers write algorithms and implement them with sequential code, relying on automated tools to handle all parallelization. 
This approach allows developers to work with familiar, sequential algorithms without having to restructure them for parallel execution.

However, fully automated parallelization remains challenging and is currently not entirely feasible. 
The primary limitation is that these tools struggle to extract all potential parallelism from code initially designed for sequential execution. 
Consequently, the performance gains from automatic parallelization are often limited, especially for complex or intricate tasks.

\paragraph*{Manual parallelization}
Manual parallelization requires the programmer to play an active role in the parallelization process. 
Developers must design parallel algorithms and implement them using high-level parallel programming constructs. 
By providing targeted information to the tools, the programmer assists in optimizing the parallel execution, leaving only the compilation of code to automated tools.
When using manual parallelization, there are three critical aspects to consider:
\begin{itemize} 
    \item \textit{Parallelism type}: determine which type of parallelism is most suitable for the application. 
    \item \textit{Tool communication}: effectively communicate information about the parallelism to tools so that they can optimize performance during compilation. 
\end{itemize}

\subsection{Taxonomy}
Parallelism comes in various forms, each offering different ways to execute instructions and process data simultaneously. 
The two primary types are instruction parallelism and data parallelism, which can also be combined according to Flynn's taxonomy. 
This classification, proposed in 1966, helps categorize computer architectures by their parallel processing capabilities.
\begin{table}[H]
    \centering
    \begin{tabular}{|c|lll|}
    \hline
    \multicolumn{1}{|l|}{\textbf{Architecture}} & \textbf{Instructions} & \textbf{Data} & \textbf{Example}       \\ \hline
    \textit{SISD}                               & Single                & Single        & Single-core processors \\ \hline
    \textit{SIMD}                               & Single                & Multiple      & GPU                    \\ \hline
    \textit{MISD}                               & Multiple              & Single        & -                      \\ \hline
    \textit{MIMD}                               & Multiple              & Multiple      & Multicore processors   \\ \hline
    \end{tabular}
\end{table}

\paragraph*{Parallelism classification}
Parallelism can also be categorized by the level at which it occurs:
\begin{enumerate}
    \item \textit{Bit-level parallelism}: bits within data words can represent distinct data elements, allowing a single instruction to manipulate multiple data bits at once.
    \item \textit{Instruction-level parallelism}: multiple instructions are executed simultaneously on a single core.
        Compilers can often extract this type of parallelism automatically.
    \item \textit{Task-level parallelism}: tasks are discrete units of computational work, typically composed of program-like sets of instructions.
        Multiple tasks can be executed on multiple processors, supported by shared memory and cache coherence mechanisms, though this parallelism is challenging to automate fully.
        
        Task-level parallelism can be represented with a parallel task graph in whick tasks are represented by graph vertices, while edges indicate dependencies or data communications.
        Each task is ideally executed once, following the directed acyclic structure.

        Alternatively, we may use a classic pipeline, in which each processor represents a stage in the pipeline.
        Edges denote data passing between stages, making this model well-suited for stream processing tasks like audio and video encoding, where each stage processes data continuously.
\end{enumerate}

\paragraph*{Communication classification}
Effective communication between tasks is essential in parallel systems, with two primary models:
\begin{itemize}
    \item \textit{Shared memory}: all tasks have access to a global memory space, allowing them to read from and write to common memory locations. 
        Any modification is visible across all processors.
    \item \textit{Private memory}: each task has its own private memory, and communication is achieved through explicit message exchanges. 
        This model offers high modularity and isolation between tasks, reducing shared memory conflicts.
\end{itemize}