\section{Parallelism design}

Designing an effective parallel algorithm involves more than simply extracting all available parallelism. 
Not all parallelism is usable on a given architecture, as certain types may introduce overhead or be limited by the hardware. 
To create a practical and efficient parallel solution, designers must consider the architecture's specific parallel capabilities and communicate this parallelism to the compilation tools effectively.

New languages developed specifically for parallel programming offer advanced features for expressing parallelism, but their adoption has been limited: since they tend to have immature compilers, and developers must invest time in learning new syntax and semantics.
In contrast, extensions to established languages are easier to adopt and integrate with existing compilers but have limitations. 
These extensions typically only support certain forms of parallelism, making it challenging to represent more complex structures like pipeline parallelism.

To design an algorithm, we usually understand the problem to be solved, analyze the dependencies, partition the solution, and use the PCAM methodology to define other aspects. 

\subsection{PCAM mehod}
The Partitioning-Communication-Agglomeration-Mapping (PCAM) methodology is a structured approach to designing efficient parallel algorithms, consisting of four interrelated phases:
\begin{enumerate}
    \item \textit{Partitioning}: the goal of partitioning is to reveal parallelizable components within the problem, dividing it into many small, independent tasks to create a fine-grained decomposition. 
        Effective partitioning divides both the computational tasks and associated data without unnecessary replication.
        \begin{itemize}
            \item \textit{Functional partitioning}: focuses on decomposing the problem based on the actions or computations required. 
                Tasks are defined by the different functions or operations needed to solve the problem.
            \item \textit{Domain partitioning}: divides the problem based on data, where each task processes a subset of the data. 
                This method is often preferred for problems that naturally separate into data chunks; ideally, tasks should have equal data sizes to balance workloads.
        \end{itemize}
        A combination of functional and domain partitioning may sometimes be effective to optimize parallelization.
    \item \textit{Communication}: once partitioned, tasks must interact to exchange necessary information. 
        This phase addresses the nature and management of communication between tasks, aiming to minimize overhead and maximize efficiency.        
        The communication can be classified as: 
        \begin{itemize}
            \item \textit{Local or global}: limited to a small set of neighboring tasks or involve many tasks simultaneously. 
            \item \textit{Structured or unstructured}: follow regular patterns or arbitrary, dynamic connections. 
            \item \textit{Static or dynamic}: communication partners may be fixed in advance or determined at runtime.
            \item \textit{Synchronous or asynchronous}: tasks may synchronize their communication or communicate independently. 
            \item \textit{Point-to-point or collective}: direct data transfer between two tasks or between multiple tasks within a group. 
        \end{itemize} 
    \item \textit{Agglomeration}: this phase transitions from theoretical task partitioning to a more practical implementation by grouping tasks into larger units to improve efficiency on the target parallel architecture: 
        \begin{itemize}
            \item \textit{Task consolidation}: combines smaller tasks into larger ones to increase computational granularity and reduce communication needs.
            \item \textit{Data and computation replication}: may replicate data or computation selectively to reduce dependency on frequent communication.
            \item \textit{Load balancing and overlapping}: ensures an even distribution of workload, overlapping communication with computation when possible.
        \end{itemize}
        Agglomeration often changes key performance ratios, increasing task size to reduce communication demands, thereby enhancing efficiency but potentially decreasing parallelism. 
        This phase must balance the reduction of communication overhead with maintaining adequate parallelism.
    \item \textit{Mapping}: mapping assigns tasks to specific processors, with the objective of minimizing total execution time. 
        Although straightforward in shared-memory systems, mapping in distributed systems requires careful consideration of locality (place frequently communicating tasks on the same or neighboring processors to minimize communication time) and concurrency (assign concurrent tasks to separate processors to maximize parallel execution).
        Mapping is an $\mathcal{NP}$-complete problem, so heuristic algorithms are used to approximate optimal solutions.
        Mapping can be complex due to resource limitations and the potential for conflicting goals between maximizing concurrency and minimizing communication costs.
\end{enumerate}