\section{Patterns}

Parallel patterns refer to recurring combinations of task distribution and data access strategies that address specific challenges in parallel algorithm design. 
One of the key strengths of parallel patterns is their universality.

\subsubsection{Serial control patterns}
Structured serial programming relies on four fundamental control patterns:
\begin{itemize}
    \item \textit{Sequence}: a sequence is an ordered list of tasks executed in a specific order. 
    \item \textit{Selection}: in a selection pattern, a condition $c$ is evaluated first. 
        Depending on whether the result of $c$ is true or false, either task $a$ or task $b$ is executed. 
        The assumptions are that neither task $a$ nor task $b$ can be executed before $c$, and only one of them is executed.
    \item \textit{Iteration}: an iteration involves evaluating a condition $c$. 
        If $c$ is true, task $a$ is executed, and the condition is evaluated again. 
        This cycle repeats until $c$ becomes false. 
    \item \textit{Recursion}: dynamic form of nesting where functions can call themselves. 
\end{itemize}

\subsubsection{Parallel control pattern}
Parallel control patterns extend the serial control patterns, building on the same principles but relaxing some of the assumptions inherent in serial execution. 
Each parallel control pattern corresponds to at least one serial control pattern but allows for concurrent execution, enabling parallelism.
The key parallel control patterns are:
\begin{itemize}
    \item \textit{Fork-join}: this pattern enables the control flow to fork into multiple parallel flows, which later rejoin. 
        Functions that spawn another function call continue to execute, while the caller synchronizes with the spawned function to join both paths. 
    \item \textit{Map}: this pattern applies a function to every element of a collection. 
        It replicates a serial iteration pattern where each iteration is independent, the number of iterations is known in advance, and the computation depends only on the iteration index and input data from the collection. 
    \item \textit{Stencil}: an elemental function accesses a set of neighbors.
        Stencil is a generalization of the map pattern and is commonly used in iterative solvers or to evolve a system over time. 
    \item \textit{Reduction}: combines every element in a collection using an associative combiner function.
        The associativity of the combiner function allows different orderings of the reduction, making the process parallelizable. 
    \item \textit{Scan}: computes all partial reductions of a collection. 
        For each output in the collection, a reduction of the input up to that point is computed. 
        If the function used in the scan is associative, the operation can be parallelized. 
    \item \textit{Recurrence}: a more complex version of the map pattern, the recurrence pattern allows loop iterations to depend on one another. 
        For a recurrence to be computed in parallel, there must be a serial ordering of the recurrence elements, ensuring that each element is computed using previously computed outputs.
\end{itemize}

\subsubsection{Serial data management patterns}
Serial programs manage data in various ways depending on how it is allocated, shared, read, written, and copied.
Data management is essential for ensuring that data is handled correctly and efficiently, especially when transitioning from serial to parallel execution.
The following are key serial data management patterns:
\begin{itemize}
    \item \textit{Random read and write}: this pattern involves accessing memory locations indexed by addresses. 
    \item \textit{Stack allocation}: stack allocation is used for dynamically allocating data in a LIFO manner. 
        It is efficient, as an arbitrary amount of data can be allocated in constant time. 
    \item \textit{Heap allocation}: heap allocation is useful when data cannot be allocated in a LIFO manner. 
        While more flexible than stack allocation, heap allocation is slower and more complex due to the dynamic nature of memory management. 
    \item \textit{Objects}: objects are constructs in object-oriented languages that associate data with functions to manipulate and manage that data.
\end{itemize}

\subsubsection{Parallel data management patterns}
To avoid issues such as race conditions, it is crucial to understand when data is potentially shared among multiple parallel workers. 
Effective data management is essential for ensuring correct and efficient parallel execution. Some parallel data management patterns help improve data locality and prevent conflicts.
The following are key parallel data management patterns:
\begin{itemize}
    \item \textit{Pack}: used to eliminate unused space in a collection. 
        Elements marked as false are discarded, and the remaining elements are placed in a contiguous sequence, preserving their original order. 
    \item \textit{Pipeline}: connects tasks in a producer-consumer fashion, where one task produces data that is consumed by another. 
    \item \textit{Geometric decomposition}: divides data into subcollections, which can be either overlapping or non-overlapping. 
        This pattern offers an alternative view of the data to improve parallel processing.
    \item \textit{Gather}: reads a collection of data based on a set of indices. 
        It combines elements of the map pattern with random serial reads. 
        The output collection shares the same type as the input collection but takes the shape of the indices collection.
    \item \textit{Scatter}: requires a set of input data and corresponding indices. 
        Each element of the input data is written to the output collection at the specified index. 
\end{itemize}