\section{Dependencies}

Parallel execution is inherently constrained by the sequence of operations required to ensure a correct result. 
To achieve efficient parallelism, it is crucial to address dependencies. 
A dependency occurs when one operation must complete and produce a result before a subsequent operation can proceed.

In addition to the traditional dependencies among operations, we extend this concept to resource dependencies, where certain operations may rely on the availability of specific resources to execute.

The fundamental assumption in concurrent execution is that processors operate independently. 
There are no assumptions regarding the speed of execution between processors, meaning each processor can run at its own pace.

\paragraph*{Sequential consistency}
Sequential consistency ensures that the execution of statements does not interfere with one another, and the computation results remain consistent, regardless of the order in which the operations are executed.
When this condition holds true, we can conclude that the two statements are independent of each other. 

\begin{definition}[\textit{True dependence}]
    A statement $S_2$ has a true (flow) dependence on $S_1$ if and only if $S_2$ reads a value that was written by $S_1$.
\end{definition}
\begin{definition}[\textit{Anti-dependence}]
    A statement $S_2$ has an anti-dependence on $S_1$ if and only if $S_2$ writes a value that was previously read by $S_1$.
\end{definition}
\begin{definition}[\textit{Output dependence}]
    A statement $S_2$ has an output dependence on $S_1$ if and only if $S_2$ writes a value that was also written by $S_1$.
\end{definition}
\renewcommand*{\arraystretch}{2}
\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|}
    \hline
    \textbf{Condition} & \textbf{Dependence Type} & \textbf{Description} \\
    \hline
    $\text{out}(S_1) \cap \text{in}(S_2)^1$ & Flow Dependence & $S\delta S_2$ \\
    \hline
    $\text{in}(S_1) \cap \text{out}(S_2)^1$ & Anti-Dependence & $S\delta^{-1} S_2$ \\
    \hline
    $\text{out}(S_1) \cap \text{out}(S_2)^1$ & Output Dependence & $S\delta^0 S_2$ \\
    \hline
    \end{tabular}
\end{table}
\renewcommand*{\arraystretch}{1}
Both anti-dependences and output dependences are referred to as name dependencies, where the dependence arises due to the reuse of variable names, rather than the actual values.
Two statements $S_1$ and $S_2$ can be executed in parallel only if there are no dependencies between them. 

Dependencies relations can be analyzed by comparing the $\text{in}$ and $\text{out}$ sets for each statement:
\begin{itemize}
    \item $\text{in}(S)$: the set of memory locations (variables) that may be read by $S$.
    \item $\text{out}(S)$: the set of memory locations (variables) that may be modified by $S$.
\end{itemize}

\paragraph*{Loops}
Significant opportunities for parallelism often exist within loops. 
A loop-carried dependence is a dependence that occurs only when the statements are part of a loop's execution. 
In contrast, dependencies between two instances of a statement in the same iteration are loop-independent.
Loop-carried dependencies can limit the parallelization of loop iterations, as they require that certain operations occur in sequence.